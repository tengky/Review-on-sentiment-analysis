{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "x8PDArDSAi1Q"
   },
   "source": [
    "# Review of Sentiment Analysis Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "6wQD_KG5lV2E"
   },
   "source": [
    "The purpose of this review is to provide a broad but moderately deep understanding of different types of sentiment analysis models and their respective pros and cons. We will be exploring 3 different types of datasets and analysed them through different sentiment analysis models, which include:\n",
    "\n",
    "## 1. Traditional models\n",
    "\n",
    "a. TextBlob\n",
    "    \n",
    "b. VADER\n",
    "    \n",
    "c. Logistic Regression\n",
    "\n",
    "d. Naive Bayes \n",
    "\n",
    "e. Support Vector Machine (SVM)\n",
    "   \n",
    "    \n",
    "## 2. Deep learning models\n",
    "\n",
    "a. ULMFit\n",
    "\n",
    "b. BERT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "_hmLWTDKomMl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "hidden": true,
    "id": "eI84VYxTLjmH",
    "outputId": "9ec537a2-933a-420a-d2f4-31df6f26de80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "zfTybDq7mthZ"
   },
   "source": [
    "# Reading and Filtering Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "MNEC5D_iAi1W"
   },
   "source": [
    "### 1. Twitter dataset\n",
    "\n",
    "This dataset includes 1,600,000 tweets with emoticons pre-removed. The dataset was collected using the Twitter API. Source: https://www.kaggle.com/kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "colab_type": "code",
    "hidden": true,
    "id": "CAtNBVaopq9v",
    "outputId": "35c5ae05-a3ba-4288-8a69-21f73c190701",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  ...                                                  5\n",
       "0        0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1        0  ...  is upset that he can't update his Facebook by ...\n",
       "2        0  ...  @Kenichan I dived many times for the ball. Man...\n",
       "3        0  ...    my whole body feels itchy and like its on fire \n",
       "4        0  ...  @nationwideclass no, it's not behaving at all....\n",
       "...     ..  ...                                                ...\n",
       "1599995  4  ...  Just woke up. Having no school is the best fee...\n",
       "1599996  4  ...  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997  4  ...  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998  4  ...  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999  4  ...  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df = pd.read_csv('/content/gdrive/My Drive/Dataset/twitter_train.csv',\n",
    "                       encoding='iso-8859-1',\n",
    "                       header=None)\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "VsF-51GNxRae",
    "outputId": "510a8287-42bf-41ea-d5de-e1225f6bfd3b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text polarity\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...      neg\n",
       "1        is upset that he can't update his Facebook by ...      neg\n",
       "2        @Kenichan I dived many times for the ball. Man...      neg\n",
       "3          my whole body feels itchy and like its on fire       neg\n",
       "4        @nationwideclass no, it's not behaving at all....      neg\n",
       "...                                                    ...      ...\n",
       "1599995  Just woke up. Having no school is the best fee...      pos\n",
       "1599996  TheWDB.com - Very cool to hear old Walt interv...      pos\n",
       "1599997  Are you ready for your MoJo Makeover? Ask me f...      pos\n",
       "1599998  Happy 38th Birthday to my boo of alll time!!! ...      pos\n",
       "1599999  happy #charitytuesday @theNSPCC @SparksCharity...      pos\n",
       "\n",
       "[200000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.columns = ['polarity', 'tweet ID', 'date', 'query', 'username', 'text']\n",
    "tweet_df = tweet_df.replace({'polarity':{0:'neg',4:'pos'}})\n",
    "tweet_df = tweet_df.filter(['text','polarity'])\n",
    "tweet_df_head = tweet_df.head(100000)\n",
    "tweet_df_tail = tweet_df.tail(100000)\n",
    "tweet_df = pd.concat([tweet_df_head, tweet_df_tail])\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "hidden": true,
    "id": "8LCo-GYaiNWH",
    "outputId": "8ee140d2-2ea4-4c60-cb1a-a76a23dec079"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAP9klEQVR4nO3df6yeZX3H8fdn7UDQAQVOCGthh0ij\nK2xOaKDOxCx2gYLLyjZ1MCMNa2wywfljy4b7p4lCgokZgwzYOukozonITGi02jWo21xS5CAMLMg4\ngWHbgFTLjzmmrPrdH89VfVLO1dLz0HNq+34lT851f6/ruu/rSZ700/vHc06qCkmSpvJzs70ASdLB\ny5CQJHUZEpKkLkNCktRlSEiSuubO9gJeaSeeeGKNj4/P9jIk6WfKvffe+92qGtuzfsiFxPj4OBMT\nE7O9DEn6mZLkianqXm6SJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6tpnSCRZm+TpJN8cqh2fZFOS\nR9vPea2eJNcnmUzyQJKzhuasaOMfTbJiqH52kgfbnOuTZG/HkCTNnJdzJnELsGyP2pXAXVW1ELir\nbQNcACxsr1XATTD4Bx9YDZwLnAOsHvpH/ybgPUPzlu3jGJKkGbLPkKiqfwV27lFeDqxr7XXARUP1\nW2tgM3BckpOB84FNVbWzqp4BNgHLWt8xVbW5Bn/Y4tY99jXVMSRJM2S637g+qaqebO2ngJNaez6w\ndWjctlbbW33bFPW9HeMlkqxicObCqaeeur/v5SfGr/zCtOfq0PZf17xttpcA+BlV34H6jI5847qd\nARzQP2+3r2NU1ZqqWlxVi8fGXvKrRyRJ0zTdkPhOu1RE+/l0q28HThkat6DV9lZfMEV9b8eQJM2Q\n6YbEemD3E0orgDuH6pe2p5yWAM+1S0YbgfOSzGs3rM8DNra+55MsaU81XbrHvqY6hiRphuzznkSS\nTwO/AZyYZBuDp5SuAW5PshJ4AnhnG74BuBCYBF4ALgOoqp1JPgrc08Z9pKp23wx/L4MnqI4Cvthe\n7OUYkqQZss+QqKpLOl1LpxhbwOWd/awF1k5RnwDOnKL+vamOIUmaOX7jWpLUZUhIkroMCUlSlyEh\nSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKk\nLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoy\nJCRJXYaEJKnLkJAkdRkSkqSukUIiyQeTbEnyzSSfTvKqJKcluTvJZJLPJDmijT2ybU+2/vGh/Xy4\n1R9Jcv5QfVmrTSa5cpS1SpL237RDIsl84I+BxVV1JjAHuBj4GHBtVZ0OPAOsbFNWAs+0+rVtHEkW\ntXlnAMuAG5PMSTIHuAG4AFgEXNLGSpJmyKiXm+YCRyWZCxwNPAm8Fbij9a8DLmrt5W2b1r80SVr9\ntqr6YVU9DkwC57TXZFU9VlUvAre1sZKkGTLtkKiq7cDHgW8zCIfngHuBZ6tqVxu2DZjf2vOBrW3u\nrjb+hOH6HnN69ZdIsirJRJKJHTt2TPctSZL2MMrlpnkM/md/GvCLwKsZXC6acVW1pqoWV9XisbGx\n2ViCJB2SRrnc9JvA41W1o6r+D/gc8GbguHb5CWABsL21twOnALT+Y4HvDdf3mNOrS5JmyCgh8W1g\nSZKj272FpcBDwFeAt7cxK4A7W3t926b1f7mqqtUvbk8/nQYsBL4O3AMsbE9LHcHg5vb6EdYrSdpP\nc/c9ZGpVdXeSO4BvALuA+4A1wBeA25Jc1Wo3tyk3A59MMgnsZPCPPlW1JcntDAJmF3B5Vf0IIMkV\nwEYGT06traot012vJGn/TTskAKpqNbB6j/JjDJ5M2nPsD4B3dPZzNXD1FPUNwIZR1ihJmj6/cS1J\n6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQu\nQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIk\nJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlS10ghkeS4JHck+VaSh5O8KcnxSTYlebT9nNfG\nJsn1SSaTPJDkrKH9rGjjH02yYqh+dpIH25zrk2SU9UqS9s+oZxLXAV+qqtcDbwAeBq4E7qqqhcBd\nbRvgAmBhe60CbgJIcjywGjgXOAdYvTtY2pj3DM1bNuJ6JUn7YdohkeRY4C3AzQBV9WJVPQssB9a1\nYeuAi1p7OXBrDWwGjktyMnA+sKmqdlbVM8AmYFnrO6aqNldVAbcO7UuSNANGOZM4DdgB/H2S+5J8\nIsmrgZOq6sk25ingpNaeD2wdmr+t1fZW3zZFXZI0Q0YJibnAWcBNVfVG4H/46aUlANoZQI1wjJcl\nyaokE0kmduzYcaAPJ0mHjVFCYhuwrarubtt3MAiN77RLRbSfT7f+7cApQ/MXtNre6gumqL9EVa2p\nqsVVtXhsbGyEtyRJGjbtkKiqp4CtSV7XSkuBh4D1wO4nlFYAd7b2euDS9pTTEuC5dllqI3Beknnt\nhvV5wMbW93ySJe2ppkuH9iVJmgFzR5z/PuBTSY4AHgMuYxA8tydZCTwBvLON3QBcCEwCL7SxVNXO\nJB8F7mnjPlJVO1v7vcAtwFHAF9tLkjRDRgqJqrofWDxF19IpxhZweWc/a4G1U9QngDNHWaMkafr8\nxrUkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVI\nSJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQk\nqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKlr5JBIMifJfUk+37ZPS3J3kskk\nn0lyRKsf2bYnW//40D4+3OqPJDl/qL6s1SaTXDnqWiVJ++eVOJN4P/Dw0PbHgGur6nTgGWBlq68E\nnmn1a9s4kiwCLgbOAJYBN7bgmQPcAFwALAIuaWMlSTNkpJBIsgB4G/CJth3grcAdbcg64KLWXt62\naf1L2/jlwG1V9cOqehyYBM5pr8mqeqyqXgRua2MlSTNk1DOJvwL+DPhx2z4BeLaqdrXtbcD81p4P\nbAVo/c+18T+p7zGnV3+JJKuSTCSZ2LFjx4hvSZK027RDIslvAU9X1b2v4HqmparWVNXiqlo8NjY2\n28uRpEPG3BHmvhn47SQXAq8CjgGuA45LMredLSwAtrfx24FTgG1J5gLHAt8bqu82PKdXlyTNgGmf\nSVTVh6tqQVWNM7jx/OWqehfwFeDtbdgK4M7WXt+2af1frqpq9Yvb00+nAQuBrwP3AAvb01JHtGOs\nn+56JUn7b5QziZ4/B25LchVwH3Bzq98MfDLJJLCTwT/6VNWWJLcDDwG7gMur6kcASa4ANgJzgLVV\nteUArFeS1PGKhERVfRX4ams/xuDJpD3H/AB4R2f+1cDVU9Q3ABteiTVKkvaf37iWJHUZEpKkLkNC\nktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJ\nXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRl\nSEiSugwJSVKXISFJ6jIkJEldhoQkqWvaIZHklCRfSfJQki1J3t/qxyfZlOTR9nNeqyfJ9UkmkzyQ\n5Kyhfa1o4x9NsmKofnaSB9uc65NklDcrSdo/o5xJ7AL+pKoWAUuAy5MsAq4E7qqqhcBdbRvgAmBh\ne60CboJBqACrgXOBc4DVu4OljXnP0LxlI6xXkrSfph0SVfVkVX2jtf8beBiYDywH1rVh64CLWns5\ncGsNbAaOS3IycD6wqap2VtUzwCZgWes7pqo2V1UBtw7tS5I0A16RexJJxoE3AncDJ1XVk63rKeCk\n1p4PbB2atq3V9lbfNkV9quOvSjKRZGLHjh0jvRdJ0k+NHBJJXgP8E/CBqnp+uK+dAdSox9iXqlpT\nVYuravHY2NiBPpwkHTZGCokkP88gID5VVZ9r5e+0S0W0n0+3+nbglKHpC1ptb/UFU9QlSTNklKeb\nAtwMPFxVfznUtR7Y/YTSCuDOofql7SmnJcBz7bLURuC8JPPaDevzgI2t7/kkS9qxLh3alyRpBswd\nYe6bgXcDDya5v9X+ArgGuD3JSuAJ4J2tbwNwITAJvABcBlBVO5N8FLinjftIVe1s7fcCtwBHAV9s\nL0nSDJl2SFTV14De9xaWTjG+gMs7+1oLrJ2iPgGcOd01SpJG4zeuJUldhoQkqcuQkCR1GRKSpC5D\nQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQk\nSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLU\nZUhIkroMCUlSlyEhSeoyJCRJXQd9SCRZluSRJJNJrpzt9UjS4eSgDokkc4AbgAuARcAlSRbN7qok\n6fBxUIcEcA4wWVWPVdWLwG3A8llekyQdNubO9gL2YT6wdWh7G3DunoOSrAJWtc3vJ3lkBtZ2ODgR\n+O5sL+JgkI/N9grU4We0eQU+o780VfFgD4mXparWAGtmex2HmiQTVbV4ttch9fgZPfAO9stN24FT\nhrYXtJokaQYc7CFxD7AwyWlJjgAuBtbP8pok6bBxUF9uqqpdSa4ANgJzgLVVtWWWl3U48RKeDnZ+\nRg+wVNVsr0GSdJA62C83SZJmkSEhSeoyJCRJXYaEJKnLkDhMJRlP8nCSv0uyJck/JzkqyWuTfCnJ\nvUn+Lcnr2/jXJtmc5MEkVyX5/my/Bx362uf0W0k+1T6vdyQ5OsnSJPe1z+PaJEe28dckeSjJA0k+\nPtvrPxQYEoe3hcANVXUG8CzwewweKXxfVZ0N/ClwYxt7HXBdVf0Kg1+PIs2U1wE3VtUvA88DHwJu\nAX6/fR7nAn+U5ATgd4AzqupXgatmab2HFEPi8PZ4Vd3f2vcC48CvA59Ncj/wt8DJrf9NwGdb+x9n\ncpE67G2tqn9v7X8AljL47P5nq60D3gI8B/wAuDnJ7wIvzPhKD0EH9ZfpdMD9cKj9I+Ak4Nmq+rVZ\nWo80lT2/zPUscMJLBg2+fHsOgxB5O3AF8NYDv7xDm2cSGvY88HiSdwBk4A2tbzODy1Ew+PUo0kw5\nNcmbWvsPgAlgPMnprfZu4F+SvAY4tqo2AB8E3vDSXWl/GRLa07uAlUn+A9jCT/9+xweADyV5ADid\nwam9NBMeAS5P8jAwD7gWuIzBZdEHgR8DfwP8AvD59hn9GoN7FxqRv5ZDL0uSo4H/rapKcjFwSVX5\nB6B0QCUZBz5fVWfO8lIOW96T0Mt1NvDXScLgmvAfzvJ6JM0AzyQkSV3ek5AkdRkSkqQuQ0KS1GVI\nSJK6DAlJUtf/A9mZRcot5MSCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(tweet_df['polarity'].unique(),tweet_df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "zhu0QdB_C3ZD"
   },
   "source": [
    "### 2. IMDB movie review dataset\n",
    "This dataset has 100,000 movie reviews which are being labelled as positive or negative review. Source: https://www.kaggle.com/utathya/imdb-review-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "gyyBCmF2JzVa",
    "outputId": "f7c09754-6ea8-4640-e715-04029fe9891e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995</td>\n",
       "      <td>train</td>\n",
       "      <td>Delightfully awful! Made by David Giancola, a ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9998_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996</td>\n",
       "      <td>train</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997</td>\n",
       "      <td>train</td>\n",
       "      <td>At the beginning we can see members of Troma t...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998</td>\n",
       "      <td>train</td>\n",
       "      <td>The movie was incredible, ever since I saw it ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>99_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999</td>\n",
       "      <td>train</td>\n",
       "      <td>TCM came through by acquiring this wonderful, ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9_0.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   type  ...  label         file\n",
       "0               0   test  ...    neg      0_2.txt\n",
       "1               1   test  ...    neg  10000_4.txt\n",
       "2               2   test  ...    neg  10001_1.txt\n",
       "3               3   test  ...    neg  10002_3.txt\n",
       "4               4   test  ...    neg  10003_3.txt\n",
       "...           ...    ...  ...    ...          ...\n",
       "99995       99995  train  ...  unsup   9998_0.txt\n",
       "99996       99996  train  ...  unsup   9999_0.txt\n",
       "99997       99997  train  ...  unsup    999_0.txt\n",
       "99998       99998  train  ...  unsup     99_0.txt\n",
       "99999       99999  train  ...  unsup      9_0.txt\n",
       "\n",
       "[100000 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('/content/gdrive/My Drive/Dataset/imdb.csv', \n",
    "                      encoding='iso-8859-1')\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "5DpllAv6KJV1",
    "outputId": "013623d2-f21b-4c82-9d97-44d371150ccc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "25000  Story of a man who has unnatural feelings for ...   neg\n",
       "25001  Airport '77 starts as a brand new luxury 747 p...   neg\n",
       "25002  This film lacked something I couldn't put my f...   neg\n",
       "25003  Sorry everyone,,, I know this is supposed to b...   neg\n",
       "25004  When I was little my parents took me along to ...   neg\n",
       "...                                                  ...   ...\n",
       "49995  Seeing as the vote average was pretty low, and...   pos\n",
       "49996  The plot had some wretched, unbelievable twist...   pos\n",
       "49997  I am amazed at how this movie(and most others ...   pos\n",
       "49998  A Christmas Together actually came before my t...   pos\n",
       "49999  Working-class romantic drama from director Mar...   pos\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = imdb_df[(imdb_df['type'] == 'train') & (imdb_df['label'] != 'unsup')]\n",
    "imdb_df = imdb_df.filter(['review','label'])\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "hidden": true,
    "id": "COgsKnq-uyH3",
    "outputId": "8858310c-a5f3-48f4-cc96-d8df8b7adc6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntfZWdEHhLAUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(imdb_df['label'].unique(),imdb_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "nfnOQtSsQ-rt"
   },
   "source": [
    "### 3. Amazon's Products review dataset\n",
    "\n",
    "This is a list of over 160,000 consumer reviews for Amazon products like the Kindle, Fire TV Stick, and more provided by Datafiniti's Product Database. The dataset includes basic product information, rating and review text. Source: https://www.kaggle.com/harshaiitj08/amazon-product-ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "BRc1VmdpS2mb",
    "outputId": "e02a8b58-c6fa-4195-f1a4-5ff4712c5f39"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I like the item pricing. My granddaughter want...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Love the magnet easel... great for moving to d...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Both sides are magnetic.  A real plus when you...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bought one a few years ago for my daughter and...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I have a stainless steel refrigerator therefor...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167592</th>\n",
       "      <td>167592</td>\n",
       "      <td>This drone is very fun and super duarable. Its...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167593</th>\n",
       "      <td>167593</td>\n",
       "      <td>This is my brother's most prized toy. It's ext...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167594</th>\n",
       "      <td>167594</td>\n",
       "      <td>This Panther Drone toy is awesome. I definitel...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167595</th>\n",
       "      <td>167595</td>\n",
       "      <td>This is my first drone and it has proven to be...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167596</th>\n",
       "      <td>167596</td>\n",
       "      <td>This is a super fun toy to have around. In our...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                            reviews  ratings\n",
       "0                0  I like the item pricing. My granddaughter want...      5.0\n",
       "1                1  Love the magnet easel... great for moving to d...      4.0\n",
       "2                2  Both sides are magnetic.  A real plus when you...      5.0\n",
       "3                3  Bought one a few years ago for my daughter and...      5.0\n",
       "4                4  I have a stainless steel refrigerator therefor...      4.0\n",
       "...            ...                                                ...      ...\n",
       "167592      167592  This drone is very fun and super duarable. Its...      5.0\n",
       "167593      167593  This is my brother's most prized toy. It's ext...      5.0\n",
       "167594      167594  This Panther Drone toy is awesome. I definitel...      5.0\n",
       "167595      167595  This is my first drone and it has proven to be...      5.0\n",
       "167596      167596  This is a super fun toy to have around. In our...      4.0\n",
       "\n",
       "[167597 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_df = pd.read_csv('/content/gdrive/My Drive/Dataset/amazon.csv')\n",
    "amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "8MGjjVgpJ7Q6",
    "outputId": "dc7c0695-f92e-4f65-ffc3-fea35251e833"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>A crappy cardboard ghost of the original.  Har...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>We have this same game but it was made in 1967...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Hated this product.Predictable.  Not fun.  It ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>I had high hopes for this game, as I am a big ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>thought this was a book with pages to illustra...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>My 3 &amp; 4 y/o love this puzzle.  There is enoug...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>my 4 year old got this last year and still lov...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888</th>\n",
       "      <td>This puzzle is very well made.  The pieces are...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>We love the Melissa and Doug line. We have abo...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>We bought this pirate ship puzzle along witht ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  ratings\n",
       "157   A crappy cardboard ghost of the original.  Har...      1.0\n",
       "165   We have this same game but it was made in 1967...      1.0\n",
       "186   Hated this product.Predictable.  Not fun.  It ...      1.0\n",
       "191   I had high hopes for this game, as I am a big ...      1.0\n",
       "298   thought this was a book with pages to illustra...      1.0\n",
       "...                                                 ...      ...\n",
       "6884  My 3 & 4 y/o love this puzzle.  There is enoug...      5.0\n",
       "6887  my 4 year old got this last year and still lov...      5.0\n",
       "6888  This puzzle is very well made.  The pieces are...      5.0\n",
       "6889  We love the Melissa and Doug line. We have abo...      5.0\n",
       "6890  We bought this pirate ship puzzle along witht ...      5.0\n",
       "\n",
       "[22500 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_df = amz_df.dropna()\n",
    "amz1_df = amz_df.query(\"ratings == 1\").head(4500)\n",
    "amz2_df = amz_df.query(\"ratings == 2\").head(4500)\n",
    "amz3_df = amz_df.query(\"ratings == 3\").head(4500)\n",
    "amz4_df = amz_df.query(\"ratings == 4\").head(4500)\n",
    "amz5_df = amz_df.query(\"ratings == 5\").head(4500)\n",
    "amz_df = pd.concat([amz1_df, amz2_df, amz3_df, amz4_df, amz5_df])\n",
    "amz_df = amz_df.filter(['reviews','ratings'])\n",
    "amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Q5vtAi81UTuQ",
    "outputId": "47d98a78-3515-4d2c-d39d-3e34c7b8f498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(amz_df['ratings'].unique(),amz_df['ratings'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "O1iMn6peChUg"
   },
   "source": [
    "### 4. SST2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "QcHZjrf3CmXP",
    "outputId": "ecfc4cb5-be53-45f3-90fb-1d72e56409ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "6915  painful , horrifying and oppressively tragic ,...      1\n",
       "6916  take care is nicely performed by a quintet of ...      0\n",
       "6917  the script covers huge , heavy topics in a bla...      0\n",
       "6918  a seriously bad film with seriously warped log...      0\n",
       "6919  a deliciously nonsensical comedy about a city ...      1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "sst_df.columns=['review','label']\n",
    "sst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "hidden": true,
    "id": "0Hw7RzXaDtvH",
    "outputId": "d7b7e28f-d10b-4426-991b-606acf69313c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(sst_df['label'].unique(),sst_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "Sn39hhPqdoMv"
   },
   "source": [
    "# Cleaning and Pre-processing Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "mXrs4HPbAi2G"
   },
   "source": [
    "## 1) Remove punctuation, number & lowercase the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "bF6Nt_FnAi2H"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "hidden": true,
    "id": "8or5J3EwyEsI",
    "outputId": "0b1167f8-0a42-4222-b4f0-598db7cad296"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot httptwitpiccomyzl  awww thats a bum...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kwesidei not the whole crew</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>need a hug</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>loltrish hey  long time no see yes rains a bit...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tatianak nope they didnt have it</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>twittera que me muera</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spring break in plain city its snowing</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i just repierced my ears</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>caregiving i couldnt bear to watch it  and i t...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>octolinz it it counts idk why i did either you...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>smarrison i wouldve been the first but i didnt...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>iamjazzyfizzle i wish i got to watch it with y...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hollis death scene will hurt me severely to wa...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>about to file taxes</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lettya ahh ive always wanted to see rent  love...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fakerpattypattz oh dear were you drinking out ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text polarity\n",
       "0   switchfoot httptwitpiccomyzl  awww thats a bum...      neg\n",
       "1   is upset that he cant update his facebook by t...      neg\n",
       "2   kenichan i dived many times for the ball manag...      neg\n",
       "3     my whole body feels itchy and like its on fire       neg\n",
       "4   nationwideclass no its not behaving at all im ...      neg\n",
       "5                        kwesidei not the whole crew       neg\n",
       "6                                         need a hug       neg\n",
       "7   loltrish hey  long time no see yes rains a bit...      neg\n",
       "8                   tatianak nope they didnt have it       neg\n",
       "9                             twittera que me muera        neg\n",
       "10            spring break in plain city its snowing       neg\n",
       "11                          i just repierced my ears       neg\n",
       "12  caregiving i couldnt bear to watch it  and i t...      neg\n",
       "13  octolinz it it counts idk why i did either you...      neg\n",
       "14  smarrison i wouldve been the first but i didnt...      neg\n",
       "15  iamjazzyfizzle i wish i got to watch it with y...      neg\n",
       "16  hollis death scene will hurt me severely to wa...      neg\n",
       "17                               about to file taxes       neg\n",
       "18  lettya ahh ive always wanted to see rent  love...      neg\n",
       "19  fakerpattypattz oh dear were you drinking out ...      neg"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweet_df['text'] = tweet_df['text'].apply(lambda x: clean(x))\n",
    "tweet_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "otcp2aFMMGZO"
   },
   "source": [
    "### 2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "hidden": true,
    "id": "uTXqbqXjKaUN",
    "outputId": "629d83d1-733c-40f3-b057-efcc9727ade5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>airport  starts as a brand new luxury  plane i...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>sorry everyone i know this is supposed to be a...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25005</th>\n",
       "      <td>it appears that many critics find the idea of ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25006</th>\n",
       "      <td>the second attempt by a new york intellectual ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25007</th>\n",
       "      <td>i dont know who to blame the timid writers or ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25008</th>\n",
       "      <td>this film is mediocre at best angie harmon is ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25009</th>\n",
       "      <td>the film is bad there is no other way to say i...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25010</th>\n",
       "      <td>this film is one giant pant load paul schrader...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25011</th>\n",
       "      <td>the plot for descent if it actually can be cal...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25012</th>\n",
       "      <td>plot is not worth discussion even if it hints ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25013</th>\n",
       "      <td>this film is about a male escort getting invol...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25014</th>\n",
       "      <td>this movie must be in line for the most boring...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25015</th>\n",
       "      <td>a wornout plot of a man who takes the rap for ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25016</th>\n",
       "      <td>i saw this movie at a drivein in  until howard...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25017</th>\n",
       "      <td>ghost of dragstrip hollow is a typical s teens...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25018</th>\n",
       "      <td>ghost of dragstrip hollow was one of the many ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25019</th>\n",
       "      <td>ghost of dragstrip hollow appears to take plac...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "25000  story of a man who has unnatural feelings for ...   neg\n",
       "25001  airport  starts as a brand new luxury  plane i...   neg\n",
       "25002  this film lacked something i couldnt put my fi...   neg\n",
       "25003  sorry everyone i know this is supposed to be a...   neg\n",
       "25004  when i was little my parents took me along to ...   neg\n",
       "25005  it appears that many critics find the idea of ...   neg\n",
       "25006  the second attempt by a new york intellectual ...   neg\n",
       "25007  i dont know who to blame the timid writers or ...   neg\n",
       "25008  this film is mediocre at best angie harmon is ...   neg\n",
       "25009  the film is bad there is no other way to say i...   neg\n",
       "25010  this film is one giant pant load paul schrader...   neg\n",
       "25011  the plot for descent if it actually can be cal...   neg\n",
       "25012  plot is not worth discussion even if it hints ...   neg\n",
       "25013  this film is about a male escort getting invol...   neg\n",
       "25014  this movie must be in line for the most boring...   neg\n",
       "25015  a wornout plot of a man who takes the rap for ...   neg\n",
       "25016  i saw this movie at a drivein in  until howard...   neg\n",
       "25017  ghost of dragstrip hollow is a typical s teens...   neg\n",
       "25018  ghost of dragstrip hollow was one of the many ...   neg\n",
       "25019  ghost of dragstrip hollow appears to take plac...   neg"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "imdb_df['review'] = imdb_df['review'].apply(lambda x: clean(x))\n",
    "imdb_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "HnimTVnkeqc7"
   },
   "source": [
    "### 3. Amazon's Product review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "hidden": true,
    "id": "qmclcOIgTtuF",
    "outputId": "44ca7674-1451-4ba1-dcf3-2c5fd8248c23"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>a crappy cardboard ghost of the original  hard...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>we have this same game but it was made in  we ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hated this productpredictable  not fun  it att...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>i had high hopes for this game as i am a big f...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>thought this was a book with pages to illustra...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>started out as a great road trip activity for ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>it didnt work when we bought it a bummer becau...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>heres a creepy little elf and a book  if you w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>this game looked like fun but after trying it ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>it took forever for me to write this review si...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>my granddaughter abigail had to have the new a...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>i thought my one year old would be super into ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>this book contains lead do not give it to your...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>first of all i always wanted to play this game...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>other than being very pretty i dont get why th...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>i installed fresh batteries in the robot and s...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>again like arkham horror i really looked forwa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>so this is a star wars game if you like star w...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>i thought these were made of metal they are a ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>the only reason that they can even justify sel...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  ratings\n",
       "157   a crappy cardboard ghost of the original  hard...      1.0\n",
       "165   we have this same game but it was made in  we ...      1.0\n",
       "186   hated this productpredictable  not fun  it att...      1.0\n",
       "191   i had high hopes for this game as i am a big f...      1.0\n",
       "298   thought this was a book with pages to illustra...      1.0\n",
       "330   started out as a great road trip activity for ...      1.0\n",
       "534   it didnt work when we bought it a bummer becau...      1.0\n",
       "586   heres a creepy little elf and a book  if you w...      1.0\n",
       "730   this game looked like fun but after trying it ...      1.0\n",
       "742   it took forever for me to write this review si...      1.0\n",
       "871   my granddaughter abigail had to have the new a...      1.0\n",
       "941   i thought my one year old would be super into ...      1.0\n",
       "1017  this book contains lead do not give it to your...      1.0\n",
       "1123  first of all i always wanted to play this game...      1.0\n",
       "1256  other than being very pretty i dont get why th...      1.0\n",
       "1344  i installed fresh batteries in the robot and s...      1.0\n",
       "1409  again like arkham horror i really looked forwa...      1.0\n",
       "1469  so this is a star wars game if you like star w...      1.0\n",
       "1521  i thought these were made of metal they are a ...      1.0\n",
       "1565  the only reason that they can even justify sel...      1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "amz_df['reviews'] = amz_df['reviews'].apply(lambda x: clean(x))\n",
    "amz_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "_pYZfb7gEAB3"
   },
   "source": [
    "### 4. SST2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "hidden": true,
    "id": "sWLXMIWfD_Hy",
    "outputId": "9a56e312-2846-40fe-df0a-136889cf3499"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring  funny and finally transporting re ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo nt sit still fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker s bartleby should have been th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>campanella gets the tone just right funny in t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a fan film that for the uninitiated plays bett...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b art and berling are both superb  while huppe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a little less extreme than in the past  with l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the film is strictly routine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a lyrical metaphor for cultural and personal s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the most repugnant adaptation of a classic tex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>for something as splendid looking as this part...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>this is a stunning film  a one of a kind tour ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>may be more genial than ingenious  but it gets...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>there is a freedom to watching stunts that are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>if the tuxedo actually were a suit  it would f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>as quiet  patient and tenacious as mr lopez hi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>final verdict you ve seen it all before</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>blue crush follows the formula  but throws in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  label\n",
       "0   a stirring  funny and finally transporting re ...      1\n",
       "1   apparently reassembled from the cutting room f...      0\n",
       "2   they presume their audience wo nt sit still fo...      0\n",
       "3   this is a visually stunning rumination on love...      1\n",
       "4   jonathan parker s bartleby should have been th...      1\n",
       "5   campanella gets the tone just right funny in t...      1\n",
       "6   a fan film that for the uninitiated plays bett...      0\n",
       "7   b art and berling are both superb  while huppe...      1\n",
       "8   a little less extreme than in the past  with l...      0\n",
       "9                        the film is strictly routine      0\n",
       "10  a lyrical metaphor for cultural and personal s...      1\n",
       "11  the most repugnant adaptation of a classic tex...      0\n",
       "12  for something as splendid looking as this part...      0\n",
       "13  this is a stunning film  a one of a kind tour ...      1\n",
       "14  may be more genial than ingenious  but it gets...      1\n",
       "15  there is a freedom to watching stunts that are...      1\n",
       "16  if the tuxedo actually were a suit  it would f...      0\n",
       "17  as quiet  patient and tenacious as mr lopez hi...      1\n",
       "18            final verdict you ve seen it all before      0\n",
       "19  blue crush follows the formula  but throws in ...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "sst_df['review'] = sst_df['review'].apply(lambda x: clean(x))\n",
    "sst_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "p9VQ_NcFyXHj"
   },
   "source": [
    "## 2) Count vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "QYabZ939Ai2U"
   },
   "source": [
    "The vectorizer shows the frequecy of a term t occurs in a document d. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "Fxj9z_e5Ai2V"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "hidden": true,
    "id": "h5U5X35jybUk",
    "outputId": "5c905307-749f-4e28-d0a1-7d3592355667"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>dont</th>\n",
       "      <th>going</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>home</th>\n",
       "      <th>im</th>\n",
       "      <th>just</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>lol</th>\n",
       "      <th>love</th>\n",
       "      <th>new</th>\n",
       "      <th>night</th>\n",
       "      <th>really</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>want</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        day  dont  going  good  got  ...  think  time  today  want  work\n",
       "0         1     0      0     0    1  ...      0     0      0     0     0\n",
       "1         0     0      0     0    0  ...      0     0      1     0     0\n",
       "2         0     0      0     0    0  ...      0     0      0     0     0\n",
       "3         0     0      0     0    0  ...      0     0      0     0     0\n",
       "4         0     0      0     0    0  ...      0     0      0     0     0\n",
       "...     ...   ...    ...   ...  ...  ...    ...   ...    ...   ...   ...\n",
       "199995    0     0      0     0    0  ...      0     0      0     0     0\n",
       "199996    0     0      0     0    0  ...      0     0      0     0     0\n",
       "199997    0     0      0     0    0  ...      0     0      0     0     0\n",
       "199998    0     0      0     0    0  ...      0     1      0     0     0\n",
       "199999    0     0      0     0    0  ...      0     0      0     0     0\n",
       "\n",
       "[200000 rows x 20 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(tweet_df[\"text\"]).toarray()\n",
    "tweet_count_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "tweet_count_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "gKXpgPVLd5ef"
   },
   "source": [
    "###  2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "hidden": true,
    "id": "a2qPCRFEKkH8",
    "outputId": "a43c31cf-56a0-4a81-b7be-103f28c635c6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>br</th>\n",
       "      <th>br br</th>\n",
       "      <th>characters</th>\n",
       "      <th>dont</th>\n",
       "      <th>film</th>\n",
       "      <th>films</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>just</th>\n",
       "      <th>like</th>\n",
       "      <th>make</th>\n",
       "      <th>movie</th>\n",
       "      <th>movies</th>\n",
       "      <th>people</th>\n",
       "      <th>really</th>\n",
       "      <th>story</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>way</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bad  br  br br  characters  dont  ...  really  story  think  time  way\n",
       "0        0   0      0           0     0  ...       0      1      1     1    0\n",
       "1        1   4      0           2     1  ...       0      0      1     4    0\n",
       "2        0   2      0           0     2  ...       0      0      0     0    0\n",
       "3        0   0      0           0     0  ...       1      1      1     1    0\n",
       "4        0   0      0           2     0  ...       0      1      0     0    0\n",
       "...    ...  ..    ...         ...   ...  ...     ...    ...    ...   ...  ...\n",
       "24995    0   3      0           0     0  ...       0      0      1     0    0\n",
       "24996    0   1      0           0     0  ...       0      0      0     0    0\n",
       "24997    1   5      0           0     1  ...       0      1      0     0    1\n",
       "24998    0   0      0           0     1  ...       0      0      0     1    0\n",
       "24999    0   0      0           2     0  ...       1      1      0     0    1\n",
       "\n",
       "[25000 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\",\n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(imdb_df[\"review\"]).toarray()\n",
    "imdb_count_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "imdb_count_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "G1TpiMjie0xE"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "hidden": true,
    "id": "0oIwW1ZKbZsM",
    "outputId": "bddd157f-55be-4352-c50d-55a99b6a31d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daughter</th>\n",
       "      <th>dont</th>\n",
       "      <th>fun</th>\n",
       "      <th>game</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>great</th>\n",
       "      <th>just</th>\n",
       "      <th>kids</th>\n",
       "      <th>like</th>\n",
       "      <th>little</th>\n",
       "      <th>old</th>\n",
       "      <th>play</th>\n",
       "      <th>really</th>\n",
       "      <th>son</th>\n",
       "      <th>time</th>\n",
       "      <th>toy</th>\n",
       "      <th>use</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       daughter  dont  fun  game  good  ...  time  toy  use  year  year old\n",
       "0             0     0    0     0     0  ...     0    0    0     0         0\n",
       "1             0     0    0     2     0  ...     0    0    0     0         0\n",
       "2             0     0    1     1     0  ...     0    0    0     0         0\n",
       "3             0     0    0     8     0  ...     0    0    1     0         0\n",
       "4             0     0    0     0     0  ...     0    0    0     0         0\n",
       "...         ...   ...  ...   ...   ...  ...   ...  ...  ...   ...       ...\n",
       "22495         0     0    0     0     0  ...     0    0    0     0         0\n",
       "22496         0     0    0     0     0  ...     0    0    0     2         1\n",
       "22497         0     0    0     0     0  ...     0    0    0     1         1\n",
       "22498         0     0    0     0     0  ...     0    0    0     1         0\n",
       "22499         0     0    0     0     1  ...     0    0    0     2         2\n",
       "\n",
       "[22500 rows x 20 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(amz_df[\"reviews\"]).toarray()\n",
    "amz_count_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "amz_count_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "arnleFafAi2g"
   },
   "source": [
    "## 3) TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "bJTX-zXjyfg_"
   },
   "source": [
    "\n",
    "\n",
    "1. Term Frequency (TF)\n",
    "\n",
    "  The frequency of a word **w** appears in a document divided by the total number of words in the document. Every document has its own term frequency.\n",
    "\n",
    "  ![alt text](https://miro.medium.com/proxy/1*HM0Vcdrx2RApOyjp_ZeW_Q.png)\n",
    "\n",
    "2. Inverse Data Frequency (IDF)\n",
    "\n",
    "  The log of the number of documents divided by the number of documents that  contain the word **w**. IDF determines the weight of rare words across all documents in the corpus.\n",
    "\n",
    "  ![alt text](https://miro.medium.com/proxy/1*A5YGwFpcTd0YTCdgoiHFUw.png)\n",
    "\n",
    "  Eg: Assume we have 1000 documents and the word 'the' appears 1000 times while 'python' appears 500 times across all the documents and the IDF score will penalise the common words.\n",
    "  \n",
    "  idf(the) = log(1000/1000) = 0\n",
    "  \n",
    "  idf(python) = log(1000/500) = 0.301\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "  The TF-IDF vectorizer is simply the TF multiplied by IDF. The TF-IDF score increases with number of occurrences within a document and rarity of terms in the collection of documents.\n",
    "\n",
    "![alt text](https://miro.medium.com/proxy/1*nSqHXwOIJ2fa_EFLTh5KYw.png)\n",
    "\n",
    "Reference: https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "JWnASjDIAi2h"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "hidden": true,
    "id": "4IOIZ2uTyjTg",
    "outputId": "4d701dd1-6f60-4adb-8ac9-e8de9eef1367"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>dont</th>\n",
       "      <th>going</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>home</th>\n",
       "      <th>im</th>\n",
       "      <th>just</th>\n",
       "      <th>know</th>\n",
       "      <th>like</th>\n",
       "      <th>lol</th>\n",
       "      <th>love</th>\n",
       "      <th>new</th>\n",
       "      <th>night</th>\n",
       "      <th>really</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>want</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.671002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             day  dont  going  good       got  ...  think  time  today  want  work\n",
       "0       0.671002   0.0    0.0   0.0  0.741456  ...    0.0   0.0    0.0   0.0   0.0\n",
       "1       0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    1.0   0.0   0.0\n",
       "2       0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "3       0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "4       0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "...          ...   ...    ...   ...       ...  ...    ...   ...    ...   ...   ...\n",
       "199995  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "199996  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "199997  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "199998  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   1.0    0.0   0.0   0.0\n",
       "199999  0.000000   0.0    0.0   0.0  0.000000  ...    0.0   0.0    0.0   0.0   0.0\n",
       "\n",
       "[200000 rows x 20 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(tweet_df['text']).toarray()\n",
    "tweet_tfidf_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "tweet_tfidf_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "16mRTl8Ld9-R"
   },
   "source": [
    "### 2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ZxFkecnHKpP7",
    "outputId": "270013b4-480e-4d2b-81be-11fb4f0f83f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>br</th>\n",
       "      <th>br br</th>\n",
       "      <th>characters</th>\n",
       "      <th>dont</th>\n",
       "      <th>film</th>\n",
       "      <th>films</th>\n",
       "      <th>good</th>\n",
       "      <th>great</th>\n",
       "      <th>just</th>\n",
       "      <th>like</th>\n",
       "      <th>make</th>\n",
       "      <th>movie</th>\n",
       "      <th>movies</th>\n",
       "      <th>people</th>\n",
       "      <th>really</th>\n",
       "      <th>story</th>\n",
       "      <th>think</th>\n",
       "      <th>time</th>\n",
       "      <th>way</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335162</td>\n",
       "      <td>0.405050</td>\n",
       "      <td>0.316614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375876</td>\n",
       "      <td>0.429411</td>\n",
       "      <td>0.357035</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.130018</td>\n",
       "      <td>0.322704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.270551</td>\n",
       "      <td>0.125055</td>\n",
       "      <td>0.254427</td>\n",
       "      <td>0.410294</td>\n",
       "      <td>0.104034</td>\n",
       "      <td>0.377180</td>\n",
       "      <td>0.393106</td>\n",
       "      <td>0.187491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133288</td>\n",
       "      <td>0.443291</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666688</td>\n",
       "      <td>0.226064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211993</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.378259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734474</td>\n",
       "      <td>0.221856</td>\n",
       "      <td>0.224530</td>\n",
       "      <td>0.256509</td>\n",
       "      <td>0.213275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.394546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.618386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.573268</td>\n",
       "      <td>0.136709</td>\n",
       "      <td>0.186747</td>\n",
       "      <td>0.115979</td>\n",
       "      <td>0.193218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367688</td>\n",
       "      <td>0.287410</td>\n",
       "      <td>0.274159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.232585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.389802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.634500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.128307</td>\n",
       "      <td>0.398070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102664</td>\n",
       "      <td>0.124072</td>\n",
       "      <td>0.096983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.549381</td>\n",
       "      <td>0.653754</td>\n",
       "      <td>0.125542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.578422</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.710070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302561</td>\n",
       "      <td>0.306207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            bad        br  br br  ...     think      time       way\n",
       "0      0.000000  0.000000    0.0  ...  0.429411  0.357035  0.000000\n",
       "1      0.130018  0.322704    0.0  ...  0.133288  0.443291  0.000000\n",
       "2      0.000000  0.430096    0.0  ...  0.000000  0.000000  0.000000\n",
       "3      0.000000  0.000000    0.0  ...  0.256509  0.213275  0.000000\n",
       "4      0.000000  0.000000    0.0  ...  0.000000  0.000000  0.000000\n",
       "...         ...       ...    ...  ...       ...       ...       ...\n",
       "24995  0.000000  0.707811    0.0  ...  0.389802  0.000000  0.000000\n",
       "24996  0.000000  0.634500    0.0  ...  0.000000  0.000000  0.000000\n",
       "24997  0.128307  0.398070    0.0  ...  0.000000  0.000000  0.127580\n",
       "24998  0.000000  0.000000    0.0  ...  0.000000  0.578422  0.000000\n",
       "24999  0.000000  0.000000    0.0  ...  0.000000  0.000000  0.339303\n",
       "\n",
       "[25000 rows x 20 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(imdb_df['review']).toarray()\n",
    "imdb_tfidf_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "imdb_tfidf_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "sZ844o2Ne4iG"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "hidden": true,
    "id": "6u5WBrNUbi66",
    "outputId": "a299374a-db12-471b-def8-a0d0781a9114"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daughter</th>\n",
       "      <th>dont</th>\n",
       "      <th>fun</th>\n",
       "      <th>game</th>\n",
       "      <th>good</th>\n",
       "      <th>got</th>\n",
       "      <th>great</th>\n",
       "      <th>just</th>\n",
       "      <th>kids</th>\n",
       "      <th>like</th>\n",
       "      <th>little</th>\n",
       "      <th>old</th>\n",
       "      <th>play</th>\n",
       "      <th>really</th>\n",
       "      <th>son</th>\n",
       "      <th>time</th>\n",
       "      <th>toy</th>\n",
       "      <th>use</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253213</td>\n",
       "      <td>0.283729</td>\n",
       "      <td>0.523363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372708</td>\n",
       "      <td>0.395595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.328855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.683298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.948393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107862</td>\n",
       "      <td>0.226104</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.132661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.76129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.39742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.753432</td>\n",
       "      <td>0.407833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423438</td>\n",
       "      <td>0.358947</td>\n",
       "      <td>0.402205</td>\n",
       "      <td>0.370951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425087</td>\n",
       "      <td>0.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524964</td>\n",
       "      <td>0.445010</td>\n",
       "      <td>0.498640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279224</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544337</td>\n",
       "      <td>0.589300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       daughter  dont       fun      game  ...  toy       use      year  year old\n",
       "0           0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000\n",
       "1           0.0   0.0  0.000000  0.586391  ...  0.0  0.000000  0.000000  0.000000\n",
       "2           0.0   0.0  0.372708  0.395595  ...  0.0  0.000000  0.000000  0.000000\n",
       "3           0.0   0.0  0.000000  0.948393  ...  0.0  0.132661  0.000000  0.000000\n",
       "4           0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000\n",
       "...         ...   ...       ...       ...  ...  ...       ...       ...       ...\n",
       "22495       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.000000  0.000000\n",
       "22496       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.753432  0.407833\n",
       "22497       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.425087  0.460200\n",
       "22498       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.527008  0.000000\n",
       "22499       0.0   0.0  0.000000  0.000000  ...  0.0  0.000000  0.544337  0.589300\n",
       "\n",
       "[22500 rows x 20 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=20)\n",
    "vector = vectorizer.fit_transform(amz_df['reviews']).toarray()\n",
    "amz_tfidf_vector_df = pd.DataFrame(vector, columns = vectorizer.get_feature_names())\n",
    "amz_tfidf_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "3ba3KUm5Ai2q"
   },
   "source": [
    "# A) Traditional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "D7qlOvjKAi2r"
   },
   "source": [
    "# 1. TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "o2K9NcSSyo9j"
   },
   "source": [
    "TextBlob is built based on NLTK and Pattern. It has great API for all the common NLP operations. It’s great for initial prototyping in almost every NLP project. Unfortunately, it inherits the low performance from NLTK and therefore it’s not good for large scale production usage. \n",
    "\n",
    "\n",
    "Functionalities:\n",
    "1. Tokenization\n",
    "2. Parts Of Speech Tagging (POS) \n",
    "3. Name Entity Recognition (NER)\n",
    "4. Classification \n",
    "5. Sentiment analysis\n",
    "\n",
    "Sentiment analysis using TextBlob will result two sentiment metrics, which are: \n",
    "1. Polarity \n",
    "\n",
    "    lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement    \n",
    "2. Subjectivity \n",
    "\n",
    "    lies in the range of [0,1] where 0 indicates as factual information ans 1 indicates as personal opinion\n",
    "\n",
    "Example: \n",
    "\n",
    "* Towards Data Science is a great platform to learn data science.\n",
    "    \n",
    "    Sentiment(polarity=0.8, subjectivity=0.75)\n",
    "\n",
    "* We can see that polarity is 0.8, which means that the statement is positive and 0.75 subjectivity refers that mostly it is a personal opinion and not a factual information. We can say it's a positive and subjective statement.\n",
    "\n",
    "Pros:\n",
    "1. easy to learn and offers a lot of features like sentiment analysis, pos-tagging, noun phrase extraction etc\n",
    "\n",
    "2. provides language translation and detection which is powered by Google Translate\n",
    "\n",
    "3. does not require training process to predict sentiment of text, more efficient in some extent\n",
    "\n",
    "Cons:\n",
    "1. slow in performance when dealing with huge amount of data (more than 100k)\n",
    "\n",
    "2. does not recognise the context of data, hence the result generated are usually less accurate compared to other custom trained model (Eg: Logistic Regression, Naive Bayes etc)\n",
    "\n",
    "For more details, kindly refer to Textblob documentation https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "Reference: https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "eFgCARtqfPzI"
   },
   "outputs": [],
   "source": [
    "#Example for Textblob language translation\n",
    "from textblob import TextBlob\n",
    "TextBlob('你在做什么？').translate(from_lang='zh-CN', to ='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "aB9fO4nkgE6E"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 858
    },
    "colab_type": "code",
    "hidden": true,
    "id": "uO5Q0qJiytog",
    "outputId": "c63f27cc-864e-48a1-ba82-da0c2203abc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot httptwitpiccomyzl  awww thats a bum...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.6250</td>\n",
       "      <td>neg</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>thewdbcom  very cool to hear old walt intervie...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy charitytuesday thenspcc sparkscharity sp...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  ... textblob_subjectivity\n",
       "0        switchfoot httptwitpiccomyzl  awww thats a bum...  ...                0.4500\n",
       "1        is upset that he cant update his facebook by t...  ...                0.0000\n",
       "2        kenichan i dived many times for the ball manag...  ...                0.5000\n",
       "3          my whole body feels itchy and like its on fire   ...                0.4000\n",
       "4        nationwideclass no its not behaving at all im ...  ...                1.0000\n",
       "...                                                    ...  ...                   ...\n",
       "1599995  just woke up having no school is the best feel...  ...                0.3000\n",
       "1599996  thewdbcom  very cool to hear old walt intervie...  ...                0.5225\n",
       "1599997  are you ready for your mojo makeover ask me fo...  ...                0.5000\n",
       "1599998  happy th birthday to my boo of alll time tupac...  ...                1.0000\n",
       "1599999  happy charitytuesday thenspcc sparkscharity sp...  ...                1.0000\n",
       "\n",
       "[200000 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df['textblob_polarity'] = tweet_df.apply(lambda x: TextBlob(x['text']).sentiment.polarity, axis=1)\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(tweet_df):\n",
    "    if ((tweet_df.iloc[i]['textblob_polarity'] >= 0)):\n",
    "        predicted_value.append('pos')\n",
    "        i = i+1\n",
    "    elif ((tweet_df.iloc[i]['textblob_polarity'] < 0)):\n",
    "        predicted_value.append('neg')\n",
    "        i = i+1\n",
    "tweet_df['textblob_predicted'] = predicted_value\n",
    "\n",
    "tweet_df['textblob_subjectivity'] = tweet_df.apply(lambda x: TextBlob(x['text']).sentiment.subjectivity, axis=1)       \n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "cMBgrlCq4EWr",
    "outputId": "529acbba-78e8-48e6-80fa-0acf8c95ed4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.6920631466036549\n",
      "Accuracy score: 0.60383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "m1 = f1_score(tweet_df['polarity'], tweet_df['textblob_predicted'], pos_label='pos')\n",
    "m2 = accuracy_score(tweet_df['polarity'], tweet_df['textblob_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "dB_NF6H3eC6S"
   },
   "source": [
    "We can observe that TextBlob doesn't work well with Twitter dataset as the F1 score and accuracy score are 0.692 and 0.604 respectively, which are belowe 0.7. This shows results generated by TextBlob are less in terms of accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "HqqmszejAi24"
   },
   "source": [
    "### 2. IMDB movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "colab_type": "code",
    "hidden": true,
    "id": "T1KYHaFGK_Ok",
    "outputId": "993de994-3387-4343-d9fb-24fb9101edcf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.071759</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.620370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>airport  starts as a brand new luxury  plane i...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.499230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>sorry everyone i know this is supposed to be a...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.043542</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.647083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.055741</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.557328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>seeing as the vote average was pretty low and ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.291961</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.563529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>the plot had some wretched unbelievable twists...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am amazed at how this movieand most others h...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.136099</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.645995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>a christmas together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.118069</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.461614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>workingclass romantic drama from director mart...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.635819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  ... textblob_subjectivity\n",
       "25000  story of a man who has unnatural feelings for ...  ...              0.620370\n",
       "25001  airport  starts as a brand new luxury  plane i...  ...              0.499230\n",
       "25002  this film lacked something i couldnt put my fi...  ...              0.527778\n",
       "25003  sorry everyone i know this is supposed to be a...  ...              0.647083\n",
       "25004  when i was little my parents took me along to ...  ...              0.557328\n",
       "...                                                  ...  ...                   ...\n",
       "49995  seeing as the vote average was pretty low and ...  ...              0.563529\n",
       "49996  the plot had some wretched unbelievable twists...  ...              0.600000\n",
       "49997  i am amazed at how this movieand most others h...  ...              0.645995\n",
       "49998  a christmas together actually came before my t...  ...              0.461614\n",
       "49999  workingclass romantic drama from director mart...  ...              0.635819\n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df['textblob_polarity'] = imdb_df.apply(lambda x: TextBlob(x['review']).sentiment.polarity, axis=1)\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(imdb_df):\n",
    "    if ((imdb_df.iloc[i]['textblob_polarity'] >= 0)):\n",
    "        predicted_value.append('pos')\n",
    "        i = i+1\n",
    "    elif ((imdb_df.iloc[i]['textblob_polarity'] < 0)):\n",
    "        predicted_value.append('neg')\n",
    "        i = i+1        \n",
    "imdb_df['textblob_predicted'] = predicted_value\n",
    "\n",
    "imdb_df['textblob_subjectivity'] = imdb_df.apply(lambda x: TextBlob(x['review']).sentiment.subjectivity, axis=1)\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "DWNs4Wq14gRt",
    "outputId": "c82e3b09-dbb9-4d15-93d6-79bbc88a1585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7515891177218408\n",
      "Accuracy score: 0.68736\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(imdb_df['label'], imdb_df['textblob_predicted'], pos_label='pos')\n",
    "m2 = accuracy_score(imdb_df['label'], imdb_df['textblob_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "pEeI5plRe9yv"
   },
   "source": [
    "The performance of TextBlob in movie review dataset is slightly better than Twitter dataset as the F1 score and accuracy score are 0.752 and 0.687 respectively, which one of the measure exceeds 0.7 . Besides, we can notice that most of the reviews produce subjectivity score that exceeds 0.5, indicating they are fairly subjective statement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "WVVFkjz-Ai3E"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "colab_type": "code",
    "hidden": true,
    "id": "3EsPK9n3bpk2",
    "outputId": "e146ac29-b165-480e-bd70-1a6ba852ce1d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>a crappy cardboard ghost of the original  hard...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.305556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.763889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>we have this same game but it was made in  we ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081389</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hated this productpredictable  not fun  it att...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.194792</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.463542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>i had high hopes for this game as i am a big f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.024593</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.419584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>thought this was a book with pages to illustra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.189231</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.344808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>my    yo love this puzzle  there is enough of ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>my  year old got this last year and still love...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888</th>\n",
       "      <td>this puzzle is very well made  the pieces are ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.177546</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.531812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>we love the melissa and doug line we have abou...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.219123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.506364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>we bought this pirate ship puzzle along witht ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.229091</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.436818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  ...  textblob_subjectivity\n",
       "157   a crappy cardboard ghost of the original  hard...  ...               0.763889\n",
       "165   we have this same game but it was made in  we ...  ...               0.415000\n",
       "186   hated this productpredictable  not fun  it att...  ...               0.463542\n",
       "191   i had high hopes for this game as i am a big f...  ...               0.419584\n",
       "298   thought this was a book with pages to illustra...  ...               0.344808\n",
       "...                                                 ...  ...                    ...\n",
       "6884  my    yo love this puzzle  there is enough of ...  ...               0.566667\n",
       "6887  my  year old got this last year and still love...  ...               0.412500\n",
       "6888  this puzzle is very well made  the pieces are ...  ...               0.531812\n",
       "6889  we love the melissa and doug line we have abou...  ...               0.506364\n",
       "6890  we bought this pirate ship puzzle along witht ...  ...               0.436818\n",
       "\n",
       "[22500 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_df['textblob_polarity'] = amz_df.apply(lambda x: TextBlob(x['reviews']).sentiment.polarity, axis=1)\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(amz_df):\n",
    "    if ((amz_df.iloc[i]['textblob_polarity'] > 0.6)):\n",
    "        predicted_value.append(5.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['textblob_polarity'] <= 0.6 and amz_df.iloc[i]['textblob_polarity'] > 0.2)):\n",
    "        predicted_value.append(4.0)\n",
    "        i = i+1 \n",
    "    elif ((amz_df.iloc[i]['textblob_polarity'] <= 0.2 and amz_df.iloc[i]['textblob_polarity'] > -0.2)):\n",
    "        predicted_value.append(3.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['textblob_polarity'] <= -0.2 and amz_df.iloc[i]['textblob_polarity']> -0.6)):\n",
    "        predicted_value.append(2.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['textblob_polarity'] <= -0.6)):\n",
    "        predicted_value.append(1.0)\n",
    "        i = i+1         \n",
    "amz_df['textblob_predicted'] = predicted_value\n",
    "\n",
    "amz_df['textblob_subjectivity'] = amz_df.apply(lambda x: TextBlob(x['reviews']).sentiment.subjectivity, axis=1)\n",
    "amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "GPs9rKbFUHx4",
    "outputId": "7185f0eb-a1fd-4968-b4ac-1f5bed955d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.15556798102787572\n",
      "Accuracy score: 0.22924444444444445\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(amz_df['ratings'], amz_df['textblob_predicted'], average='macro')\n",
    "m2 = accuracy_score(amz_df['ratings'], amz_df['textblob_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "1zv0TpBvAi3L"
   },
   "source": [
    "In conclusion, TextBlob shines if your intention is having a quick prediction on the sentiment of documents but be awared that the accuracy is lower compared to custom trained model like Logistic Regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "9ApAb4F6Ai3N"
   },
   "source": [
    "# 2. VADER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "1bFaFZubzDfF"
   },
   "source": [
    "Widely used in analyzing sentiment on social media text because it has been specifically attuned to analyze sentiments expressed in social media (Eg: Twitter, Facebook etc)\n",
    "\n",
    "VADER produces four sentiment metrics:\n",
    " 1. positive\n",
    " 2. neutral \n",
    " 3. negative \n",
    " 4. compound\n",
    " \n",
    "The first three, positive, neutral and negative, represent the proportion of the text that falls into those categories. \n",
    "\n",
    "The compound score is a normalized score of **sum_s** and\n",
    "**sum_s** is the sum of the valence score computed based on pre-defined sentiment lexicon (aka Sentiment Intensity) and\n",
    "the normalized score is simply the **sum_s** divided by square root of  its square plus an alpha parameter (a hyperparameter).\n",
    "\n",
    "> norm(sum_s) = sum_s / sqrt(sum_s*sum_s + alpha)\n",
    " \n",
    "\n",
    "Pros\n",
    "1. Among the most comprehensive tools for social media analysis \n",
    "\n",
    "2. Easy to use and does not require training process\n",
    "\n",
    "Cons\n",
    "1. Not accurate when dealing when longer text and complex data\n",
    "\n",
    "2. Does not recognize context of the datasets, hence the result generated are less accurate when dealing with datasets \n",
    "\n",
    "Reference: \n",
    "1. https://www.kaggle.com/nikhilsable/sentiment-using-airline-tweets-using-vader\n",
    "\n",
    "2. http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "17qMPg4aAi3S"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "colab_type": "code",
    "hidden": true,
    "id": "AOuacolEzFk1",
    "outputId": "201496b3-d2f8-42e0-b163-8f2d52b1f13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
      "\r",
      "\u001b[K     |██▋                             | 10kB 20.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 30kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 40kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 51kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 61kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 81kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 92kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 112kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 122kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.2.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>Vader_score</th>\n",
       "      <th>Vader_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot httptwitpiccomyzl  awww thats a bum...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.4500</td>\n",
       "      <td>-0.3818</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.7269</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.6250</td>\n",
       "      <td>neg</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>thewdbcom  very cool to hear old walt intervie...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5225</td>\n",
       "      <td>0.3804</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>happy th birthday to my boo of alll time tupac...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>happy charitytuesday thenspcc sparkscharity sp...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>pos</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  ... Vader_predicted\n",
       "0        switchfoot httptwitpiccomyzl  awww thats a bum...  ...             neg\n",
       "1        is upset that he cant update his facebook by t...  ...             neg\n",
       "2        kenichan i dived many times for the ball manag...  ...             pos\n",
       "3          my whole body feels itchy and like its on fire   ...             neg\n",
       "4        nationwideclass no its not behaving at all im ...  ...             neg\n",
       "...                                                    ...  ...             ...\n",
       "1599995  just woke up having no school is the best feel...  ...             pos\n",
       "1599996  thewdbcom  very cool to hear old walt intervie...  ...             pos\n",
       "1599997  are you ready for your mojo makeover ask me fo...  ...             pos\n",
       "1599998  happy th birthday to my boo of alll time tupac...  ...             pos\n",
       "1599999  happy charitytuesday thenspcc sparkscharity sp...  ...             pos\n",
       "\n",
       "[200000 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "i = 0\n",
    "comp_value = []\n",
    "while i < len(tweet_df):\n",
    "    k = analyser.polarity_scores(tweet_df.iloc[i]['text'])\n",
    "    comp_value.append(k['compound'])\n",
    "    i += 1\n",
    "    \n",
    "comp_value = np.array(comp_value)\n",
    "tweet_df['Vader_score'] = comp_value\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(tweet_df):\n",
    "    if ((tweet_df.iloc[i]['Vader_score'] >= 0)):\n",
    "        predicted_value.append('pos')\n",
    "        i = i+1\n",
    "    elif ((tweet_df.iloc[i]['Vader_score'] < 0)):\n",
    "        predicted_value.append('neg')\n",
    "        i = i+1\n",
    "        \n",
    "tweet_df['Vader_predicted'] = predicted_value\n",
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "5vhLkWoq4yzJ",
    "outputId": "929e9a8a-e490-4173-89d5-610cd5ba9deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7211264630247287\n",
      "Accuracy score: 0.656775\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(tweet_df['polarity'], tweet_df['Vader_predicted'], pos_label='pos')\n",
    "m2 = accuracy_score(tweet_df['polarity'], tweet_df['Vader_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "PqxnN851Ai3e"
   },
   "source": [
    "VADER is doing well in predicting the sentiment for Twitter dataset as the F1 score and accuracy score are 0.721 and 0.657 respectively, which are better than the result produced by TextBlob. This proves that VADER is better in analysing social media text than TextBlob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Ry4BfHLveFgb"
   },
   "source": [
    "### 2. IMDB movie review dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "STfGOP4kLljB",
    "outputId": "0a1185ba-9963-43d1-ce6e-b9d7f081fb2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>Vader_score</th>\n",
       "      <th>Vader_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.071759</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.620370</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>airport  starts as a brand new luxury  plane i...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.040492</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.499230</td>\n",
       "      <td>-0.9657</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.8936</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>sorry everyone i know this is supposed to be a...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.043542</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.647083</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>-0.055741</td>\n",
       "      <td>neg</td>\n",
       "      <td>0.557328</td>\n",
       "      <td>-0.9757</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>seeing as the vote average was pretty low and ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.291961</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.563529</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>the plot had some wretched unbelievable twists...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.8934</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i am amazed at how this movieand most others h...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.136099</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.645995</td>\n",
       "      <td>0.9594</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>a christmas together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.118069</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.461614</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>workingclass romantic drama from director mart...</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>pos</td>\n",
       "      <td>0.635819</td>\n",
       "      <td>0.8937</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  ... Vader_predicted\n",
       "25000  story of a man who has unnatural feelings for ...  ...             pos\n",
       "25001  airport  starts as a brand new luxury  plane i...  ...             neg\n",
       "25002  this film lacked something i couldnt put my fi...  ...             pos\n",
       "25003  sorry everyone i know this is supposed to be a...  ...             pos\n",
       "25004  when i was little my parents took me along to ...  ...             neg\n",
       "...                                                  ...  ...             ...\n",
       "49995  seeing as the vote average was pretty low and ...  ...             pos\n",
       "49996  the plot had some wretched unbelievable twists...  ...             pos\n",
       "49997  i am amazed at how this movieand most others h...  ...             pos\n",
       "49998  a christmas together actually came before my t...  ...             pos\n",
       "49999  workingclass romantic drama from director mart...  ...             pos\n",
       "\n",
       "[25000 rows x 7 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "i = 0\n",
    "comp_value = []\n",
    "while i < len(imdb_df):\n",
    "    k = analyser.polarity_scores(imdb_df.iloc[i]['review'])\n",
    "    comp_value.append(k['compound'])\n",
    "    i += 1\n",
    "    \n",
    "comp_value = np.array(comp_value)\n",
    "imdb_df['Vader_score'] = comp_value\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(imdb_df):\n",
    "    if ((imdb_df.iloc[i]['Vader_score'] >= 0)):\n",
    "        predicted_value.append('pos')\n",
    "        i = i+1\n",
    "    elif ((imdb_df.iloc[i]['Vader_score'] < 0)):\n",
    "        predicted_value.append('neg')\n",
    "        i = i+1\n",
    "        \n",
    "imdb_df['Vader_predicted'] = predicted_value\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "CRDJHfRw5AQ5",
    "outputId": "dc3d5ddf-f11d-46b2-877d-dae2f0527c06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7342437337942956\n",
      "Accuracy score: 0.69252\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(imdb_df['label'], imdb_df['Vader_predicted'], pos_label='pos')\n",
    "m2 = accuracy_score(imdb_df['label'], imdb_df['Vader_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "DzFEJfJVAi3q"
   },
   "source": [
    "Different scenario happens here as the result generated by VADER for this dataset (F1 score: 0.734 and accuracy score: 0.693) are less accurate compared to TextBlob, this maybe due to VADER is weak in handling longer and more complex text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "mujoRrVVfAH4"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "colab_type": "code",
    "hidden": true,
    "id": "m0Pzsy3yb2lQ",
    "outputId": "29c67c5f-fba1-42f4-ccea-1e91b2ae9816"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>ratings</th>\n",
       "      <th>textblob_polarity</th>\n",
       "      <th>textblob_predicted</th>\n",
       "      <th>textblob_subjectivity</th>\n",
       "      <th>Vader_score</th>\n",
       "      <th>Vader_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>a crappy cardboard ghost of the original  hard...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.305556</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>-0.9052</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>we have this same game but it was made in  we ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081389</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.7964</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>hated this productpredictable  not fun  it att...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.194792</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.463542</td>\n",
       "      <td>-0.1205</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>i had high hopes for this game as i am a big f...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.024593</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.419584</td>\n",
       "      <td>0.9607</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>thought this was a book with pages to illustra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.189231</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.344808</td>\n",
       "      <td>-0.9230</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>my    yo love this puzzle  there is enough of ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.4118</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>my  year old got this last year and still love...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6888</th>\n",
       "      <td>this puzzle is very well made  the pieces are ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.177546</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.531812</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6889</th>\n",
       "      <td>we love the melissa and doug line we have abou...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.219123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.506364</td>\n",
       "      <td>0.8573</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6890</th>\n",
       "      <td>we bought this pirate ship puzzle along witht ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.229091</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.436818</td>\n",
       "      <td>0.7172</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  ...  Vader_predicted\n",
       "157   a crappy cardboard ghost of the original  hard...  ...              1.0\n",
       "165   we have this same game but it was made in  we ...  ...              5.0\n",
       "186   hated this productpredictable  not fun  it att...  ...              3.0\n",
       "191   i had high hopes for this game as i am a big f...  ...              5.0\n",
       "298   thought this was a book with pages to illustra...  ...              1.0\n",
       "...                                                 ...  ...              ...\n",
       "6884  my    yo love this puzzle  there is enough of ...  ...              4.0\n",
       "6887  my  year old got this last year and still love...  ...              5.0\n",
       "6888  this puzzle is very well made  the pieces are ...  ...              5.0\n",
       "6889  we love the melissa and doug line we have abou...  ...              5.0\n",
       "6890  we bought this pirate ship puzzle along witht ...  ...              5.0\n",
       "\n",
       "[22500 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "i = 0\n",
    "comp_value = []\n",
    "while i < len(amz_df):\n",
    "    k = analyser.polarity_scores(amz_df.iloc[i]['reviews'])\n",
    "    comp_value.append(k['compound'])\n",
    "    i += 1\n",
    "    \n",
    "comp_value = np.array(comp_value)\n",
    "amz_df['Vader_score'] = comp_value\n",
    "\n",
    "i = 0\n",
    "predicted_value = [ ] \n",
    "while i < len(amz_df):\n",
    "    if ((amz_df.iloc[i]['Vader_score'] > 0.6)):\n",
    "        predicted_value.append(5.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['Vader_score'] <= 0.6 and amz_df.iloc[i]['Vader_score'] > 0.2)):\n",
    "        predicted_value.append(4.0)\n",
    "        i = i+1 \n",
    "    elif ((amz_df.iloc[i]['Vader_score'] <= 0.2 and amz_df.iloc[i]['Vader_score'] > -0.2)):\n",
    "        predicted_value.append(3.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['Vader_score'] <= -0.2 and amz_df.iloc[i]['Vader_score']> -0.6)):\n",
    "        predicted_value.append(2.0)\n",
    "        i = i+1\n",
    "    elif ((amz_df.iloc[i]['Vader_score'] <= -0.6)):\n",
    "        predicted_value.append(1.0)\n",
    "        i = i+1         \n",
    "amz_df['Vader_predicted'] = predicted_value\n",
    "amz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "3VtRQdFHUn3z",
    "outputId": "2e64cb98-eadb-4a2b-9c7f-503c47145e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.23950762351467847\n",
      "Accuracy score: 0.2876444444444444\n"
     ]
    }
   ],
   "source": [
    "m1 = f1_score(amz_df['ratings'], amz_df['Vader_predicted'], average='macro')\n",
    "m2 = accuracy_score(amz_df['ratings'], amz_df['Vader_predicted'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "KjYluguAAi30"
   },
   "source": [
    "Again, the result produced by VADER for this dataset is almost the same situation as the second dataset, where it's less accurate compared to using TextBlob. In short, VADER really shines in analysing social media text like the Twitter dataset. However, VADER don't do well in handling longer text and complex sentence like in the movie review dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "bC2ZbH5vAi31"
   },
   "source": [
    "# 3. Losgistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "a1iBt9ap1GDW"
   },
   "source": [
    "## Why not Linear Regression?\n",
    "    \n",
    "Linear regression is unbounded as the predicted value (output) can exceed 0 and 1 range. This brings logistic regression into picture as their ouput value strictly ranges from 0 to 1, which is suitable for classification problem.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/2900/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Logistic regression is a classification algorithm used to assign observations (input) to a discrete set of classes (often in binary). Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign. \n",
    "\n",
    "Logistic regression transforms its output using the logistic sigmoid function to return a probability value. Sigmoid function maps any real value into another value between 0 and 1, just like probability. \n",
    "![alt text](https://miro.medium.com/max/800/1*OUOB_YF41M-O4GgZH_F2rw.png)\n",
    "\n",
    "## Formula of sigmoid function\n",
    "\n",
    "![alt text](https://miro.medium.com/max/339/1*Gp5E23P5d2PY5D5kOo8ePw.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## How sigmoid function is derived though?\n",
    "\n",
    "It is derived from the log(odds) function, log [p/(1-p)].\n",
    "\n",
    "Odds is defined as probability of success/probability of failure. So the odds of a success (80% chance of rain) has an accompanying odds of failure (20% chance it doesn’t rain); as an equation (the “odds ratio“), that’s .8/.2 = 4. Conversion to log odds results in symmetry around zero, which is easier for analysis.\n",
    "\n",
    "The name 'logistic regression' can be justified as data is fit into linear regression model, which then applied by a logistic (sigmoid) function to produce binary outcome and predict the target categorical dependent variable.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Pros\n",
    "1. doesn't require high computation power\n",
    "\n",
    "2. easy to implement, interpret and very efficient to train. \n",
    "\n",
    "Cons\n",
    "1. Main limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world,the data is rarely linearly separable. Most of the time data would be a jumbled mess.\n",
    "\n",
    "2. If the number of features are less than the number of classes, it \n",
    "tends to overfit.\n",
    "\n",
    "3. will not perform well with independent variables that are not correlated to the target variable\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "One way to prevent overfitting is to perform cross validation. The idea is to\n",
    "\n",
    "1. partition your data into training data and testing data (sometimes called validation data); treat testing data as unobserved, and\n",
    "\n",
    "2. fit your model using only the training data.\n",
    "\n",
    "3. evaluate your model on the testing data that you held out earlier, compare with the actual results, and obtain a testing error.\n",
    "\n",
    "Repeat the process K times then take average of the testing errors as a final performance measure.\n",
    "\n",
    "Reference:\n",
    "1. https://towardsdatascience.com/sentiment-analysis-with-python-part-2-4f71e7bde59a\n",
    "\n",
    "2. https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148\n",
    "\n",
    "3. https://machinelearningmastery.com/logistic-regression-for-machine-learning/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "GTKikVnhAi33"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "yhf2RZC_5UWE",
    "outputId": "494903f8-db4f-4151-9d6d-2c90d7494565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.750875\n",
      "Accuracy for C=0.05: 0.770025\n",
      "Accuracy for C=0.25: 0.77995\n",
      "Accuracy for C=0.5: 0.78075\n",
      "Accuracy for C=1: 0.780975\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#using n-grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2), \n",
    "                                   stop_words=\"english\", \n",
    "                                   binary=True)\n",
    "ngram_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = ngram_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "rv0ZImX9PZxO",
    "outputId": "b93f240a-c1b9-4fd2-9d41-b2b7aa8ef755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.757575\n",
      "Accuracy for C=0.05: 0.7716\n",
      "Accuracy for C=0.25: 0.780525\n",
      "Accuracy for C=0.5: 0.782025\n",
      "Accuracy for C=1: 0.78225\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1,3),\n",
    "                                stop_words=\"english\")\n",
    "wc_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = wc_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "8Xlkg3z_Q5Cw",
    "outputId": "c9cdd1a2-f146-4628-a31f-f64b01d8c1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.7248\n",
      "Accuracy for C=0.05: 0.740175\n",
      "Accuracy for C=0.25: 0.7606\n",
      "Accuracy for C=0.5: 0.768975\n",
      "Accuracy for C=1: 0.7742\n"
     ]
    }
   ],
   "source": [
    "#using tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = tfidf_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "N_tt_Z0CAi4T"
   },
   "source": [
    "For Twitter dataset, the best accuracy score falls around the value of 0.78 for different pre-processing methods used and count vectorizer works better in this case. Another thing to take note is that the accuracy score is higher compared to using TextBlob and VADER.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ZsIsEjYLeJsd"
   },
   "source": [
    "### 2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "oh_OW-wIPEfA",
    "outputId": "58fdbfb6-27fc-4974-8993-7aad6ef0d6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.871\n",
      "Accuracy for C=0.05: 0.8758\n",
      "Accuracy for C=0.25: 0.8788\n",
      "Accuracy for C=0.5: 0.879\n",
      "Accuracy for C=1: 0.8796\n"
     ]
    }
   ],
   "source": [
    "#using n-grams only\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words=\"english\", \n",
    "                                   binary=True)\n",
    "ngram_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = ngram_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "alk1WbXy02Gy",
    "outputId": "3ea99e10-e06c-4918-e50e-0c04a79d1874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.8806\n",
      "Accuracy for C=0.05: 0.8856\n",
      "Accuracy for C=0.25: 0.8886\n",
      "Accuracy for C=0.5: 0.8878\n",
      "Accuracy for C=1: 0.8878\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words=\"english\")\n",
    "wc_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = wc_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8, )\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "-XqOc6ZvQcz9",
    "outputId": "7a2f6e40-84c8-4829-af4b-4aab218bb2c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.79\n",
      "Accuracy for C=0.05: 0.8116\n",
      "Accuracy for C=0.25: 0.8396\n",
      "Accuracy for C=0.5: 0.855\n",
      "Accuracy for C=1: 0.8664\n"
     ]
    }
   ],
   "source": [
    "#using tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = tfidf_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8, )\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Z7XMyZhYAi4m"
   },
   "source": [
    "Logistic regression works quite well for movie review dataset as the accuracy score is within the range of 0.87 and 0.89 for different pre-processing methods used and again, count vectorizer works best in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "8BNmhN-tfDjd"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "o0XNL3VDcK5t",
    "outputId": "2dc6dfcd-1c83-4b87-e9c2-698a6ab15b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.4915555555555556\n",
      "Accuracy for C=0.05: 0.5068888888888889\n",
      "Accuracy for C=0.25: 0.5008888888888889\n",
      "Accuracy for C=0.5: 0.49822222222222223\n",
      "Accuracy for C=1: 0.49644444444444447\n"
     ]
    }
   ],
   "source": [
    "#using n-grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words=\"english\", \n",
    "                                   binary=True)\n",
    "ngram_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = ngram_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000, multi_class='multinomial')\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "SHMOW59gclsy",
    "outputId": "cd29af54-9c9e-49a2-c75b-32b17c44c827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.4948888888888889\n",
      "Accuracy for C=0.05: 0.5044444444444445\n",
      "Accuracy for C=0.25: 0.5035555555555555\n",
      "Accuracy for C=0.5: 0.5024444444444445\n",
      "Accuracy for C=1: 0.49977777777777777\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words=\"english\")\n",
    "wc_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = wc_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "NgcQPLCKc-KI",
    "outputId": "a01f1f04-8968-489b-cafa-8740cb134489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.4448888888888889\n",
      "Accuracy for C=0.05: 0.46111111111111114\n",
      "Accuracy for C=0.25: 0.484\n",
      "Accuracy for C=0.5: 0.494\n",
      "Accuracy for C=1: 0.504\n"
     ]
    }
   ],
   "source": [
    "#using tf-idf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = tfidf_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "lNQelh6ZAi4u"
   },
   "source": [
    "Logistic regression don't do well in predicting the sentiment of this dataset with just getting accuracy score of around 0.50. However, it's doing significantly better than TextBlob and Vader (0.23 and 0.29 respectively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "MSChuc_KB3QJ"
   },
   "source": [
    "### 4. SST2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Dgbb100EB2Vm",
    "outputId": "5d9afd05-b0fc-48da-b29b-20c99dad906f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.6604046242774566\n",
      "Accuracy for C=0.05: 0.7413294797687862\n",
      "Accuracy for C=0.25: 0.755057803468208\n",
      "Accuracy for C=0.5: 0.7579479768786127\n",
      "Accuracy for C=1: 0.7608381502890174\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#using n-grams\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2), \n",
    "                                   stop_words=\"english\", \n",
    "                                   binary=True)\n",
    "ngram_vectorizer.fit(sst_df[\"review\"])\n",
    "X = ngram_vectorizer.transform(sst_df[\"review\"])\n",
    "target = sst_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "j3_LUD4lFdz1",
    "outputId": "abd89cc4-943a-4caa-caa2-2e52bfad413d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.6734104046242775\n",
      "Accuracy for C=0.05: 0.7427745664739884\n",
      "Accuracy for C=0.25: 0.7752890173410405\n",
      "Accuracy for C=0.5: 0.7695086705202312\n",
      "Accuracy for C=1: 0.7680635838150289\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words=\"english\")\n",
    "wc_vectorizer.fit(sst_df[\"review\"])\n",
    "X = wc_vectorizer.transform(sst_df[\"review\"])\n",
    "target = sst_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "FoT6euXBFnKL",
    "outputId": "6b880e2d-3b63-4c59-c73d-1b5f6d7b3f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.5187861271676301\n",
      "Accuracy for C=0.05: 0.5317919075144508\n",
      "Accuracy for C=0.25: 0.6719653179190751\n",
      "Accuracy for C=0.5: 0.7312138728323699\n",
      "Accuracy for C=1: 0.7666184971098265\n"
     ]
    }
   ],
   "source": [
    "#using tf-idf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(sst_df[\"review\"])\n",
    "X = tfidf_vectorizer.transform(sst_df[\"review\"])\n",
    "target = sst_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    lr = LogisticRegression(C=c, max_iter=1000)\n",
    "    lr.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "GYI627ymAi4v"
   },
   "source": [
    "# 4. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "hLIfWDa21QF7"
   },
   "source": [
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. This theorem provides a way of calculating a type or probability called posterior probability, in which the probability of a event A (like the review is positive/negative) occurring is reliant on probabilistic known background (e.g. event B evidence).\n",
    "\n",
    "![alt text](https://miro.medium.com/max/918/1*LB-G6WBuswEfpg20FMighA.png)\n",
    "\n",
    "## But why is it called ‘Naive’?\n",
    "\n",
    "Naive Bayes classifier assumes that all the features are unrelated to each other. Presence or absence of a feature does not influence the presence or absence of any other feature. In real datasets, we test a hypothesis given multiple evidence(feature). So, calculations become complicated. To simplify the work, the feature independence approach is used to ‘uncouple’ multiple evidence and treat each as an independent one.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## How Naive Bayes works?\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1QR2fjZtuVEyYhDllWTt3U-PS4qnc2CFd)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Pros\n",
    "1. easy and fast to predict class of test data set. It is also a highly scalable algorithm.\n",
    "\n",
    "2. can be used for Binary and Multiclass classification\n",
    "\n",
    "3. performs better compare to other models like logistic regression and you need less training data (p.s: if the assumption of independence holds)\n",
    "\n",
    "4. perform well in case of categorical input variables compared to numerical variable(s)\n",
    "\n",
    "Cons\n",
    "1. If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency\" (p.s: can be solved by smoothing technique, such as Laplace smoothing, basically add 1 to every count so it’s never zero)\n",
    "\n",
    "2. Naive Bayes is based on the asssumption of the independence of predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
    "\n",
    "3. Naive Bayes can learn individual features importance but can’t determine the relationship among features.\n",
    "\n",
    "Reference: \n",
    "1. https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/\n",
    "\n",
    "2. https://levelup.gitconnected.com/movie-review-sentiment-analysis-with-naive-bayes-machine-learning-from-scratch-part-v-7bb869391bab\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "8k9ddreLAi4w"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ye9msFer1T4T",
    "outputId": "6f054760-62db-4db0-a5a2-7ef6ab8a6d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7604726100966702\n",
      "Accuracy score: 0.771425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words='english')\n",
    "wc_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = wc_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "w9aY4nqQtHMO",
    "outputId": "12f0f0fd-c7f7-451e-a549-4e2d0a6b7e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.7645965821360341\n",
      "Accuracy score: 0.775125\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = tfidf_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "lhkkDDjIAi49"
   },
   "source": [
    "Naive Bayes is getting around the same accuracy with Logistic Regression with the accuracy score around 0.77 and TF-IDF works better in this case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "82cOeTLGU2lR"
   },
   "source": [
    "### 2. IMDB movie review dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "R4jtdX6OZdMH",
    "outputId": "1e872a8b-09e1-446d-af7b-66e542b8e359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.8666114333057168\n",
      "Accuracy score: 0.8712\n"
     ]
    }
   ],
   "source": [
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words='english')\n",
    "wc_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = wc_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "XmGsJhSpaGW8",
    "outputId": "2e4789ca-74cd-436d-d04e-9cac9f9e3c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.878323932312651\n",
      "Accuracy score: 0.8792\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                  stop_words='english')\n",
    "tfidf_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = tfidf_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, pos_label='pos')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "n27uWN12Ai5c"
   },
   "source": [
    "As the same as the first dataset, Naive Bayes achieces almost the same accuracy as Logistic Regression with accuracy score of 0.88 and TF-IDF works better in this case too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "dNjjGfAgfV9g"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "aF7Z0tIYapZD",
    "outputId": "3f63f5fc-3e64-47d0-d551-68aef419be05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.46494102319457725\n",
      "Accuracy score: 0.46555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "wc_vectorizer = CountVectorizer(ngram_range=(1, 3),\n",
    "                                stop_words='english')\n",
    "wc_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = wc_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, average='macro')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "KDbAb3goa8mj",
    "outputId": "f95501f7-a2d5-4e9a-c346-e3ebd930a12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.49659489818015884\n",
      "Accuracy score: 0.506\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                                   stop_words='english')\n",
    "tfidf_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = tfidf_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "m1 = f1_score(y_test, y_pred, average='macro')\n",
    "m2 = accuracy_score(y_test, y_pred)\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "h4hDc8-fAi5q"
   },
   "source": [
    "In conclusion, the accuracy of Naive Bayes are almost the same as Logistic Regression in three of the datasets. One thing to notice is that Naive Bayes tends to work better with TF-IDF than count vectorizer. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "p3G2UNKdAi5r"
   },
   "source": [
    "\n",
    "# 5. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "32TJctR0VLVN"
   },
   "source": [
    "## What is Support Vector Machine?\n",
    "\n",
    "The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space (N — the number of features) that distinctly classifies the data points.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/375/0*9jEWNXTAao7phK-5.png)\n",
    "![alt text](https://miro.medium.com/max/375/0*0o8xIA4k3gXUDCFU.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is hyperplane?\n",
    "\n",
    "To separate the two classes of data points, there are many possible hyperplanes that could be chosen. Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
    "\n",
    "\n",
    "![alt text](https://miro.medium.com/max/944/0*ecA4Ls8kBYSM5nza.jpg)\n",
    "\n",
    "## What is support vectors?\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM.\n",
    "\n",
    "Pros:\n",
    "\n",
    "1. It is effective and accurate in the higher dimensional spaces.\n",
    "\n",
    "2. Effective when the number of features are more than training samples, thus works well in small  datasets. (still prone to overfitting, if number of features is much greater than the number of samples)\n",
    "\n",
    "Cons:\n",
    "\n",
    "1. SVMs are not very efficient computationally, thus if your dataset is very big, it takes large amount of time to process.\n",
    "\n",
    "2. Less effective on noisier datasets with overlapping classes.\n",
    "\n",
    "Reference:\n",
    "1. https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n",
    "\n",
    "2. https://towardsdatascience.com/support-vector-machines-svm-c9ef228\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "6m52FuKHAi5t"
   },
   "source": [
    "### 1. Twitter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "iERkO7lcUgRk",
    "outputId": "763c0848-9a80-41d9-f327-b057eb32f945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.777925\n",
      "Accuracy for C=0.05: 0.784075\n",
      "Accuracy for C=0.25: 0.78105\n",
      "Accuracy for C=0.5: 0.77915\n",
      "Accuracy for C=1: 0.774025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#using SVM model\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words='english')\n",
    "ngram_vectorizer.fit(tweet_df[\"text\"])\n",
    "X = ngram_vectorizer.transform(tweet_df[\"text\"])\n",
    "target = tweet_df[\"polarity\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.8)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:   \n",
    "    svm = LinearSVC(C=c)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, svm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "TgAOB00ZVos0"
   },
   "source": [
    "### 2. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "R-4mCciLQsdr",
    "outputId": "6a0a883f-7217-4544-8a2c-e8ac6f23b5ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.88432\n",
      "Accuracy for C=0.05: 0.88304\n",
      "Accuracy for C=0.25: 0.88288\n",
      "Accuracy for C=0.5: 0.88256\n",
      "Accuracy for C=1: 0.88256\n"
     ]
    }
   ],
   "source": [
    "#using SVM model\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words='english')\n",
    "ngram_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = ngram_vectorizer.transform(imdb_df[\"review\"])\n",
    "target = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.75)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:   \n",
    "    svm = LinearSVC(C=c, max_iter=3000)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, svm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "4ZGbf8L3Vxom"
   },
   "source": [
    "### 3. Amazon's Products review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "743brpXydTCt",
    "outputId": "040f2a40-91c8-4ac3-b614-2947abd2c7d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.5032888888888889\n",
      "Accuracy for C=0.05: 0.49777777777777776\n",
      "Accuracy for C=0.25: 0.4912\n",
      "Accuracy for C=0.5: 0.4885333333333333\n",
      "Accuracy for C=1: 0.488\n"
     ]
    }
   ],
   "source": [
    "#using SVM model\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 3), \n",
    "                                   stop_words='english')\n",
    "ngram_vectorizer.fit(amz_df[\"reviews\"])\n",
    "X = ngram_vectorizer.transform(amz_df[\"reviews\"])\n",
    "target = amz_df[\"ratings\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, train_size = 0.75)\n",
    "\n",
    "for c in [0.01, 0.05, 0.25, 0.5, 1]:    \n",
    "    svm = LinearSVC(C=c, max_iter=5000)\n",
    "    svm.fit(X_train, y_train)\n",
    "    print (\"Accuracy for C=%s: %s\" \n",
    "           % (c, accuracy_score(y_test, svm.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "7rh59qY_Ai54"
   },
   "source": [
    "# B)Deep Learning Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ARqGNmH1bNFo"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Deep learning is an AI function that imitates the workings of the human brain in processing data and creating patterns for use in decision making.\n",
    "\n",
    "Deep learning, a subset of machine learning, utilizes artificial neural networks to carry out the process of machine learning. The artificial neural networks are built like the human brain, with neuron nodes connected together like a web. While traditional programs build analysis with data in a linear way, the hierarchical function of deep learning systems enables machines to process data with a nonlinear approach.\n",
    "\n",
    "Neural networks are a series of algorithms that mimic the operations of a human brain to recognize relationships between vast amounts of data. There are tons of different neural networks, such as:\n",
    "\n",
    "1. Feed Forward (FF)\n",
    "\n",
    "2. Recurrent Neural Network (RNN)\n",
    "\n",
    "3. Long Short Term Memory (LSTM) \n",
    "\n",
    "4. Transformer etc.\n",
    "\n",
    "Reference: https://www.investopedia.com/terms/d/deep-learning.asp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "I9OPyX-fAi55"
   },
   "source": [
    "## What is Recurent Neural Network (RNN)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "5h8RNMtEAi56"
   },
   "source": [
    "\n",
    "\n",
    "![alt text](https://miro.medium.com/max/250/1*L38xfe59H5tAgvuIjKoWPg.png)\n",
    "\n",
    "In the figure above, we see part of the neural network, A, processing some input x_t and outputs h_t. h_t is also known as the hidden state, which preserve short term memory and allows information to be passed from one step to the next. A RNN can be thought of as multiple copies of the same network, A, each network passing a message to a successor as below:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/3383/1*NKhwsOYNUT5xU7Pyf6Znhg.png)\n",
    "\n",
    "The following picture shows how usually a sequence to sequence model works using RNNs. Each word is processed separately, and the resulting sentence is generated by passing a hidden state to the decoding stage that, then, generates the output.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/750/1*8GcdjBU5TAP36itWBcZ6iA.gif)\n",
    "\n",
    "## The inside of RNNs:\n",
    "\n",
    "![alt text](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "SeXJOYnIAi57"
   },
   "source": [
    "## What is the problem with RNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "xjWWP-wSAi57"
   },
   "source": [
    "\n",
    "\n",
    "RNNs become very ineffective when the gap between the relevant information and the point where it is needed become very large. That is due to the fact that the information is passed at each step and the longer the chain is, the more probable the information is lost along the chain.\n",
    "\n",
    "For example,  let’s say that you are trying to predict the last word of the text: “I grew up in France… I speak fluent …”. Recent information suggests that the next word is probably a language, but if we want to narrow down which language, we need context of France, that is further back in the text. Thus, RNN can't predict well when the information gap is too big. \n",
    "\n",
    "![alt text](https://miro.medium.com/max/3154/1*a5EbLhyxbPR78PhiV5Esjg.png)\n",
    "\n",
    "This common problem with RNN in which RNN can't handle long term dependecies well is due to vanishing gradient. Basically it means your gradient are getting lower and lower each time you propagate back to minimise your error and update your weight. The information being passed down are lost along the chain. For detailed explaination, click [here](https://www.superdatascience.com/blogs/recurrent-neural-networks-rnn-the-vanishing-gradient-problem). The invention  of Long-Short Term Memory (LSTM) network greatly addressed to this issue faced by RNN.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "BZ0XJc_YAi57"
   },
   "source": [
    "## What is LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Hx7tSFUGAi58"
   },
   "source": [
    "\n",
    "\n",
    "When arranging one’s calendar for the day, we prioritize our appointments. If there is anything important, we can cancel some of the meetings and accommodate what is important.\n",
    "\n",
    "RNNs don’t do that. Whenever it adds new information, it transforms existing information completely by applying a function. The entire information is modified, and there is no consideration of what is important and what is not.\n",
    "\n",
    "LSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states, which preserve long term memory. In this way, LSTMs can selectively remember or forget things that are important and not so important.\n",
    "\n",
    "The structure of LSTM is as below:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1305/1*MwU5yk8f9d6IcLybvGgNxA.jpeg)\n",
    "\n",
    "## How LSTM works?\n",
    "\n",
    "Each cell takes as inputs x_t (a word in the case of a sentence to sentence translation), the previous cell state c_t-1 and the output of the previous cell h_t-1. It manipulates these inputs and based on them through 3 gates (forget gate, input gate and output gate) which function differently. Afterwards, it generates a new cell state c_t, and an output h_t to pass to the next cell.\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=14TPKg6sIShWKzKSuXPl1s33s-Ufk86wB)\n",
    "\n",
    "To have futher insight on how it performs in action, we will look at ULMFit model which utilise the LSTM network.\n",
    "\n",
    "Reference: \n",
    "\n",
    "1. https://towardsdatascience.com/transformers-141e32e69591\n",
    "\n",
    "2. http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "wkLl9LeiAi7f"
   },
   "source": [
    "# Word Embeddings to Pretrained Language Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![alt text](https://miro.medium.com/max/1375/1*ff_bprXLuTueAx7-5-MHew.png)\n",
    "\n",
    "Before methods like ELMo and BERT, pretraining in NLP was limited to word embeddings such as word2vec and GloVe. Word embeddings mapped each word to a vector that represented some aspects of its meaning (e.g. the vector for \"King\" would include information about status, gender, etc.). Word embeddings are generally trained on large, unlabeled corpora (such as the Wikipedia dump), and then used to train models on labeled data for downstream tasks such as sentiment analysis. This allows the downstream models to leverage linguistic information that is learned from larger datasets. Word embeddings were shown to be almost universally useful across a wide range of tasks, but there were many limitations to this simple method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Static vs. Dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "3-5fedAzAi7f"
   },
   "source": [
    "1. Static Word Embeddings fail to capture polysemy. They generate the same embedding for the same word in different contexts. Contextualized (Dynamic) words embeddings aim at capturing word semantics in different contexts to address the issue of polysemous and the context-dependent nature of words. For instance, the word \"bank\" can have a different meaning depending on the context (e.g. \"I stole money from the bank\" vs. \"The bank of the river overflowed with water\"). However, traditional word embedding methods only allocate a single vector for each word, which is forced to represent this wide range of meanings.\n",
    "\n",
    "\n",
    "2. Static Word Embeddings could only leverage off the vector outputs from unsupervised models for downstream tasks — not the unsupervised models themselves.They were mostly shallow models to begin with and were often discarded after training (e.g. word2vec, Glove) The output of Contextualized (Dynamic) Word Embedding training is the trained model and vectors — not just vectors.\n",
    "\n",
    "\n",
    "3. Traditional word vectors are shallow representations (a single layer of weights, known as embeddings). They only incorporate previous knowledge in the first layer of the model. The rest of the network still needs to be trained from scratch for a new target task. They fail to capture higher-level information that might be even more useful. Word embeddings are useful in only capturing semantic meanings of words but we also need to understand higher level concepts like anaphora, long-term dependencies, agreement, negation, and many more.\n",
    "\n",
    "These limitations have motivated the use of deep language models (language models that use architectures like LSTMs) for transfer learning. Instead of just training a model to map a single vector for each word, these methods train a complex, deep neural network to map a vector to each word based on the entire sentence/surrounding context. Though the precise methods are different, ELMo, ULMFiT, and BERT are all examples of this idea in action. The basic idea is to\n",
    "\n",
    "1. Train a deep language model\n",
    "\n",
    "\n",
    "2. Use the representations learned by the language model in downstream tasks\n",
    "\n",
    "\n",
    "Ps: Word2vec output, which is just word vectors (aka language representation), captures semantic similarity between words. Word2vec and language models are almost complementary in a sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "qIpEDbnuAi7g"
   },
   "source": [
    "## Basic idea of language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Es9msk_dAi7g"
   },
   "source": [
    "Language modeling - although it sounds formidable - is essentially just predicting words in a blank. More formally, given a context, a language model predicts the probability of a word occurring in that context. For instance, given the following context\n",
    "\n",
    "    \"The _____ sat on the mat\"\n",
    "\n",
    "where _____ is the word we are trying to predict, a language model might tell us that the word \"cat\" would fill the blank 50% of the time, \"dog\" would fill the blank 20% of the time, etc.\n",
    "\n",
    "Language models have generally been trained from \"left to right\". They are given a sequence of words, then have to predict the next word.\n",
    "\n",
    "For instance, if the network is given the sequence\n",
    "\n",
    "    \"Which Sesame Street\"\n",
    "\n",
    "the network is trained to predict what word comes next. This approach is effective when we actually want to generate sentences. We can predict the next word, append that to the sequence, then predict the next word, etc..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "uuspow7zYvz-"
   },
   "source": [
    "## How a Language Model is built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "c3eIxdghYvlS"
   },
   "source": [
    "### Types of Language Models\n",
    "There are primarily two types of Language Models:\n",
    "\n",
    "1. Statistical Language Models: These models use traditional statistical techniques like N-grams, Hidden Markov Models (HMM) and certain linguistic rules to learn the probability distribution of words\n",
    "\n",
    "\n",
    "2. Neural Language Models: These are new players in the NLP town and have surpassed the statistical language models in their effectiveness. They use different kinds of Neural Networks (eg: RNN and Transformer) to model language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "3HOF7NXIYvW3"
   },
   "source": [
    "### Building a Statistical Language Model (N-grams)\n",
    "\n",
    "![alt text](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/LM1.jpg)\n",
    "\n",
    "An N-gram language model predicts the probability of a given N-gram within any sequence of words in the language. If we have a good N-gram model, we can predict p(w | h) – what is the probability of seeing the word w given a history of previous words h – where the history contains n-1 words.\n",
    "\n",
    "We must estimate this probability to construct an N-gram model.\n",
    "\n",
    "We compute this probability in two steps:\n",
    "\n",
    "1. Apply the chain rule of probability\n",
    "\n",
    "2. We then apply a very strong simplification assumption to allow us to compute p(w1…ws) in an easy manner\n",
    "\n",
    "The chain rule of probability is:\n",
    "\n",
    "    p(w1...ws) = p(w1) . p(w2 | w1) . p(w3 | w1 w2) . p(w4 | w1 w2 w3) ..... p(wn | w1...wn-1)\n",
    "\n",
    "So what is the chain rule? It tells us how to compute the joint probability of a sequence by using the conditional probability of a word given previous words.\n",
    "\n",
    "But we do not have access to these conditional probabilities with complex conditions of up to n-1 words. So how do we proceed?\n",
    "\n",
    "This is where we introduce a simplification assumption. We can assume for all conditions, that:\n",
    "\n",
    "    p(wk | w1...wk-1) = p(wk | wk-1)\n",
    "\n",
    "Here, we approximate the history (the context) of the word wk by looking only at the last word of the context. This assumption is called the Markov assumption. (We used it here with a simplified context of length 1 – which corresponds to a bigram model – we could use larger fixed-sized histories in general)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ZfzH9AA5djK2"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurance  \n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    " \n",
    "# Transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Pr18h2LKdlT0"
   },
   "source": [
    "The code above is pretty straightforward. We first split our text into trigrams with the help of NLTK and then calculate the frequency in which each combination of the trigrams occurs in the dataset.\n",
    "\n",
    "We then use it to calculate probabilities of a word, given the previous two words. That’s essentially what gives us our Language Model!\n",
    "\n",
    "Example predictions:\n",
    "\n",
    "![alt text](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/08/Screenshot-from-2019-08-07-05-44-22.png)\n",
    "\n",
    "### Limitations of N-gram approach to Language Modeling\n",
    "\n",
    "N-gram based language models do have a few drawbacks:\n",
    "\n",
    "1. The higher the N, the better is the model usually. But this leads to lots of computation overhead that requires large computation power in terms of RAM\n",
    "\n",
    "2. N-grams are a sparse representation of language. This is because we build the model based on the probability of words co-occurring but not capturing the semantic meaning of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Y_EjEnfVdlHD"
   },
   "source": [
    "### Building a Neural Language Model\n",
    "\n",
    "The problem statement is to train a language model on the given text and then generate text given an input text in such a way that it looks straight out of this document and is grammatically correct and legible to read.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Cleaning text\n",
    "  \n",
    "    Transform the raw text into a sequence of tokens or words that we can use as a source to train the model. Eg:\n",
    "\n",
    "    a. Replace ‘–‘ with a white space so we can split words better.\n",
    "\n",
    "    b. Split the words based on white space.\n",
    "\n",
    "    c. Remove all punctuation from words to reduce the vocabulary size \n",
    "\n",
    "    d. Normalize all words to lowercase to reduce the vocabulary size.\n",
    "\n",
    "    Vocabulary size is a big deal with language modeling. A smaller vocabulary results in a smaller model that trains faster.\n",
    "\n",
    "\n",
    "2. Creating Sequences\n",
    "\n",
    "    Take in 10 words as context and ask the model to predict the next words. 10 is a number by trial and error and you can experiment with it too. You essentially need enough words in the input sequence that your model is able to get the context.\n",
    "\n",
    "\n",
    "3. Encoding Sequences\n",
    "\n",
    "    The word embedding layer expects input sequences to be comprised of integers. We can map each word in our vocabulary to a unique integer and encode our input sequences. Later, when we make predictions, we can convert the prediction to numbers and look up their associated words in the same mapping.\n",
    "\n",
    "    To do this encoding, we can use the Tokenizer class in the HuggingFace. The Tokenizer must be trained on the entire training dataset, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
    "\n",
    "\n",
    "\n",
    "4. Model Building\n",
    "     \n",
    " a. Embedding layer of to learn a 50 dimension embedding for each words. This helps the model in understanding complex relationships between characters. Used a GRU layer (similar to LSTM, variant of RNN)  as the base model, which has 150 timesteps. Finally, a Dense layer is used with a softmax activation for prediction.\n",
    "\n",
    " b. The model is compiled specifying the categorical cross entropy loss needed to fit the model. Technically, the model is learning a multi-class classification and this is the suitable loss function for this type of problem. The efficient Adam implementation to mini-batch gradient descent is used and accuracy is evaluated of the model.\n",
    "\n",
    " c. The model is fit on the data for 100 training epochs with a modest batch size of 128 to speed things up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "rUhWKbC33lKA"
   },
   "outputs": [],
   "source": [
    "#define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
    "model.add(LSTM(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "\n",
    "#fit the model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ZRjhLEF4lFWh"
   },
   "source": [
    "During training, you will see a summary of performance, including the loss and accuracy evaluated from the training data at the end of each batch update.\n",
    "\n",
    "You will get different results, but perhaps an accuracy of just over 50% of predicting the next word in the sequence, which is not bad. We are not aiming for 100% accuracy (e.g. a model that memorized the text), but rather a model that captures the essence of the text.\n",
    "\n",
    "Example output:\n",
    "   \n",
    "    Epoch 96/100\n",
    "    118633/118633 [==============================] - 265s - loss: 2.0324 - acc: 0.5187\n",
    "\n",
    "    Epoch 97/100\n",
    "    118633/118633 [==============================] - 265s - loss: 2.0136 - acc: 0.5247\n",
    "\n",
    "    Epoch 98/100\n",
    "    118633/118633 [==============================] - 267s - loss: 1.9956 - acc: 0.5262\n",
    "\n",
    "    Epoch 99/100\n",
    "    118633/118633 [==============================] - 266s - loss: 1.9812 - acc: 0.5291\n",
    "\n",
    "    Epoch 100/100\n",
    "    118633/118633 [==============================] - 270s - loss: 1.9709 - acc: 0.5315\n",
    "    \n",
    "\n",
    "Reference:\n",
    "\n",
    "1. https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/\n",
    "\n",
    "\n",
    "2. https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "\n",
    "\n",
    "3. https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "D6L5gPBqAi58"
   },
   "source": [
    "# 1. ULMFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "6cZKwl26jXVL"
   },
   "source": [
    "\n",
    "\n",
    "The idea of using generative pretrained LM + task-specific fine-tuning was first explored in ULMFiT (Howard & Ruder, 2018), the base model is AWD-LSTM. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## ULMFit Model Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following techniques are applied from the ULMFit paper to achieve better accuracy:\n",
    "1. Discriminative fine-tuning\n",
    "\n",
    "    Motivated by the fact that different layers of LM capture different types of information. ULMFiT proposed to tune each layer with different learning rates, {η1,…,ηℓ,…,ηL}, where η is the base learning rate for the first layer, ηℓ is for the ℓ-th layer and there are L layers in total.\n",
    "\n",
    "2. 1-cycle learning rate policy\n",
    "\n",
    "    Allows a large initial learning rate (LR_{max}=10^{-3}, for example), but decreases it by several orders of magnitude just at the last epoch. This seems to provide greater final accuracy. In the ULMFit implementation, this 1-cycle policy has been tweaked and is referred to as slanted triangular learning rate.\n",
    "\n",
    "3. Gradual unfreezing\n",
    "\n",
    "    Rather than training all the layers at once during classification, the layers are \"frozen\" and the last layer is fine-tuned first, followed by the next layer before it, and so on. This avoids the phenomenon known as catastrophic forgetting (by fine-tuning all layers too aggressively).\n",
    "\n",
    "4. Concatenated pooling\n",
    "\n",
    "    Because an input text can consist of hundreds or thousands of words, information might get lost if we only consider the last hidden state.\n",
    "\n",
    "    Hence, the hidden state at the last time step, h_T is concatenated with both the max-pooled and mean-pooled representation of the hidden states over as many time steps as can fit in GPU memory.\n",
    "\n",
    "    h_C = [h_T, maxpool(H), meanpool(H)]\n",
    "\n",
    "    Where H is the vector of all hidden states.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 3-stage fine-tuning methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The classification task is done in a 3-stage process:\n",
    "\n",
    "1. General-domain LM pretraining: \n",
    "\n",
    "    ULMFit has a pretrained model generated using an AWD-LSTM to develop a language model called Wikitext-103 and was trained of 28,595 preprocessed Wikipedia articles, totalling to 103 million words.\n",
    "\n",
    "\n",
    "2. Target task LM fine-tuning: \n",
    "\n",
    "    Since the target data which we are trying to analyse, will likely come from a different distribution, ULMFit allows us to use the pre-trained language model and fine-tune it (using the above techniques) to adapt to the different context of the target data.\n",
    "    \n",
    "    \n",
    "3. Target task classifier fine-tuning: \n",
    "    Once we save the updated weights from the language model fine-tuning step, we can fine-tune the classifier with gradual unfreezing and the other techniques described above to perform task-specific class prediction.\n",
    "\n",
    "Reference: \n",
    "\n",
    "1. https://github.com/prrao87/tweet-stance-prediction\n",
    "\n",
    "\n",
    "2. https://docs.fast.ai/\n",
    "\n",
    "\n",
    "3. https://sgugger.github.io/the-1cycle-policy.html\n",
    "\n",
    "\n",
    "4. https://towardsdatascience.com/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6\n",
    "\n",
    "\n",
    "5. https://forums.fast.ai/t/determining-when-you-are-overfitting-underfitting-or-just-right/7732"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "q5FFzhXwj88d",
    "outputId": "3cd319c4-a5e1-4e97-d924-f86899d7442f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
      "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.60)\n",
      "Requirement already satisfied: fastprogress>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.2.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.18.2)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.4.0)\n",
      "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (20.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (1.0.3)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.7)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.4.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.5.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
      "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.2.4)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.7.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (7.0.0)\n",
      "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2020.4.5.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.2.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (46.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (4.38.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.6.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (7.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.0.18->fastai) (3.1.0)\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch\n",
    "!pip3 install torchvision\n",
    "!pip3 install tqdm\n",
    "!pip3 install fastai\n",
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "VOnov8m0VWtW"
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "colab_type": "code",
    "hidden": true,
    "id": "0CjWJQfBxgbG",
    "outputId": "d63c4149-6917-4e88-fef1-9d011b716cde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>train</td>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25001</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25002</td>\n",
       "      <td>train</td>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25003</td>\n",
       "      <td>train</td>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25004</td>\n",
       "      <td>train</td>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>49995</td>\n",
       "      <td>train</td>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9998_9.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>49996</td>\n",
       "      <td>train</td>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9999_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>49997</td>\n",
       "      <td>train</td>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>999_10.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>49998</td>\n",
       "      <td>train</td>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "      <td>99_8.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>49999</td>\n",
       "      <td>train</td>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>pos</td>\n",
       "      <td>9_7.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   type  ... label         file\n",
       "0           25000  train  ...   neg      0_3.txt\n",
       "1           25001  train  ...   neg  10000_4.txt\n",
       "2           25002  train  ...   neg  10001_4.txt\n",
       "3           25003  train  ...   neg  10002_1.txt\n",
       "4           25004  train  ...   neg  10003_1.txt\n",
       "...           ...    ...  ...   ...          ...\n",
       "24995       49995  train  ...   pos   9998_9.txt\n",
       "24996       49996  train  ...   pos   9999_8.txt\n",
       "24997       49997  train  ...   pos   999_10.txt\n",
       "24998       49998  train  ...   pos     99_8.txt\n",
       "24999       49999  train  ...   pos      9_7.txt\n",
       "\n",
       "[25000 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('/content/gdrive/My Drive/Dataset')\n",
    "trainfile = 'imdb_train.csv'\n",
    "testfile = 'imdb_test.csv'\n",
    "train_orig = pd.read_csv(path/trainfile, encoding='iso-8859-1')\n",
    "train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "wsYRLHaQ4iF9",
    "outputId": "28624105-c5fb-45e7-9614-969b35a87f7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25000</td>\n",
       "      <td>train</td>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25001</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25002</td>\n",
       "      <td>train</td>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25003</td>\n",
       "      <td>train</td>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25004</td>\n",
       "      <td>train</td>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   type  ... label         file\n",
       "0       25000  train  ...   neg      0_3.txt\n",
       "1       25001  train  ...   neg  10000_4.txt\n",
       "2       25002  train  ...   neg  10001_4.txt\n",
       "3       25003  train  ...   neg  10002_1.txt\n",
       "4       25004  train  ...   neg  10003_1.txt\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_ascii(text):\n",
    "    # function to remove non-ASCII chars from data\n",
    "    return ''.join(i for i in text if ord(i) < 128)\n",
    "  \n",
    "train_orig['review'] = train_orig['review'].apply(clean_ascii)\n",
    "train_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "cwS6JY6bzmLE",
    "outputId": "c1371002-88fa-4b8c-fd5a-f25fce142459"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>pos</td>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>pos</td>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>pos</td>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>pos</td>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>pos</td>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             review\n",
       "24995   pos  Seeing as the vote average was pretty low, and...\n",
       "24996   pos  The plot had some wretched, unbelievable twist...\n",
       "24997   pos  I am amazed at how this movie(and most others ...\n",
       "24998   pos  A Christmas Together actually came before my t...\n",
       "24999   pos  Working-class romantic drama from director Mar..."
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train_orig['label'], train_orig['review']], axis=1)\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "O0aSmEd84eXf"
   },
   "outputs": [],
   "source": [
    "# Write train to csv\n",
    "train.to_csv(path/'train.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ngSpw62U4xbU",
    "outputId": "75ab8881-c57c-4055-81bc-eb22778cfc75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Language model data\n",
    "data_lm = TextLMDataBunch.from_csv(path, 'train.csv', min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "mCFiIDEO5YLF"
   },
   "outputs": [],
   "source": [
    "# Save the language and classifier model data for re-use\n",
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "colab_type": "code",
    "hidden": true,
    "id": "98c_u0A542Fw",
    "outputId": "a465aac3-4979-42e6-e620-6dedbed31240"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>two numbers for the lottery . \\n \\n  xxmaj our star of the picture has his number and his friend his . xxmaj when he asks his friend , would he share half of the dough , should his ticket be the winning number , his friend promptly says no . xxmaj in fact , xxup h.e. double hockey sticks no ! is the way he acts about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>still investing time in something related to this piece of sh!t is startlingly similar to waking up after a night of suicidally heavy drinking next to the heaving form of a still slumbering 200 pound college girl . xxmaj your first urge is a desperate desire to flee . xxmaj this is natural . xxbos i imagine when xxmaj hitchcock scholars and experts find themselves together , the talk is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>cliche at all the right moments . xxmaj superb xxmaj new xxmaj york xxmaj city locations - gritty , real - are a fantastic antidote to the commercial imperatives of \" xxmaj sex in the xxmaj city \" - in fact , the entire film is an antidote to the xxup hbo / xxmaj hollywood notion of xxmaj new xxmaj york xxmaj city , sex and relationships . xxmaj it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>stooge counterparts a running gag throughout the 53- minute movie is xxmaj moe hitting xxmaj curly . xxmaj wayne 's character , a skirt chasing bully , is not very endearing , but is supposed to be the good guy . \\n \\n  xxmaj playing a traveling rodeo cowboy xxmaj wayne holds up the rodeo box office at gunpoint and takes the prize money he would have won if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>king xxmaj david } , xxmaj susan xxmaj hayward ( xxmaj bathsheba ) , xxmaj raymond xxmaj massey ( xxmaj nathan ) , xxmaj kieron xxmaj moore ( xxmaj uriah ) and xxmaj jayne xxmaj meadows ( xxmaj michal ) . \\n \\n  xxmaj the film is based around the second xxmaj old xxmaj testament book of xxmaj samuel from the xxmaj holy xxmaj bible . xxmaj it follows</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "zHChoBHH5xAQ"
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, arch = AWD_LSTM, pretrained = True, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "hidden": true,
    "id": "NO_OYTQA50tT",
    "outputId": "f81c809b-97d0-436e-a970-7741f4e7a0c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AWD_LSTM(\n",
       "   (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "   (encoder_dp): EmbeddingDropout(\n",
       "     (emb): Embedding(60000, 400, padding_idx=1)\n",
       "   )\n",
       "   (rnns): ModuleList(\n",
       "     (0): WeightDropout(\n",
       "       (module): LSTM(400, 1152, batch_first=True)\n",
       "     )\n",
       "     (1): WeightDropout(\n",
       "       (module): LSTM(1152, 1152, batch_first=True)\n",
       "     )\n",
       "     (2): WeightDropout(\n",
       "       (module): LSTM(1152, 400, batch_first=True)\n",
       "     )\n",
       "   )\n",
       "   (input_dp): RNNDropout()\n",
       "   (hidden_dps): ModuleList(\n",
       "     (0): RNNDropout()\n",
       "     (1): RNNDropout()\n",
       "     (2): RNNDropout()\n",
       "   )\n",
       " ), LinearDecoder(\n",
       "   (decoder): Linear(in_features=400, out_features=60000, bias=True)\n",
       "   (output_dp): RNNDropout()\n",
       " )]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(learn.model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "hidden": true,
    "id": "6lUfEg0B59gb",
    "outputId": "dae31e29-66fa-43c0-a2d8-524654187dec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='1347', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      7.35% [99/1347 00:23<04:51 12.2639]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnIJARKTOKQhEROrc/weN0D+kLH+VSQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "hidden": true,
    "id": "sHkiI0nB75Xv",
    "outputId": "3f6c40ab-b662-4ea8-f7be-4a94313c3019"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.251603</td>\n",
       "      <td>4.017988</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>05:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = language_model_learner(data_lm,  arch = AWD_LSTM, pretrained = True, drop_mult=0.5)\n",
    "learn.fit_one_cycle(cyc_len=1, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "hidden": true,
    "id": "_7afM4FY8Yvc",
    "outputId": "2a6de0a0-3084-423d-c299-7de0fa7eb7b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.001808</td>\n",
       "      <td>3.941348</td>\n",
       "      <td>0.301745</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.128095</td>\n",
       "      <td>4.060712</td>\n",
       "      <td>0.289842</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.214559</td>\n",
       "      <td>4.133401</td>\n",
       "      <td>0.282311</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.171031</td>\n",
       "      <td>4.105160</td>\n",
       "      <td>0.285697</td>\n",
       "      <td>05:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.107690</td>\n",
       "      <td>4.054721</td>\n",
       "      <td>0.290646</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.952753</td>\n",
       "      <td>3.991771</td>\n",
       "      <td>0.297385</td>\n",
       "      <td>05:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.815827</td>\n",
       "      <td>3.928691</td>\n",
       "      <td>0.304834</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.655516</td>\n",
       "      <td>3.889913</td>\n",
       "      <td>0.310102</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.498987</td>\n",
       "      <td>3.881652</td>\n",
       "      <td>0.312519</td>\n",
       "      <td>05:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.386326</td>\n",
       "      <td>3.888191</td>\n",
       "      <td>0.312559</td>\n",
       "      <td>05:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(cyc_len=10, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "RwwZu4GjNTgj"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned encoder\n",
    "learn.save_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "hidden": true,
    "id": "gjXtKrqwH5PT",
    "outputId": "ea44c263-8fd5-48cb-a25e-cc925d0e5bf7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_csv(path, 'train.csv', vocab=data_lm.train_ds.vocab,\n",
    "                                       min_freq=1, bs=32)\n",
    "data_clas.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "BM1Gurh-IAgy"
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, arch= AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder('ft_enc')\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "hidden": true,
    "id": "EiqhoDXXIHau",
    "outputId": "08315ad9-c726-4e0b-d4cb-df293a920075"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='95' class='' max='624', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      15.22% [95/624 00:32<03:03 1.4106]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSxDGGGP8sgRhjDHGr/8HnWVtHQNybnoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "hidden": true,
    "id": "bK9QxdlIILOz",
    "outputId": "d643101c-8fe1-4b4f-fd53-9d1e192e33d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.314346</td>\n",
       "      <td>0.229890</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>04:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ltFoLS7IIMBd",
    "outputId": "5c9ca106-0900-4fb1-d432-eeb5392eeeb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.196820</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>04:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2) \n",
    "learn.fit_one_cycle(1, slice(1e-3,1e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "hidden": true,
    "id": "poUIW7V6IPz2",
    "outputId": "556b2dc2-5a2f-4066-9a47-221f984b0d97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.248752</td>\n",
       "      <td>0.181083</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>04:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3) \n",
    "learn.fit_one_cycle(1, slice(8e-4,1e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "hidden": true,
    "id": "gm6ZPICSIVft",
    "outputId": "c2146105-ed6f-45c2-d285-bd9fc9a90bd2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.246294</td>\n",
       "      <td>0.183238</td>\n",
       "      <td>0.927400</td>\n",
       "      <td>04:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210732</td>\n",
       "      <td>0.163227</td>\n",
       "      <td>0.933000</td>\n",
       "      <td>04:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.139298</td>\n",
       "      <td>0.155351</td>\n",
       "      <td>0.939800</td>\n",
       "      <td>04:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089309</td>\n",
       "      <td>0.167842</td>\n",
       "      <td>0.944600</td>\n",
       "      <td>03:53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(8e-4,2e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "5DO3sQmpId69",
    "outputId": "7bcc2696-1cda-40cd-969b-337e00991745"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>A gritty Australian film, with all the element...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>This movie gives you more of an idiea how Aust...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Before I start my review here is a quick lesso...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>A good story, well-acted with unexpected chara...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Two Hands restored my faith in Aussie films. I...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review label\n",
       "0     Once again Mr. Costner has dragged out a movie...   neg\n",
       "1     This is an example of why the majority of acti...   neg\n",
       "2     First of all I hate those moronic rappers, who...   neg\n",
       "3     Not even the Beatles could write songs everyon...   neg\n",
       "4     Brass pictures (movies is not a fitting word f...   neg\n",
       "...                                                 ...   ...\n",
       "9995  A gritty Australian film, with all the element...   pos\n",
       "9996  This movie gives you more of an idiea how Aust...   pos\n",
       "9997  Before I start my review here is a quick lesso...   pos\n",
       "9998  A good story, well-acted with unexpected chara...   pos\n",
       "9999  Two Hands restored my faith in Aussie films. I...   pos\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(path/testfile, encoding='iso-8859-1')\n",
    "test_pred = test.filter(['review','label'])\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Q9X9gCJgmHJy",
    "outputId": "1915d027-2b60-4a8b-c7eb-af5099526fdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review label predicted label\n",
       "0  Once again Mr. Costner has dragged out a movie...   neg             neg\n",
       "1  This is an example of why the majority of acti...   neg             neg\n",
       "2  First of all I hate those moronic rappers, who...   neg             neg\n",
       "3  Not even the Beatles could write songs everyon...   neg             neg\n",
       "4  Brass pictures (movies is not a fitting word f...   neg             pos"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred['predicted label'] = test_pred['review'].apply(lambda row: str(learn.predict(row)[0]))\n",
    "test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "-xTZNEZ5jg-x",
    "outputId": "b7a7050d-1dd0-43e3-8be2-92db4900b660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score      : 0.9318844954220745\n",
      "Accuracy score: 0.9323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "m1 = f1_score(test_pred['label'], test_pred['predicted label'], pos_label='pos')\n",
    "m2 = accuracy_score(test_pred['label'], test_pred['predicted label'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "d5tZp-fcAi7U"
   },
   "source": [
    "We can see that by utilising LSTM network in ULMFit, the accuracy score improves significantly from 0.88 to 0.93. But then, there are still some rooms for improvement and problems do exist in LSTM network too.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "gG6DOn-rHSUJ"
   },
   "source": [
    "## 2. SST2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "RV7g1svALAS3"
   },
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "nTuZ3mAGHWgv",
    "outputId": "3dbcfac5-7789-4355-a4d3-c88d8d4b395c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "6915  painful , horrifying and oppressively tragic ,...      1\n",
       "6916  take care is nicely performed by a quintet of ...      0\n",
       "6917  the script covers huge , heavy topics in a bla...      0\n",
       "6918  a seriously bad film with seriously warped log...      0\n",
       "6919  a deliciously nonsensical comedy about a city ...      1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "train.columns=['review','label']\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "4mopKlFvHV0S",
    "outputId": "633e92a7-3fe1-4bc4-f38e-42f9c2b926a7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  a stirring , funny and finally transporting re...      1\n",
       "1  apparently reassembled from the cutting room f...      0\n",
       "2  they presume their audience wo n't sit still f...      0\n",
       "3  this is a visually stunning rumination on love...      1\n",
       "4  jonathan parker 's bartleby should have been t...      1"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_ascii(text):\n",
    "    # function to remove non-ASCII chars from data\n",
    "    return ''.join(i for i in text if ord(i) < 128)\n",
    "  \n",
    "train['review'] = train['review'].apply(clean_ascii)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "j-vwIgnwH-C_"
   },
   "outputs": [],
   "source": [
    "# Write train to csv\n",
    "path = Path('/content/gdrive/My Drive/Dataset')\n",
    "train.to_csv(path/'train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "hidden": true,
    "id": "vNnMXBUnH-Az",
    "outputId": "3c14cd59-1c34-4966-fa29-30161e2103e4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Language model data\n",
    "data_lm = TextLMDataBunch.from_csv(path, 'train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "-VP6FqvhH969"
   },
   "outputs": [],
   "source": [
    "# Save the language and classifier model data for re-use\n",
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "hidden": true,
    "id": "GQG1VOiKH93X",
    "outputId": "f8a259ab-529d-4fc0-a123-61cf8fe26b0f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>all the hallmarks of a movie designed strictly for children 's home video , a market so xxunk it xxunk all manner of lame entertainment , as long as 3 year xxunk find it diverting xxbos a portrait of alienation so perfect , it will certainly succeed in xxunk most viewers xxbos the rules of attraction gets us too drunk on the party favors to sober us up with the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>xxunk and most sordid of human behavior on the screen , then laughs at how clever it 's being xxbos fails in making this character xxunk , in getting under her skin , in exploring xxunk well before the end , the film grows as dull as its characters , about whose fate it is hard to care xxbos several uninteresting , xxunk people do bad things to and with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the film is not only a love song to the movies but it also is more fully an example of the kind of lush , all enveloping movie experience it xxunk xxbos neil burger here succeeded in making the mystery of four decades back the xxunk for a more immediate mystery in the present xxbos it 's about individual moments of mood , and an xxunk that 's actually sort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>of movies that flow through the hollywood xxunk without a xxunk xxbos it 's xxunk to american psycho but still has xxunk enough to get inside you and stay there for a couple of hours xxbos slap her she 's not funny ! xxbos you can tell almost immediately that welcome to collinwood is n't going to jell xxbos not many movies have that kind of impact on me these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>to say about growing up catholic or , really , anything xxbos watching this film , what we feel is n't mainly suspense or excitement xxbos this is more fascinating being real than anything seen on jerry xxunk xxbos the appeal of the vulgar , xxunk , xxunk humour went over my head or considering just how low brow it is perhaps it xxunk under my feet xxbos an invaluable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "vHCfrnrfH9zs"
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, arch = AWD_LSTM, pretrained = True, drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "hidden": true,
    "id": "HB_KQ54OH9pm",
    "outputId": "8cacb58e-834d-4994-eb8c-229cf7fa786b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      75.00% [3/4 00:09<00:03]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.990418</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.979654</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.683331</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='24' class='' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      96.00% [24/25 00:02<00:00 11.8164]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Yw5txr6-H9dS",
    "outputId": "cb1c19f6-40d5-4450-cf39-429cec1285e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.503974</td>\n",
       "      <td>4.804791</td>\n",
       "      <td>0.210969</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = language_model_learner(data_lm,  arch = AWD_LSTM, pretrained = True, drop_mult=0.5)\n",
    "learn.fit_one_cycle(cyc_len=1, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "hidden": true,
    "id": "oHxKe84nI8mD",
    "outputId": "6b32b7b0-9976-4ff1-a76c-1f8e043cfd7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.739492</td>\n",
       "      <td>4.384730</td>\n",
       "      <td>0.247226</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.239692</td>\n",
       "      <td>4.333302</td>\n",
       "      <td>0.257334</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(cyc_len=2, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Tc2i_fGYI8bQ"
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned encoder\n",
    "learn.save_encoder('ft_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "hidden": true,
    "id": "BhS058o3I8Or",
    "outputId": "98641d9c-e39e-4608-f5ce-b43cc387aff6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/fastai/data_block.py:537: UserWarning: You are labelling your items with CategoryList.\n",
      "Your valid set contained the following unknown labels, the corresponding items have been discarded.\n",
      "3394, 1037, 499, 6041, 5258...\n",
      "  if getattr(ds, 'warn', False): warn(ds.warn)\n"
     ]
    }
   ],
   "source": [
    "# Classifier model data\n",
    "data_clas = TextClasDataBunch.from_csv(path, 'train2.csv', vocab=data_lm.train_ds.vocab, bs=32)\n",
    "data_clas.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "OMLmsJ68JYCh"
   },
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, arch= AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder('ft_enc')\n",
    "learn.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "hidden": true,
    "id": "7t-BVCQZJX14",
    "outputId": "f2ddac0c-f82e-434e-c085-e451394fd551"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='93' class='' max='172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      54.07% [93/172 00:02<00:02 16.3166]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdDUlEQVR4nO3deZRcZ3nn8e9TW+/aW7IiWZYtZNkMYJaGQyAONpCM4yHBwDEzPsyMAR98yIRtMiEJ4xMgw5CwZTkJkwHnYBwYj0kmwISweCHEiGGNbGwj2y1Lli1bwq1uSVbv1bXcZ/64t6Ryqzep695b1fX7nFOnq27dqvep6q5fv3Xve99r7o6IiLSPTNoFiIhIshT8IiJtRsEvItJmFPwiIm1GwS8i0mZyaRewFBs2bPDt27enXYaISEu59957j7l7/+zlLRH827dvZ8+ePWmXISLSUszs0FzLtalHRKTNKPhFRNqMgl9EpM0o+EVE2oyCX0SkzSj4RUTajIJfRKTNKPhFRJrQ0GiRT925j4MjEw1/bgW/iEgTOnR8kk//8wF+frLY8OdW8IuINKGxYgWA1V35hj+3gl9EpAmNTpcBBb+ISNuoBf+qrsZPqabgFxFpQrXg7+tUj19EpC2MTZfp68yRzVjDn1vBLyLShMamy7Fs3wcFv4hIUxqdLrMqhs08oOAXEWlKo+rxi4i0l7Gigl9EpK2oxy8i0mZGp8uxjOEHBb+ISNOZqVQplgP1+EVE2sXYdHzz9ICCX0Sk6ZyerkHBLyLSFlo2+M3sFjMbNrO9dcs+YmYPmtn9ZnaXmf1CXO2LiLSqsRhn5oR4e/y3AlfNWvZJd3+Bu78Q+DrwwRjbFxFpSWPFFg1+d98NnJi1bKzuZg/gcbUvItKq4pyLHyCeQaILMLOPAv8RGAWuXGC9G4EbAbZt25ZMcSIiTWB0KtrGv1Lm6nH3m9z9fOA24F0LrHezuw+4+0B/f39yBYqIpGx0ukxXPkshF09Epzmq5zbgTSm2LyLSlOKcpwcSDn4z21l38/XAYJLti4i0gjjn6YEYt/Gb2e3AFcAGMzsMfAi42sx2AQFwCHhnXO2LiLSqOOfpgRiD392vm2Px5+JqT0RkpRidrrBlTWdsz68jd0VEmszYdDm2o3ZBwS8i0nTiPN8uKPhFRJpKNXDGZyqxjeEHBb+ISFOJe54eUPCLiDSVuOfpAQW/iEhTiXueHlDwi4g0lbjn4gcFv4hIU1GPX0SkzcR9vl1Q8IuINBX1+EVE2szodJl81ujMxxfPCn4RkSZSm5nTzGJrQ8EvItJExorxztMDCn4RkaYS9zw9oOAXEWkqo9PlWOfpAQW/iEhTifvsW6DgFxFpKtrUIyLSRtydsWJFwS8i0i4mZipUA4/1fLug4BcRaRpJHLULCn4Rkaah4BcRaTO1Cdp0AJeISJs4NRe/xvGLiLSHJM63Cwp+EZGmcWobf7eCX0SkLYwVy2QMegsaziki0hZGp8v0debJZOKbkhkU/CIiTSOJeXpAwS8i0jQU/CIibSaJCdpAwS8i0jRGp8uxz9MDCn4RkaYxOh3/zJyg4BcRaQruzth0/OfbhRiD38xuMbNhM9tbt+yTZjZoZg+a2VfNbE1c7YuItJKxYoVSNaC/tyP2tuLs8d8KXDVr2d3A89z9BcCjwAdibF9EpGWMjM8A0N/XwsHv7ruBE7OW3eXulejmj4CtcbUvItJKhseLAC3f41/M24FvzXenmd1oZnvMbM/IyEiCZYmIJK/W49+4aoUGv5ndBFSA2+Zbx91vdvcBdx/o7+9PrjgRkRSc2tTT2xl7W/EPGJ3FzN4KvA54jbt70u2LiDSjkYkZCrlMIuP4Ew1+M7sK+F3gVe4+lWTbIiLNbGRshv7eDszinaAN4h3OeTvwQ2CXmR02sxuATwN9wN1mdr+ZfSau9kVEWsnIxEwiI3ogxh6/u183x+LPxdWeiEgrGxmf4fx13Ym0pSN3RUSawMh4cj1+Bb+ISMrK1YDjkyU2KvhFRNrD8YkSkMxRu6DgFxFJ3ekx/Ap+EZG2UJuuYeOq+A/eAgW/iEjqkpygDRT8IiKpqwX/ht5CIu0p+EVEUjYyMcPqrjwduWwi7Sn4RURSNjw2k9hQTlDwi4ikLsnpGkDBLyKSuiSP2gUFv4hIqtydkXFt6hERaRsTMxWmy1X1+EVE2kXSY/hBwS8ikqokT7lYo+AXEUnRcIInWa9R8IuIpCjpCdpAwS8ikqqRiRnyWWNNdz6xNhX8IiIpGhlP7iTrNQp+EZEUDSd88BYo+EVEUpX0Ubug4BcRSZWCX0SkjVQD58TkDP19yY3hBwW/iEhqjk/MEHiyR+3CEoPfzHrMLBNdv9jMfsPMkht7JCKyAg2nMIYflt7j3w10mtkW4C7gPwC3xlWUiEg7GJlI/qhdWHrwm7tPAW8E/srdrwX+VXxliYisfCNjzd3jNzP7ReAtwDeiZcmcHFJEZIWq9fibchs/8D7gA8BX3f0hM7sI+Of4yhIRWflGxmfo68zRmU+2H51bykru/l3guwDRTt5j7v6eOAsTEVnpkj7zVs1SR/X8bzNbZWY9wF7gYTN7f7yliYisbEdOTnPe6mTH8MPSN/U8193HgGuAbwEXEo7sERGRc+DuHByZ4MINPYm3vdTgz0fj9q8BvubuZcDjK0tEZGU7MVlirFjhog29ibe91OD/LPAE0APsNrMLgLG4ihIRWekOHpsE4ML+Ju3xu/tfuPsWd7/aQ4eAKxd6jJndYmbDZra3btm1ZvaQmQVmNrDM2kVEWtbBkQkAdjRrj9/MVpvZn5rZnujyJ4S9/4XcClw1a9lewoPAdp91pSIiK8jBkUkKuQxb1nYl3vZSN/XcAowDb44uY8DnF3qAu+8GTsxa9oi77zuHOkVEVpTHRibZvr6bbCa5M2/VLGkcP7DD3d9Ud/sPzez+OAqqMbMbgRsBtm3bFmdTIiKJO3hsgos39qXS9lJ7/NNm9ku1G2b2SmA6npJC7n6zuw+4+0B/f3+cTYmIJKpSDXjy+BQXpbBjF5be438n8AUzWx3dfga4Pp6SRERWtqeemaYSeCpj+GHpUzY8AFxmZqui22Nm9j7gwTiLExFZiWojei7qT35ED5zlGbjcfSw6ghfgtxda18xuB34I7DKzw2Z2g5m9wcwOA78IfMPM7jynqkVEWtjBkXAM/44m39QzlwV3Rbv7dfPc9dVltCki0vIOHptgXU+BNd2FVNpfzjl3NWWDiMg5eGxkkotS2r4Pi/T4zWycuQPegOSPOhARWQEePzbJlbvSG624YPC7ezqDTEVEVqjxYpmR8RkuTGGqhprlbOoREZGzVNuxm9YYflDwi4gk6uCxaHI2Bb+ISHs4ODJJNmNsW6fgFxFpCwdHJjl/bReFXHrxq+AXEUnQwWOTqR2xW6PgFxFJSBA4jx9L5zy79RT8IiIJeXqsSLEcpDqiBxT8IiKJOTU5W4pj+EHBLyKSmLQnZ6tR8IuIJOSxkQn6OnL093WkWoeCX0QkIfuPTrBjYy9myZ9nt56CX0QkIQdGJti5Md3t+6DgFxFJxOhUODnbcxT8IiLt4cDIOICCX0SkXRwYDody7tyY/mz3Cn4RkQTsPzpBRy7DlrXpn8NKwS8ikoADIxPs6O8lm0l3RA8o+EVEErH/6ERTbN8HBb+ISOymShWOnJxW8IuItIvaVA3NMIYfFPwiIrHbP9w8QzlBwS8iErsDwxPkMsYF69OdnK1GwS8iErP9Rye4YH13qqdbrNccVYiIrGDhHD3pH7hVo+AXEYlRqRJw6PhU02zfBwW/iEisnjg+STVwBb+ISLvYfzSco0fBLyLSJg4MT2AGO/oV/CIibeHAyARb13bRVcimXcopCn4RkRjtPzrOc5qotw8xBr+Z3WJmw2a2t27ZOjO728z2Rz/XxtW+iEjaqoFz8NhkU23fh3h7/LcCV81a9vvAP7n7TuCfotsiIivSUyemKFWCphrDDzEGv7vvBk7MWvx64G+i638DXBNX+yIiaXv0aDRHz6b26fHPZZO7Px1dHwI2zbeimd1oZnvMbM/IyEgy1YmINNC+oTD4L97UJj3+xbi7A77A/Te7+4C7D/T39ydYmYhIYwweHef8dV30duTSLuVZkg7+o2a2GSD6OZxw+yIiidk3NM6uTavSLuMMSQf/14Dro+vXA/+QcPsiIomYqVR5/Ngkl5zXXJt5IN7hnLcDPwR2mdlhM7sB+BjwK2a2H3htdFtEZMU5MDxBNXB2NWHwx7bhyd2vm+eu18TVpohIsxh8Otyx21Y9fhGRdrbv6DiFbIbtG5rjrFv1FPwiIjEYHBpnx8Ze8tnmi9nmq0hEZAXYNzTGpU24mQcU/CIiDXdyqsTRsZmm3LELCn4RkYYbjI7YVfCLiLSJ2lQNl5zXfAdvgYJfRKThBofGWN2VZ9OqjrRLmZOCX0SkwQaHxtl1Xh9mlnYpc1Lwi4g0UBA4jw6NN+WBWzUKfhGRBjpycprJUrVpd+yCgl9EpKEGh5p3qoYaBb+ISAPtGxoDmu/kK/UU/CIiDTQ4NM7WtV30debTLmVeCn4RkQYabPIdu6DgFxFpmPFiOTr5SnMeuFWj4BcRaZDvPjpCNXAu37kh7VIWpOAXEWmQux8+yrqeAi+5YG3apSxIwS8i0gDlasB3Bod59SUbyTXhHPz1mrs6EZEW8eODJxgvVvjV525Ku5RFKfhFRBrgroeH6MxnuHxnf9qlLErBLyKyTO7O3Q8f5fKd/XQVsmmXsygFv4jIMu09MsbTo8WW2MwDCn4RkWW7++EhMgavuVTBLyLSFu56+CgD29exrqeQdilLouAXEVmGJ49PMTg03jKbeUDBLyKyLHc9PATAryj4RURWviBwvvrTI+za1McF63vSLmfJFPwiIufob/c8xUM/H+OdV1yUdilnRcEvInIOTkyW+Pgdg7zswnVc88ItaZdzVhT8IiLn4JN3DjJerPCR1z8PM0u7nLOi4BcROUs/ffIZvvQvT/H2V25v6pOqz0fBLyJyFqqB8wf/sJeNfR2897UXp13OOVHwi4gsUTVwPnHnIHuPjHHTv3kuvR25tEs6J61ZtYhIwk5Mlnjvl37K9/Yf480DW/n1F2xOu6Rzlkrwm9l7gXcABvy1u/95GnWIiCzFfU8+w2/ddh/HJ0v88Rufz7976fktt0O3XuLBb2bPIwz9lwEl4A4z+7q7H0i6FhGRhZyYLPHp7xzgCz98gs1rOvnKb76C521ZnXZZy5ZGj/9S4MfuPgVgZt8F3gh8IoVaRETOMF2qcsv3H+cz9zzGZKnCmwfO5wNXX8rqrnzapTVEGsG/F/ioma0HpoGrgT2zVzKzG4EbAbZt25ZogSLSnoLA+b/3H+HjdwxydGyG1166id+7ahc7N7XekM2FJB787v6ImX0cuAuYBO4HqnOsdzNwM8DAwIAnWqSItJ37nnyGP/zHh3ngqZO8YOtq/vK6F/OyC9elXVYsUtm56+6fAz4HYGZ/BBxOow4RaW/jxTLfGRzm6w8+zd0PH2VjXwefuvYy3viiLWQyrbvzdjFpjerZ6O7DZraNcPv+y9OoQ0Taz9BokXv2DfPtR46y+9FjlKoB/X0dvPvVz+Gdr9pBT4uOzT8bab3CL0fb+MvAb7n7yZTqEJEVrlwNuPfQM9yzb4R79g0zODQOwJY1Xfz7l1/A1c8/jxdvW7uie/izpbWp5/I02hWRla8aOPuHx7nv0El2PzrC9w8cY3ymQi5jDGxfy+//2iVcuWsjF2/qbemx+Mux8r/TiMiKVCxXeXq0yKHjkzx5YopDx6d46Oej/OzwKJOlcLzI5tWdvO6yzbzq4o288jnr6etcGcMxl2vFB381cH5+cppDx6d4/PgkI+MzjE6VGJ0uM16sUMhlWNWZZ1VXjr7OPJ35DJ35LB25DIVchmwmQy5jZDNGEDgzlYCZSpWZSkCl6gQeXgxjXU+Bjas62NjXyYbeAr2dOTpy2TnrKparjE6XeWaqxOhUOXregFIlYHKmwtBYkadHixwdK1KuBqztLrC2O8/q7gLdhSyFbFhfPmuUKgHFckCxXKUcOFkzclkjY0ZXPsPankL0+AI9HVk68+GlkMswVaowXqwwUawwUwlY050PL10F8lljphI+73S5SqXquIPjBA5TpfBxk6UKM+WAznyWrkI2rC+XoRqE6wfulKtOqRJQroavsVT7GV2vBn7q4jgZM8yMjEEuYxRyGTpy4fN2F7L0duTo6cjRmc8yVaowOVNlciasZapUZbpUjX5WmCzV7qsyVbfOVKlKpRpQCZxK1TGDno4cvdElcGd0usxYMfxbyWUy9HZk6enI0VPIkclAxsL3OZc1+jrz9HbkWNUZ1tZdOP1+lCoBE3U11l57uRpQDaAjn6Gj7jV2RL/bfDZcVvt7LOQyZGb1UqvB6b9Dd8hmwpqyGSOXCZ8jlzXyWSOXyZDPZijkwuvZ6G87G63Xmc/Qmcs+a7OHu1OJfpf1wnZYdq+5Ug1O/Y6mSpVT79PETIXJmQonJkucmCxxfKLEsYkZhsaKDI0WOT5ZetbzdOYzXLypjze9ZCsv2raGF56/lu3ru9u2V7+QFR38f/zNR/j895+gVA1OLTODvo4ca7oL9HXmKFUCxoplxqYrTJfPGFW6bPms0duRI5vJUK4GVKoB5SAMwcWs6ylw3qpO8lnjyRNTPDNZYqxYaXiN8zHjjA97qzGDnkIYwj0dOXo6snTnc6zrKbBlTfZUKOYyduqf2XgxDJ2sGeet6uTiTX30duSoBKfDe6pUIQigSkDgUJoOOHR8ivFimbFiZcHfby3YC9kwhLOZ8B/sTLlKsVKlXE3/TS9kM2BhKAeLlBP+0zC6og5FVz4bPTb8R14JgqjDEHKvdQI8/Ews1gDhP/91PQXW93aweXUnl52/hs2rOtm8posL1ndzwbpu+vs6FPJLtKKD//lbV/O2X9rO9vU9bF/fw4Ubeujv6yA7z06cSjU41cMtRh/EwJ1qEO4gymXtWb2vXMbIRL2rauCcmCwxPFZkeHyG4xMzTJaqUYiUqQZQyBq56MPe15ljTXeetd0FVneF3zQK2Sz5nNFTyNHf10Fn/sxvC9XAmalUT/cYA6dQ66nlwyALgrCHVg2cyVKFk1Mlnpkqc2KyxFSpcurbQakS0F3I0teZp68zRz6bYXS6fGr9cjWgq5A99YHOZcJeuAGZDHTlo95xZ46OXIbpctjTni5VKVcDLOp1GpDLWl1PNuq91v0Me6cZMpmwB+kefquovZbaa56pBEyVTvcIZypVuvJhoNd64vU97dm916SUq8Gpbx7T5Sr5rNHXkae7I/wdLcRr35Cqp78VzJRPf9N89rrh7yKbMbJR6FU9/N0HAZSD8JtprcNRqYbPVwvd2reFSuCUKwHF2t9/OWwnlwm/zWTNzvgWUA1qbYXPVyyffr2nH5uJHlt7pGHGs76xduRqv7ss3R258FtVIXfq29fa7gKrunIK9QYyb4Eu3cDAgO/Zc8bBvSIisgAzu9fdB2Yv13z8IiJtRsEvItJmFPwiIm1GwS8i0mYU/CIibUbBLyLSZhT8IiJtRsEvItJmWuIALjMbBfbPcddqYHSJt+e6Xr9sA3DsHMqb3eZS71ftoVat/VzrXqi2xe5X7ar9bO/f6e5nnh3e3Zv+Aty8lOUL3Z7r+qxlexpZm2pf2bWfa92qXbWnWXvt0iqbev5xicsXuj3X9fme92ws9hyq/czrqv3c7lfty6PaIy2xqScJZrbH55jTohWo9uS1at2g2tPSTLW3So8/CTenXcAyqPbktWrdoNrT0jS1q8cvItJm1OMXEWkzCn4RkTaz4oLfzG4xs2Ez23sOj32Jmf3MzA6Y2V9Y3Sl/zOzdZjZoZg+Z2ScaW/WpNhpeu5l92MyOmNn90eXqxlce3/se3f9fzMzNbEPjKn7W88fxvn/EzB6M3vO7zOwXGl95bLV/Mvpbf9DMvmpmaxpfeWy1Xxt9RgMza/iO1OXUPM/zXW9m+6PL9XXLF/xMLNu5jitt1gvwy8CLgb3n8NifAC8HDPgW8GvR8iuBbwMd0e2NLVT7h4HfacX3PbrvfOBO4BCwoVVqB1bVrfMe4DMtVPuvArno+seBj7dQ7ZcCu4B7gIFmqTmqZ/usZeuAg9HPtdH1tQu9vkZdVlyP3913Ayfql5nZDjO7w8zuNbPvmdklsx9nZpsJP6w/8vCd/wJwTXT3bwIfc/eZqI3hFqo9ETHW/mfA73L6XN0tUbu7j9Wt2hNX/THVfpe7V6JVfwRsbaHaH3H3fXHUu5ya5/Gvgbvd/YS7PwPcDVyVxOd5xQX/PG4G3u3uLwF+B/irOdbZAhyuu304WgZwMXC5mf3YzL5rZi+NtdpnW27tAO+KvrbfYmZr4yv1DMuq3cxeDxxx9wfiLnQOy37fzeyjZvYU8BbggzHWOlsj/mZq3k7Y40xKI2tPylJqnssW4Km627XXEfvryzXyyZqRmfUCrwD+T91mso6zfJoc4dexlwMvBf7OzC6K/hvHpkG1/0/gI4Q9zo8Af0L4YY7Vcms3s27gvxJudkhUg9533P0m4CYz+wDwLuBDDStyHo2qPXqum4AKcFtjqlu0vYbVnpSFajaztwHvjZY9B/immZWAx939DUnXWm/FBz/ht5qT7v7C+oVmlgXujW5+jTAg67/SbgWORNcPA1+Jgv4nZhYQTrg0EmfhNKB2dz9a97i/Br4eZ8F1llv7DuBC4IHoA7UVuM/MXubuQ01e+2y3Ad8kgeCnQbWb2VuB1wGvibuDU6fR73sS5qwZwN0/D3wewMzuAd7q7k/UrXIEuKLu9lbCfQFHiPv1NXrnRzNcgO3U7XwBfgBcG1034LJ5Hjd7h8rV0fJ3Av8tun4x4dcza5HaN9et85+BL7XK+z5rnSeIaeduTO/7zrp13g38fQvVfhXwMNAfV81x/80Q087dc62Z+XfuPk64Y3dtdH3dUl7fsl9D3L/YpC/A7cDTQJmwp34DYc/xDuCB6A/6g/M8dgDYCzwGfJrTRzYXgP8V3Xcf8OoWqv2LwM+ABwl7S5tbpfZZ6zxBfKN64njfvxwtf5BwoqwtLVT7AcLOzf3RJa4RSXHU/obouWaAo8CdzVAzcwR/tPzt0ft9AHjb2XwmlnPRlA0iIm2mXUb1iIhIRMEvItJmFPwiIm1GwS8i0mYU/CIibUbBLy3LzCYSbu8HDXqeK8xs1MKZOwfN7FNLeMw1ZvbcRrQvouAXiZjZgkeyu/srGtjc9zw82vNFwOvM7JWLrH8NoOCXhlDwy4oy30yJZvbr0SR7PzWzb5vZpmj5h83si2b2feCL0e1bzOweMztoZu+pe+6J6OcV0f1/H/XYb6vNl25mV0fL7o3mUV9wigx3nyY8SKo2Md07zOxfzOwBM/uymXWb2SuA3wA+GX1L2LGMGSFFFPyy4sw3U+L/A17u7i8CvkQ41XPNc4HXuvt10e1LCKfMfRnwITPLz9HOi4D3RY+9CHilmXUCnyWcO/0lQP9ixUazpe4EdkeLvuLuL3X3y4BHgBvc/QeER12/391f6O6PLfA6RRbVDpO0SZtYZHbHrcDfRnOdFwjnRan5WtTzrvmGh+demDGzYWATz54mF+An7n44avd+wvlbJoCD7l577tuBG+cp93Ize4Aw9P/cT0889zwz++/AGqCX8CQ0Z/M6RRal4JeVZN6ZEoG/BP7U3b9mZlcQnpmsZnLWujN116vM/TlZyjoL+Z67v87MLgR+ZGZ/5+73A7cC17j7A9EMmVfM8diFXqfIorSpR1YMD8969biZXQtgocuiu1dzemrb6+d6fAPsAy4ys+3R7X+72AOibwcfA34vWtQHPB1tXnpL3arj0X2LvU6RRSn4pZV1m9nhustvE4blDdFmlIeA10frfphw08i9wLE4iok2F/0n4I6onXFgdAkP/Qzwy9E/jD8Afgx8HxisW+dLwPujndM7mP91iixKs3OKNJCZ9br7RDTK538A+939z9KuS6SeevwijfWOaGfvQ4Sblz6bcj0iZ1CPX0SkzajHLyLSZhT8IiJtRsEvItJmFPwiIm1GwS8i0mb+P0JhzQ6yvt+KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "hidden": true,
    "id": "MkCqXiAQJXqS",
    "outputId": "c3daff0a-1ad2-4c47-fe48-5bc529cc2d82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.178029</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, max_lr=1e-2, moms=(0.95, 0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "hidden": true,
    "id": "atuAr-nyJfoR",
    "outputId": "e5d02b80-1fbc-4bab-9651-6cdb113b4916"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.068141</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2) \n",
    "learn.fit_one_cycle(1, slice(1e-3,1e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "colab_type": "code",
    "hidden": true,
    "id": "2fnCqFe6Jfb4",
    "outputId": "bbe77e61-d08a-45f8-9593-6d396d14753a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.088950</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3) \n",
    "learn.fit_one_cycle(1, slice(8e-4,1e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "hidden": true,
    "id": "_lynKfkfJfSf",
    "outputId": "d1ea5fec-bbd7-4561-db70-283f6c62733b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>9.219917</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.707023</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.215235</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.402908</td>\n",
       "      <td>#na#</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, slice(8e-4,2e-2), moms=(0.95,0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "U4Wa-qvHJtKy",
    "outputId": "f86b3990-7fe1-48d6-c58e-7fea9cd85e85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no movement , no yuks , not much of anything</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we never really feel involved with the story ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is one of polanski 's best films</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>an often deadly boring , strange reading of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>the problem with concept films is that if the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>safe conduct , however ambitious and well inte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>a film made with as little wit , interest , an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>but here 's the real damn it is n't funny , ei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0          no movement , no yuks , not much of anything      0\n",
       "1     a gob of drivel so sickly sweet , even the eag...      0\n",
       "2     gangs of new york is an unapologetic mess , wh...      0\n",
       "3     we never really feel involved with the story ,...      0\n",
       "4                 this is one of polanski 's best films      1\n",
       "...                                                 ...    ...\n",
       "1816  an often deadly boring , strange reading of a ...      0\n",
       "1817  the problem with concept films is that if the ...      0\n",
       "1818  safe conduct , however ambitious and well inte...      0\n",
       "1819  a film made with as little wit , interest , an...      0\n",
       "1820  but here 's the real damn it is n't funny , ei...      0\n",
       "\n",
       "[1821 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/test.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "test_pred.columns=['review','label']\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "FiLGUTPyJs0G",
    "outputId": "5f9b84c6-f316-4018-dfdf-fed96f78bd78"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no movement , no yuks , not much of anything</td>\n",
       "      <td>0</td>\n",
       "      <td>2967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
       "      <td>0</td>\n",
       "      <td>1740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we never really feel involved with the story ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>2961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is one of polanski 's best films</td>\n",
       "      <td>1</td>\n",
       "      <td>3875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label  predicted label\n",
       "0       no movement , no yuks , not much of anything      0             2967\n",
       "1  a gob of drivel so sickly sweet , even the eag...      0             1740\n",
       "2  gangs of new york is an unapologetic mess , wh...      0              659\n",
       "3  we never really feel involved with the story ,...      0             2961\n",
       "4              this is one of polanski 's best films      1             3875"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred['predicted label'] = test_pred['review'].apply(lambda row: int(learn.predict(row)[0]))\n",
    "test_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "BYC4qgjzJ8Cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "m1 = f1_score(test_pred['label'], test_pred['predicted label'], pos_label='pos')\n",
    "m2 = accuracy_score(test_pred['label'], test_pred['predicted label'])\n",
    "print('F1 score      :',m1)\n",
    "print('Accuracy score:',m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## What is the problem with LSTM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "8arFR-ybbBLT"
   },
   "source": [
    "\n",
    "\n",
    "The same problem that happens to RNNs generally, happen with LSTMs, i.e. when sentences are way too long LSTMs still don’t do well. The reason for that is that the probability of keeping the context from a word that is far away from the current word being processed decreases exponentially with the distance from it.That means that when sentences are long, the model often forgets the content of distant positions in the sequence. \n",
    "\n",
    "Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you are have to process word by word. This results in training the model takes longer time and can't perform multiple actions simultaneously. Not only that but there is no explicit model of long and short range dependencies. \n",
    "\n",
    "Moving on, we will look at **attention** and **Convolutional Neural Networks (CNN)** on how they address and solve the issues faced by LSTMs and introducing the latest state-of-the-art neural network, **Transformers**! \n",
    "\n",
    "Reference: https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## What is attention?\n",
    "\n",
    "When translating a sentence, I pay special attention to the word I’m presently translating. When I’m transcribing an audio recording, I listen carefully to the segment I’m actively writing down. These are the examples of usage of attention in different scenario.\n",
    "\n",
    "Neural networks can achieve this same behavior using attention, focusing on part of a subset of the information they are given. For RNNs, instead of only encoding the whole sentence in a hidden state, each word has a corresponding hidden state that is passed all the way to the decoding stage. Then, the hidden states are used at each step of the RNN to decode. The following gif shows how that happens.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/750/1*JrxKsw2LYU9emkM-jR13uQ.gif)\n",
    "\n",
    "The idea behind it is that there might be relevant information in every word in a sentence. So in order for the decoding to be precise, it needs to take into account every word of the input, using attention.\n",
    "\n",
    "However, processing inputs (words) in parallel is still not possible. For a large corpus of text, this increases the time spent on processing the text. As result, Convolutional Neural Network is introduced to address this issue.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Why Convolutional Neural Network (CNN)?\n",
    "\n",
    "CNN enables parallelisation and thus greatly reduce the time spent in processing the inputs. Some of the most popular neural networks for sequence transduction, Wavenet and Bytenet, are CNN.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/713/1*www46FWqJCc3OZQKP_QRoQ.gif)\n",
    "\n",
    "The reason why Convolutional Neural Networks can work in parallel, is that each word on the input can be processed at the same time and does not necessarily depend on the previous words to be translated. \n",
    "\n",
    "However, the problem is that CNN do not necessarily help with the problem of figuring out the problem of dependencies when translating sentences. That’s why **Transformers** were created, they are a combination of both CNN with attention.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "kwS2nA7WAi7W"
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "cszdZSZMAi7W"
   },
   "source": [
    "\n",
    "Transformers try to solve the problems faced by LSTM by introducing CNN (for parallelisation purpose) together with attention models. Attention boosts the speed of how fast the model can translate from one sequence to another, more specifically, it uses self-attention (to be even more specific, it uses multihead attention). \n",
    "\n",
    "![alt text](https://miro.medium.com/max/1523/1*V2435M1u0tiSOz4nRBfl4g.png)\n",
    "\n",
    "The Transformer consists of six encoders and six decoders. Each encoder and decoder are similar to other encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "a6F2-Xb7Ai7W"
   },
   "source": [
    "## The Encoder Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "0S5pZ93cAi7X"
   },
   "source": [
    "Encoder consists of two layers: Self-attention and a Feed Forward Neural Network.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/990/1*HaGTuYfNHWg45GZbTBnVSA.png)\n",
    "![alt text](https://miro.medium.com/max/509/1*QcTbVCVPj4WFnqvvWU5-hQ.png)\n",
    "\n",
    "The encoder’s inputs first flow through a self-attention layer. It helps the encoder look at other words in the input sentence as it encodes a specific word. The decoder has both those layers, but between them is an encoder-decoder attention layer that helps the decoder focus on relevant parts of the input sentence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "qGihSRpYAi7X"
   },
   "source": [
    "## What is Self- Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "JPfR8NVHAi7Y"
   },
   "source": [
    "Self-attention is similar to attention, they fundamentally share the same concept and many common mathematical operations.  In layman’s terms, the self-attention mechanism allows the inputs to interact with each other (“self”) and find out who they should pay more attention to (“attention”). The outputs are aggregates of these interactions and attention scores. Here is how it looks like in a sentence:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1580/1*GQzYZuAMWr3lN_IACBfvAA.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## How Self Attention works?\n",
    "\n",
    "As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm. \n",
    "\n",
    "Each word is embedded into a vector of size 512. We’ll represent those vectors with these simple boxes.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1030/0*0oTRj6MKAYEs_cT1.png)\n",
    "\n",
    "In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder. One key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1353/0*FVCP6TqLPQeWPZqt.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Steps in calculating self-attention:\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1094/0*-P9BdUe2FCSAIpxC.png)\n",
    "\n",
    "1. Create three vectors from each of the encoder’s input vectors which are Query vector, Key vector and Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
    "\n",
    "\n",
    "2. Calculate a score to determine how much focus to place on other parts of the input sentence as we encode a word at a certain position. Score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/856/0*KlFsyIDK3O54l14X.png)\n",
    "\n",
    "\n",
    "3.  Divide the scores by 8 (the square root of the dimension of the key vectors used here — 64. This leads to having more stable gradients. There could be other possible values here, but this is the default).\n",
    "\n",
    "\n",
    "4. Pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1. Softmax score determines how much how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1084/0*rqWSBLDcJcbMmGs2.png)\n",
    "\n",
    "\n",
    "5. Multiply each value vector by the softmax score. The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n",
    "\n",
    "\n",
    "6. Sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word). The resulting vector is one we can send along to the feed-forward neural network.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/983/0*ih2c_llIiOD1-aJN.png)\n",
    "\n",
    "In the actual implementation, however, this calculation is done in matrix form for faster processing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "siTPh97hAi7Y"
   },
   "source": [
    "## Multihead attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "SpqmNOh-Ai7Y"
   },
   "source": [
    "\n",
    "\n",
    "To make transformer even better, Transformers use the concept of Multihead attention. It expands the model’s ability to focus on different positions.  Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.\n",
    "\n",
    "The idea behind it is that whenever you are translating a word, you may pay different attention to each word based on the type of question that you are asking. The images below show what that means. For example, whenever you are translating “kicked” in the sentence “I kicked the ball”, you may ask “Who kicked”. Depending on the answer, the translation of the word to another language can change. Or ask other questions, like “Did what?”\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1540/1*8H6TqcfHrtNCc9_Qva7xog.png)\n",
    "\n",
    "\n",
    "## All-in-one visual showing how multihad attention works:\n",
    "\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "FFHz0nXsAi7Y"
   },
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "3j7rq3YXAi7Z"
   },
   "source": [
    "Another important step on the Transformer is to add positional encoding when encoding each word. Positional encoders are vector that gives context based on position of word in sentence. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "vwRq1WYJAi7Z"
   },
   "source": [
    "## Residual Connection and Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "NDrKmWyTAi7Z"
   },
   "source": [
    "Each sub-layer (self-attention, ffnn) in each encoder is followed by a layer-normalization step and has a residual connection around it.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm.png)\n",
    "\n",
    "Normalization helps with the problem called internal covariate shift. Internal covariate shift refers to covariate shift occurring within a neural network, i.e. going from (say) layer 2 to layer 3. This happens because, as the network learns and the weights are updated, the distribution of outputs of a specific layer in the network changes. This forces the higher layers to adapt to that drift, which slows down learning. After normalizing the input in the neural network, we don’t have to worry about the scale of input features being extremely different.\n",
    "\n",
    "To understand layer normalization, it is useful to contrast it with batch normalization. A mini-batch consists of multiple examples with the same number of features. Mini-batches are matrices — or tensors if each input is multi-dimensional — where one axis corresponds to the batch and the other axis — or axes — correspond to the feature dimensions. Batch normalization normalizes the input features across the batch dimension. The key feature of layer normalization is that it normalizes the inputs across the features. In batch normalization, the statistics are computed across the batch and are the same for each example in the batch. In contrast, in layer normalization, the statistics are computed across each feature and are independent of other examples. Layer normalization is better for the purpose of stabilization.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1280/1*hex7_me89ax78PCv2zLTzA.png)\n",
    "\n",
    "Besides, skip connections or residual connections are used to allow gradients to flow through a network directly, without passing through non-linear activation functions. Non-linear activation functions (eg: sigmoid and tanh), by nature of being non-linear, cause the gradients to explode or vanish (depending on the weights). Skip connections form conceptually a ‘bus’ which flows right the way through the network, and in reverse, the gradients can flow backwards along it too.\n",
    "\n",
    "If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "6brJi8CJAi7Z"
   },
   "source": [
    "## The Decoder Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "iSxXAhq0Ai7Z"
   },
   "source": [
    "Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/509/1*QcTbVCVPj4WFnqvvWU5-hQ.png)\n",
    "\n",
    "The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_decoding_1.gif)\n",
    "\n",
    "The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_decoding_2.gif)\n",
    "\n",
    "The self attention layers in the decoder operate in a slightly different way than the one in the encoder:\n",
    "\n",
    "There is a masked multi-head attention at the bottom. Masked represents a mask that masks certain values so that they do not have an effect when the parameters are updated. There are two kinds of masks in the Transformer model — padding mask and sequence mask. The padding mask is used in all the scaled dot-product attention (query.key), and the sequence mask is only used in the decoder’s self-attention.\n",
    "\n",
    "A padding mask solves the problem of input sequences being of variable length. Specifically, we pad 0 after a shorter sequence. But if the input sequence is too long, the content on the left is intercepted and the excess is discarded directly. Because the location of these fills (zeroes) is actually meaningless, our attention mechanism should not focus on these locations, so we need to do some processing. The specific approach is to add a very large negative number (negative infinity) to the values of these positions, so that the probability of these positions will be close to 0 after softmax! The padding mask is actually a tensor, each value is a Boolean, and the value of False is where we want to process.\n",
    "\n",
    "A sequence mask is designed to ensure that the decoder is unable to see future information. That is, for a sequence, at time_step t, our decoded output should only depend on the output before t, not the output after t. This is specific to the Transformer architecture because we do not have RNNs where we can input our sequence sequentially. Here, we input everything together and if there were no mask, the multi-head attention would consider the whole decoder input sequence at each position. We achieve this by generating an upper triangular matrix with the values of the upper triangles all zero and applying this matrix to each sequence.\n",
    "\n",
    "The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack\n",
    "\n",
    "Another detail is that the the decoder input will be shifted to the right by one position. One reason to do this is that we do not want our model to learn how to copy our decoder input during training, but we want to learn that given the encoder sequence and a particular decoder sequence, which has been already seen by the model, we predict the next word/character. If we don’t shift the decoder sequence, the model learns to simply ‘copy’ the decoder input, since the target word/character for position i would be the word/character i in the decoder input. Thus, by shifting the decoder input by one position, our model needs to predict the target word/character for position i having only seen the word/characters 1, …, i-1 in the decoder sequence. This prevents our model from learning the copy/paste task. We fill the first position of the decoder input with a start-of-sentence token, since that place would otherwise be empty because of the right-shift. Similarly, we append an end-of-sentence token to the decoder input sequence to mark the end of that sequence and it is also appended to the target output sentence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "c0ymf9RIAi7a"
   },
   "source": [
    "## The Final Linear and Softmax Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ww_mfGFtAi7c"
   },
   "source": [
    "The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\n",
    "\n",
    "The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\n",
    "\n",
    "Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\n",
    "\n",
    "The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/transformer_decoder_output_softmax.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "346DUnGqAi7d"
   },
   "source": [
    "## The model architecture of Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "hZShPTKoAi7d"
   },
   "source": [
    "Thus, after going through all the components in details, here is how a transformer looks like:\n",
    "\n",
    "![alt_text](https://miro.medium.com/max/1800/1*BHzGVskWGS_3jEcYYi6miQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "-gxKncauAi7d"
   },
   "source": [
    "## Recap of Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "mJGXjkryAi7e"
   },
   "source": [
    "Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\n",
    "\n",
    "During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.\n",
    "\n",
    "To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).\n",
    "    \n",
    "![alt text](http://jalammar.github.io/images/t/vocabulary.png)\n",
    "\n",
    "Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:\n",
    "\n",
    "![alt_text](http://jalammar.github.io/images/t/one-hot-vocabulary-example.png)\n",
    "\n",
    "Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## The Loss Function\n",
    "\n",
    "Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.\n",
    "\n",
    "What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.\n",
    "\n",
    "![alt_text](http://jalammar.github.io/images/t/transformer_logits_output_and_label.png)\n",
    "\n",
    "Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.\n",
    "\n",
    "But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:\n",
    "\n",
    "a. Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 3,000 or 10,000)\n",
    "\n",
    "b. The first probability distribution has the highest probability at the cell associated with the word “i”\n",
    "\n",
    "c. The second probability distribution has the highest probability at the cell associated with the word “am”\n",
    "\n",
    "d. And so on, until the fifth output distribution indicates ‘<end of sentence>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/output_target_probability_distributions.png)\n",
    "\n",
    "After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/t/output_trained_model_probability_distributions.png)\n",
    "\n",
    "Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). \n",
    "\n",
    "Another way to do it would be to hold on to, say, the top two words with highest probabilities (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (because we compared the results after calculating the beams for positions #1 and #2), and top_beams is also two (since we kept two words). These are both hyperparameters that you can experiment with.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reference: \n",
    "\n",
    "1. https://towardsdatascience.com/transformers-141e32e69591\n",
    "\n",
    "\n",
    "2. http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "\n",
    "3. http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "\n",
    "4. https://towardsdatascience.com/breaking-bert-down-430461f60efb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "1A0kYlfsAi7e"
   },
   "source": [
    "# 2. Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "XR3nT6WeAi7e"
   },
   "source": [
    "BERT’s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. A language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "Thqnxeg-Ai7e"
   },
   "source": [
    "## General Idea of BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "FdUX_71FAi7f"
   },
   "source": [
    "1. Language modeling is an effective task for using unlabeled data to pretrain neural networks in NLP\n",
    "\n",
    "\n",
    "2. Traditional language models take the previous n tokens and predict the next one. In contrast, BERT trains a language model that takes both the previous and next tokens into account when predicting.\n",
    "\n",
    "\n",
    "3. BERT is also trained on a next sentence prediction task to better handle tasks that require reasoning about the relationship between two sentences (e.g. question answering)\n",
    "\n",
    "\n",
    "4. BERT uses the Transformer architecture for encoding sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ZGdEFoU0Z2f_"
   },
   "source": [
    "There is one of the key traits of BERT: Instead of predicting the next word after a sequence of words, BERT randomly masks words in the sentence and predicts them.\n",
    "\n",
    "![alt text](https://i0.wp.com/mlexplained.com/wp-content/uploads/2019/01/Screen-Shot-2019-01-03-at-11.22.11-AM.png?resize=1024%2C262)\n",
    "Differences in the Language Model Architecture between major transfer learning methods\n",
    "\n",
    "\n",
    "Why is this method effective? Because this method forces the model to learn how to use information from the entire sentence in deducing what words are missing.\n",
    "\n",
    "If you are familiar with the NLP literature, you might know about bidirectional LSTM based language models and wonder why they are insufficient. Bidirectional LSTM based language models train a standard left-to-right language model and also train a right-to-left (reverse) language model that predicts previous words from subsequent words. Actually, this is what methods like ELMo and ULMFiT did. In ELMo, there is a single LSTM for the forward language model and backward language model each. The crucial difference is this: neither LSTM takes both the previous and subsequent tokens into account at the same time.\n",
    "\n",
    "![alt_text](https://i0.wp.com/mlexplained.com/wp-content/uploads/2019/01/Screen-Shot-2019-01-03-at-4.40.22-PM.png?resize=1024%2C207)\n",
    "\n",
    "This is crucial since this forces the model to use information from the entire sentence simulatenously - regardless of the position - to make a good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "FiXJz5uuAi7h"
   },
   "source": [
    "## How BERT is built?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "6PKbQztOAi7h"
   },
   "source": [
    "BERT is basically a trained Transformer Encoder stack. BERT makes use of Transformer, an attention mechanism that learns contextual relations between words (or sub-words) in a text. In its vanilla form,Transformer includes two separate mechanisms — an encoder that reads the text input and a decoder that produces a prediction for the task. Since BERT’s goal is to generate a language model, only the encoder mechanism is necessary. The Transformer architecture is a model that does not use recurrent connections at all and uses attention over the sequence instead.\n",
    "\n",
    "As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), the Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional, though it would be more accurate to say that it’s non-directional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word).\n",
    "\n",
    "As input, BERT takes token embeddings as well as a couple of additional embeddings that provide some crucial metadata. One of these embeddings is the positional embedding. One limitation of the Transformer architecture is that - unlike RNNs - it cannot take the order of the inputs into account (i.e. it will treat the first and last tokens of the inputs exactly the same if they are the same word). To overcome this problem, BERT learns and uses positional embeddings to express the position of words in a sentence. These embeddings are added to the token embeddings before feeding them into the model.\n",
    "\n",
    "BERT also takes segment embeddings as input. BERT can be trained on sentence pairs for tasks that take sentence pairs as input (e.g. question answering and natural language inference). It learns a unique embedding for the first and second sentences to help the model distinguish between the sentences.\n",
    "\n",
    "The input schema for BERT is summarized below:\n",
    "![alt text](https://miro.medium.com/max/1468/0*m_kXt3uqZH9e7H4w.png)\n",
    "\n",
    "1. Token embedding is the task of get the embedding (i.e. a vector of real numbers) for each word in the sequence. Each word of the sequence is mapped to a emb_dim dimensional vector that the model will learn during training. You can think about it as a vector look-up for each token. The elements of those vectors are treated as model parameters and are optimized with back-propagation just like any other weights.\n",
    "\n",
    "\n",
    "2. Positional Embedding is designed to help the model learn some notion of sequences and relative positioning of tokens. This is crucial for language-based tasks especially here because we are not making use of any traditional recurrent units such as RNN, GRU or LSTM Intuitively, we aim to be able to modify the represented meaning of a specific word depending on its position. We don’t want to change the full representation of the word but we want to modify it a little to encode its position by adding numbers between [-1,1] using predetermined (non-learned) sinusoidal functions to the token embeddings. For the rest of the Encoder, the word will be represented slightly differently depending on the position the word is in (even if it is the same word).\n",
    "\n",
    "![alt text](https://miro.medium.com/max/2233/1*OsmkGAkon5IDTwZJ1ORwPA.png)\n",
    "\n",
    "\n",
    "## How does it looks like?\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1375/1*9DD12JPwj1pLY6yUEOv35A.png)\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1375/1*Y1MDr4WgzYp4eZaBOuJIYw.png)\n",
    "\n",
    "\n",
    "To overcome the challenge of bidirectional in which it allows each word to indirectly see itself in a multi-layered context, BERT uses two training strategies:\n",
    "\n",
    "### 1. Masked Language Model (MLM) Training\n",
    "    \n",
    "Language Modeling is the task of predicting the next word given a sequence of words. In masked language modeling instead of predicting every next token, a percentage of input tokens is masked at random and only those masked tokens are predicted.\n",
    "    \n",
    "Why? Bi-directional models are more powerful than uni-directional language models. But in a multi-layered model bi-directional models do not work because the lower layers leak information and allow a token to see itself in later layers.\n",
    "    \n",
    "   The masked words are not always replaced with the masked token – [MASK] because then the masked tokens would never be seen before fine-tuning. Therefore, 15% of the tokens are chosen at random and \n",
    "\n",
    "1. 80% of the time tokens are actually replaced with the token [MASK].\n",
    "\n",
    "\n",
    "2. 10% of the time tokens are replaced with a random token.\n",
    "\n",
    "\n",
    "3. 10% of the time tokens are left unchanged. \n",
    "    \n",
    "The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. In technical terms, the prediction of the output words requires:\n",
    "\n",
    "   1. Adding a classification layer on top of the encoder output.\n",
    "    \n",
    "    \n",
    "   2. Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.\n",
    "   \n",
    "   \n",
    "   3. Calculating the probability of each word in the vocabulary with softmax.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1095/0*ViwaI3Vvbnd-CJSQ.png)\n",
    "\n",
    "### 2. Next Sentence Prediction (NSP) Training\n",
    "\n",
    "In addition to masked language modeling, BERT also uses a next sentence prediction task to pretrain the model for tasks that require an understanding of the relationship between two sentences. This task can be easily generated from any monolingual corpus. It is helpful because many downstream tasks such as Question and Answering and Natural Language Inference require an understanding of the relationship between two sentences.\n",
    "\n",
    "When taking two sentences as input, BERT separates the sentences with a special [SEP] token. During training, BERT is fed two sentences and 50% of the time the second sentence comes after the first one and 50% of the time it is a randomly sampled sentence. BERT is then required to predict whether the second sentence is random or not.\n",
    "\n",
    "![alt text](https://i2.wp.com/mlexplained.com/wp-content/uploads/2019/01/Screen-Shot-2019-01-05-at-5.21.26-PM.png?w=634)\n",
    "Example inputs for the next sentence prediction task\n",
    "\n",
    "\n",
    "To predict if the second sentence is indeed connected to the first, the following steps are performed:\n",
    "\n",
    "1. The entire input sequence goes through the Transformer model.\n",
    "\n",
    "\n",
    "2. The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n",
    "\n",
    "\n",
    "3. Calculating the probability of IsNextSequence with softmax.\n",
    "\n",
    "When training the BERT model, Masked LM and Next Sentence Prediction are trained together, with the goal of minimizing the combined loss function of the two strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "i0qyZnnjAi7h"
   },
   "source": [
    "## Facts about BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "T-KCGMMeAi7h"
   },
   "source": [
    "1. WHAT IS THE MAXIMUM SEQUENCE LENGTH OF THE INPUT?\n",
    "\n",
    "    512 tokens.\n",
    "\n",
    "\n",
    "2. HOW MANY LAYERS ARE FROZEN IN THE FINE-TUNING STEP?\n",
    "    \n",
    "    No layers are frozen during fine-tuning. All the pre-trained layers along with the task-specific parameters are trained simultaneously.\n",
    "\n",
    "\n",
    "3. IS DISCRIMINATIVE FINE-TUNING USED?\n",
    "    \n",
    "    No. All the parameters are tuned with the same learning rate.\n",
    "\n",
    "\n",
    "4. WHAT ARE THE OPTIMAL VALUES OF THE HYPERPARAMETERS USED IN FINE-TUNING?\n",
    "\n",
    "    The optimal hyperparameter values are task-specific. But, the authors found that the following range of values works well across all tasks –\n",
    "\n",
    "    Dropout – 0.1\n",
    "    Batch Size – 16, 32\n",
    "    Learning Rate (Adam) – 5e-5, 3e-5, 2e-5\n",
    "    Number of epochs – 3, 4\n",
    "    \n",
    "    The authors also observed that large datasets (> 100k labeled samples) are less sensitive to hyperparameter choice than smaller datasets.\n",
    "    \n",
    "\n",
    "5. IS THE NEXT SENTENCE PREDICTION TASK NECESSARY?\n",
    "\n",
    "   No. Unlike masked language modeling, the performance drops for only a subset of the tasks. Concretely, for natural language inference and question answering (the MNLI-m, QNLI, and SQuAD datasets), next sentence prediction seems to help a lot. For paraphrase detection (MRPC), the performance change is much smaller, and for sentiment analysis (SST-2) the results are virtually the same.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "KAgPZQzNAi7j"
   },
   "source": [
    "## Steps in utilising BERT for text classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "hnueGblrAi7k"
   },
   "source": [
    "1. Tokenization\n",
    "\n",
    "    a. Use the BERT tokenizer to first split the word into tokens. \n",
    "    \n",
    "    b. add the special tokens needed for sentence classifications (these are [CLS] at the first position, and [SEP] at the end of the sentence). \n",
    "\n",
    "    c. Replace each token with its id from the embedding table which is a component we get with the trained model.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png)\n",
    "\n",
    "2. Flow through the BERT model\n",
    "\n",
    "    a. The output would be a vector for each input token. each vector is made up of 768 numbers (floats).\n",
    "\n",
    "    b. We ignore all except the first vector (the one associated with the [CLS] token). The one vector we pass as the input to the logistic regression model. (p.s: BERT was trained using a next sentence prediction (NSP) objective using the [CLS] token as a sequence approximate. The user may use this token (the first token in a sequence built with special tokens) to get a sequence prediction rather than a token prediction. However, averaging over the sequence may yield better results than using the [CLS] token.)\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-model-calssification-output-vector-cls.png)\n",
    "\n",
    "\n",
    "## Why use [CLS] token?\n",
    "\n",
    "The [CLS] token will be prepended to every input sentence. So, on the first layer, the representation of [CLS] is a function of the [CLS] token itself and all other tokens to the right. This pattern repeats until you reach the last transformer layer. I hope you can see that the [CLS] token would have multiple opportunities to look at an input sentence left and right since the token representations it depends on is looking at the sentences left and right. This means that the [CLS] token representation at the final layer can be considered a rich representation of the input sentence.\n",
    "\n",
    "[CLS] token carries the sentence representation in sentence classification tasks because this is the token whose representation is finetuned to the task at hand. We don't pick any other token as the sentence representation because the same token have different representation depending on its location. For example, the representation for the word \"the\" in the \"the cat in the hat\" is different than in \"I like the cat\". We also don't pick the n-th token as the representation because it won't be handle cases where the input sentence's lengh is less than n.\n",
    "\n",
    "So, to make things easy for us, let's just tack on a dummy token (which we will call [CLS]) to every input sentence. This way, we can be sure that we always have a token whose representation is simply a function of the other tokens in the input sentence and not its position.\n",
    "\n",
    "\n",
    "## How to process the output of BERT?\n",
    "\n",
    "The output for BERT looks like something as below:\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-output-tensor.png)\n",
    "\n",
    "For sentence classification, we’re only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.  We slice that 3d tensor to get the 2d tensor .\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png)\n",
    "\n",
    "And now features is a 2d numpy array containing the sentence embeddings of all the sentences in our dataset. Each row corresponds to a sentence in our dataset, each column corresponds to the output of a hidden unit from the feed-forward neural network at the top transformer block of the Bert/DistilBERT model. Afterwards, we use this output to train Logistic Regression.\n",
    "\n",
    "![alt text](http://jalammar.github.io/images/distilBERT/bert-output-cls-senteence-embeddings.png)\n",
    "\n",
    "\n",
    "Reference:\n",
    "1. https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "\n",
    "2. http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n",
    "\n",
    "\n",
    "3. https://huggingface.co/transformers/index.html\n",
    "\n",
    "\n",
    "4. https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/\n",
    "\n",
    "\n",
    "5. http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/\n",
    "\n",
    "\n",
    "6. https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "WZqRqrPrAi7k"
   },
   "source": [
    "## Basic BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "colab_type": "code",
    "hidden": true,
    "id": "b6p8P_nRNvN5",
    "outputId": "80a3d38a-f337-4218-c835-7b2aabf87fc2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.27)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.27)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: tensorflow==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.27.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.2.0)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.18.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (3.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (46.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.7.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.21.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "wxpX7jN8Nx3l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "HhNbHSHoRfHT",
    "outputId": "aab2f79a-96b4-436e-e0f2-37d31a553ef7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "6915  painful , horrifying and oppressively tragic ,...      1\n",
       "6916  take care is nicely performed by a quintet of ...      0\n",
       "6917  the script covers huge , heavy topics in a bla...      0\n",
       "6918  a seriously bad film with seriously warped log...      0\n",
       "6919  a deliciously nonsensical comedy about a city ...      1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "imdb_df.columns=['review','label']\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "d6ccc7c9c979400ab53c1a18b03b818d",
      "29d4fc8c9c7f4e8ebbbd2eedb9026952",
      "3fb228ab02514680bff87c2ccc995b2d",
      "5a7df33964e04a509ea103e497105cb8",
      "7f90392b10f944f1bb170ab06eb7d6fa",
      "5165d03ab3e74642b58dfb326940df93",
      "aa70859d034942e9836f4c180e522141",
      "4a22cac957fc44e182a830cc2089ebf2",
      "7185127225df4228a8c1cc5ee28e37ed",
      "6c7d9d6692df49878b9a4d73dfba383b",
      "57b1a671eb69405092c81233e7a26d46",
      "72e8958d9f5c4ac7b7c6df0f972fe24d",
      "b73957c1329742218804441595061dd7",
      "dc5a8d6ab2af47b5893bfe2854bba8cf",
      "8aff4266553743aab74edf7dfb5e786d",
      "933e7f31e2d04ff7927f2b956f5ce0f9",
      "ec3b05fe0b054b1abb0a1dcbf3c5494f",
      "e551905f01314b32968deb834ed472e8",
      "8bd677b1f6224331a45e2905cf56d74a",
      "e5c43ad5e0da463ea18829d0b34cffac",
      "42fc29f530ab46348101d8577f661a85",
      "ad2e79bec9d040378d3a20a7ad6fc576",
      "0bf515550902405b9422b04826fed59b",
      "baf80a6b07414f5297cfb01d24a4ae80"
     ]
    },
    "colab_type": "code",
    "hidden": true,
    "id": "daFAgM2iR3u2",
    "outputId": "fdeb5a73-ece4-4ae6-c003-f37b7444feb5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ccc7c9c979400ab53c1a18b03b818d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7185127225df4228a8c1cc5ee28e37ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3b05fe0b054b1abb0a1dcbf3c5494f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#BERT model:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "hidden": true,
    "id": "6E0ALlB-SGXF",
    "outputId": "e00bef4a-46df-4eed-b398-ac1ad087151b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "6915    [101, 9145, 1010, 7570, 18752, 14116, 1998, 28...\n",
       "6916    [101, 2202, 2729, 2003, 19957, 2864, 2011, 103...\n",
       "6917    [101, 1996, 5896, 4472, 4121, 1010, 3082, 7832...\n",
       "6918    [101, 1037, 5667, 2919, 2143, 2007, 5667, 2561...\n",
       "6919    [101, 1037, 12090, 2135, 2512, 5054, 19570, 23...\n",
       "Name: review, Length: 6920, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = imdb_df['review'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "hidden": true,
    "id": "CZKzft43S2Lf",
    "outputId": "e9e83e72-78c2-4209-af21-4562f912a80a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1037, 18385, ...,     0,     0,     0],\n",
       "       [  101,  4593,  2128, ...,     0,     0,     0],\n",
       "       [  101,  2027,  3653, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1996,  5896, ...,     0,     0,     0],\n",
       "       [  101,  1037,  5667, ...,     0,     0,     0],\n",
       "       [  101,  1037, 12090, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "hidden": true,
    "id": "vJ8XdN6ym7f0",
    "outputId": "db351d2f-06a6-4763-de7b-d406e1f5f717"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([101, 1037, 18385, 1010, 6057, 1998, 2633, 18276, 2128, 16603, 1997, 5053, 1998, 1996, 6841, 1998, 5687, 5469, 3152, 102]),\n",
       "       list([101, 4593, 2128, 27241, 23931, 2013, 1996, 6276, 2282, 2723, 1997, 2151, 2445, 12217, 7815, 102]),\n",
       "       list([101, 2027, 3653, 23545, 2037, 4378, 24185, 1050, 1005, 1056, 4133, 2145, 2005, 1037, 11507, 10800, 1010, 2174, 14036, 2135, 3591, 1010, 2061, 2027, 19817, 4140, 2041, 1996, 7511, 2671, 4349, 3787, 1997, 11829, 7168, 9219, 1998, 28971, 2308, 1999, 8301, 8737, 2100, 4253, 102]),\n",
       "       ...,\n",
       "       list([101, 1996, 5896, 4472, 4121, 1010, 3082, 7832, 1999, 1037, 20857, 1010, 3302, 2100, 2126, 2008, 2515, 1050, 1005, 1056, 3749, 2151, 12369, 2046, 2339, 1010, 2005, 6013, 1010, 2204, 2477, 4148, 2000, 2919, 2111, 102]),\n",
       "       list([101, 1037, 5667, 2919, 2143, 2007, 5667, 25618, 7961, 2011, 3213, 2472, 9679, 15536, 15810, 2012, 1996, 9000, 2504, 102]),\n",
       "       list([101, 1037, 12090, 2135, 2512, 5054, 19570, 2389, 4038, 2055, 1037, 2103, 2746, 4237, 2012, 2049, 25180, 2015, 102])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenized.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "toiuConHTMPI",
    "outputId": "1abfb4c4-202b-4be7-9e41-efd69828d2e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "z8P9fOl5TQz5",
    "outputId": "016aca1b-0797-4bf0-acaa-775b4fb39575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 67)"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "x62RW-bBTVFF"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "--vvzVr_UxFc"
   },
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "aPxZTf7FUzVU"
   },
   "outputs": [],
   "source": [
    "labels = imdb_df['label']\n",
    "train_features, test_features, train_labels,  test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "L0JtusUyU-Mb"
   },
   "outputs": [],
   "source": [
    "# parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "# grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "# grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# print('best parameters: ', grid_search.best_params_)\n",
    "# print('best scores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ROTkg1u5VAOf",
    "outputId": "5200bed9-7252-4fe7-d397-6e1e72a2116b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "R8_rSFCkVMEG",
    "outputId": "3686283b-24eb-430c-fbdc-4fe3087785c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8583815028901735"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "uGdhr2_ma_4q",
    "outputId": "42c50691-7c28-471d-81e6-80c85714c01d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "RpLGgbAMbbUt",
    "outputId": "8de4984e-a39b-4294-bb5e-eacd0e260b84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "nOIF5bo0Ai9M"
   },
   "source": [
    "## Fine Tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "colab_type": "code",
    "hidden": true,
    "id": "3DwSnRapbbLq",
    "outputId": "99883a45-9efa-412f-f7b1-70d37371e361",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
      "\u001b[K     |████████████████████████████████| 573kB 4.5MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 38.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.38)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 34.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
      "Collecting tokenizers==0.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7MB 43.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.38)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=b8ebf487f562ba239c34e36e5374e38f1c4899b1ee94394b832a51f18147fe8e\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "BAieKT55bxju",
    "outputId": "9dd30b7d-b65b-4872-c01d-bd1a6294628f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "6915  painful , horrifying and oppressively tragic ,...      1\n",
       "6916  take care is nicely performed by a quintet of ...      0\n",
       "6917  the script covers huge , heavy topics in a bla...      0\n",
       "6918  a seriously bad film with seriously warped log...      0\n",
       "6919  a deliciously nonsensical comedy about a city ...      1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "df.columns=['review','label']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Zvi1kLZgcNsG"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "sentences = df.review.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "99nN3BgicPA9",
    "outputId": "7877fe06-7da2-41fe-affe-09c6a6a0f800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "TGysqkPPcO4W",
    "outputId": "9f80e051-0e32-4b85-e25d-1172086391ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  67\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "hidden": true,
    "id": "j8S6erf6cOwj",
    "outputId": "09f926d8-a03f-48f4-aac3-c1a27e022f26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
      "Token IDs: tensor([  101,  1037, 18385,  1010,  6057,  1998,  2633, 18276,  2128, 16603,\n",
      "         1997,  5053,  1998,  1996,  6841,  1998,  5687,  5469,  3152,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 70,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "PCQuuALYcOnA",
    "outputId": "415aeebd-4a2e-47c6-c55f-91770267ef7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,228 training samples\n",
      "  692 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "a1k-hXSRcOcE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it here. For fine-tuning BERT on a specific task, \n",
    "# the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "KB1IXv5ec-Wh",
    "outputId": "ccbccffd-7871-45c6-ad6a-817f08e84507"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "BTWUVZ7Fc-NY"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "cjTy7zAOc-EG"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting thetraining data.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "rb9OrqU6c97I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "Sd2G6jnKc9uQ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "hidden": true,
    "id": "bjYKZUHCdWwX",
    "outputId": "a6b0e584-0556-494c-a309-8dc372badd02",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    390.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    390.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    390.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    390.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    390.    Elapsed: 0:00:32.\n",
      "  Batch   240  of    390.    Elapsed: 0:00:39.\n",
      "  Batch   280  of    390.    Elapsed: 0:00:45.\n",
      "  Batch   320  of    390.    Elapsed: 0:00:51.\n",
      "  Batch   360  of    390.    Elapsed: 0:00:58.\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epcoh took: 0:01:02\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.91\n",
      "  Validation Loss: 0.24\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    390.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    390.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    390.    Elapsed: 0:00:19.\n",
      "  Batch   160  of    390.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    390.    Elapsed: 0:00:32.\n",
      "  Batch   240  of    390.    Elapsed: 0:00:38.\n",
      "  Batch   280  of    390.    Elapsed: 0:00:45.\n",
      "  Batch   320  of    390.    Elapsed: 0:00:51.\n",
      "  Batch   360  of    390.    Elapsed: 0:00:57.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:01:02\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation Loss: 0.35\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    390.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    390.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    390.    Elapsed: 0:00:19.\n",
      "  Batch   160  of    390.    Elapsed: 0:00:25.\n",
      "  Batch   200  of    390.    Elapsed: 0:00:31.\n",
      "  Batch   240  of    390.    Elapsed: 0:00:38.\n",
      "  Batch   280  of    390.    Elapsed: 0:00:44.\n",
      "  Batch   320  of    390.    Elapsed: 0:00:50.\n",
      "  Batch   360  of    390.    Elapsed: 0:00:56.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epcoh took: 0:01:01\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.34\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    390.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    390.    Elapsed: 0:00:12.\n",
      "  Batch   120  of    390.    Elapsed: 0:00:19.\n",
      "  Batch   160  of    390.    Elapsed: 0:00:25.\n",
      "  Batch   200  of    390.    Elapsed: 0:00:31.\n",
      "  Batch   240  of    390.    Elapsed: 0:00:38.\n",
      "  Batch   280  of    390.    Elapsed: 0:00:44.\n",
      "  Batch   320  of    390.    Elapsed: 0:00:50.\n",
      "  Batch   360  of    390.    Elapsed: 0:00:56.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:01:01\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation Loss: 0.35\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:04:13 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this \n",
    "        # automatically because accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        # It returns different numbers of parameters depending on what arguments arge given and what flags are set. For our \n",
    "        # usage here, it returns the loss (because we provided labels) and the \"logits\"--the model outputs prior to activation.\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end. `loss` is \n",
    "        # a Tensor containing a single value; the `.item()` function just returns the Python value from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed \n",
    "        # for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            (loss, logits) = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "lc81A1boljVK",
    "outputId": "b69c01ca-e7fa-4f6c-d1c9-87a2d468a20a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0:01:02</td>\n",
       "      <td>0:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0:01:02</td>\n",
       "      <td>0:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0:01:01</td>\n",
       "      <td>0:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0:01:01</td>\n",
       "      <td>0:00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.35         0.24           0.91       0:01:02         0:00:02\n",
       "2               0.17         0.35           0.90       0:01:02         0:00:02\n",
       "3               0.08         0.34           0.92       0:01:01         0:00:02\n",
       "4               0.04         0.35           0.92       0:01:01         0:00:02"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 2)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "tjGlR3SHdWpA",
    "outputId": "3bdc7794-a73e-47b4-cf1b-fc4ba1197517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 1,821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df2 = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/test.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "df2.columns=['review','label']\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df2.review.values\n",
    "labels = df2.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 70,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "PGeWMqcBj_F5",
    "outputId": "83bf5a66-f266-4e89-f085-52ee18a9183f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 1,821 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "TrlxXFkKnbQl",
    "outputId": "2403f261-ca67-42b5-81c5-36e69a427779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_set = []\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" and one column for \"1\"). Pick the label with \n",
    "  # the highest value and turn this in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  accuracy = accuracy_score(true_labels[i], pred_labels_i)                \n",
    "  accuracy_set.append(accuracy)\n",
    "\n",
    "acc = accuracy_score(true_labels[i], pred_labels_i)\n",
    "\n",
    "print('Accuracy score: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "FmgGOkOKAi-E"
   },
   "source": [
    "## Transformer Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "5j8a6_eY4O1M"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In transformers v2.3.0, **pipelines** provides a high-level, easy to use, API for doing inference over a variety of downstream-tasks, including:\n",
    "\n",
    "1. Sentence Classification (Sentiment Analysis): Indicate if the overall sentence is either positive or negative. (Binary Classification task or Logitic Regression task)\n",
    "\n",
    "2. Token Classification (Named Entity Recognition, Part-of-Speech tagging): For each sub-entities (tokens) in the input, assign them a label (Classification task).\n",
    "\n",
    "3. Question-Answering: Provided a tuple (question, context) the model should find the span of text in content answering the question.\n",
    "\n",
    "4. Mask-Filling: Suggests possible word(s) to fill the masked input with respect to the provided context.\n",
    "5. Feature Extraction: Maps the input to a higher, multi-dimensional space learned from the data.\n",
    "\n",
    "Pipelines encapsulate the overall process of every NLP process:\n",
    "\n",
    "1. Tokenization: Split the initial input into multiple sub-entities with ... properties (i.e. tokens).\n",
    "\n",
    "2. Inference: Maps every tokens into a more meaningful representation.\n",
    "\n",
    "3. Decoding: Use the above representation to generate and/or extract the final output for the underlying task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82,
     "referenced_widgets": [
      "887586c393bb44daae9a8f8727f96eb4",
      "f935f7a21de84e48b2458a5b6cc8d0b0",
      "13350ea1e587493ebee49c66a376de50",
      "13c7bd6e16d643f5aa88d92bd9061767",
      "08d7e07dd04249749e06066f6793a86e",
      "4dcab2a594194141b0971580b80aae50",
      "8e3afd1eb1c04b8da54a44266b7b3a45",
      "ba9bc9690b2d4bad9d1f7ea13372d5d7"
     ]
    },
    "colab_type": "code",
    "hidden": true,
    "id": "BCcOwrlbqoot",
    "outputId": "8217d8ff-9591-4a50-a159-0155639c76d2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887586c393bb44daae9a8f8727f96eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.99974006}]"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "nlp = pipeline('sentiment-analysis')\n",
    "nlp('This movie was kind of boring.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "u6wCRB4N9kov"
   },
   "source": [
    "# 3. XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "zed_-kDVAi-N"
   },
   "source": [
    "## What's Wrong with BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "EBvMMIKwAi-N"
   },
   "source": [
    "BERT was already a revolutionary method with strong performance across multiple tasks, but it wasn't without its flaws. XLNet pointed out two major problems with BERT.\n",
    "\n",
    "1. The [MASK] token used in training does not appear during fine-tuning\n",
    "\n",
    "    BERT is trained to predict tokens replaced with the special [MASK] token. The problem is that the [MASK] token - which is at the center of training BERT - never appears when fine-tuning BERT on downstream tasks.\n",
    "\n",
    "    This can cause a whole host of issues such as:\n",
    "\n",
    "    What does BERT do for tokens that are not replaced with [MASK]?\n",
    "    \n",
    "    In most cases, BERT can simply copy non-masked tokens to the output. So would it really learn to produce meaningful representations for non-masked tokens?\n",
    "    \n",
    "    Of course, BERT still needs to accumulate information from all words in a sequence to denoise [MASK] tokens. But what happens if there are no [MASK] tokens in the input sentence?\n",
    "    \n",
    "    There are no clear answers to the above problems, but it's clear that the [MASK] token is a source of train-test skew that can cause problems during fine-tuning. The authors of BERT were aware of this issue and tried to circumvent these problems by replacing some tokens with random real tokens during training instead of replacing them with the [MASK] token. However, this only constituted 10% of the noise. When only 15% of the tokens are noised to begin with, this only amounts to 1.5% of all the tokens, so is a lackluster solution.\n",
    "\n",
    "\n",
    "2. BERT generates predictions independently\n",
    "\n",
    "    Another problem stems from the fact that BERT predicts masked tokens in parallel. Let's illustrate with an example: Suppose we have the following sentence.\n",
    "\n",
    "         I went to [MASK] [MASK] and saw the [MASK] [MASK] [MASK].\n",
    "\n",
    "    One possible way to fill this out is\n",
    "\n",
    "        I went to New York and saw the Empire State building.\n",
    "\n",
    "    Another way is\n",
    "\n",
    "        I went to San Francisco and saw the Golden Gate bridge.\n",
    "\n",
    "    However, the sentence\n",
    "\n",
    "        I went to San Francisco and saw the Empire State building\n",
    "\n",
    "    is not valid. Despite this, BERT predicts all masked positions in parallel, meaning that during training, it does not learn to handle dependencies between predicting simultaneously masked tokens. In other words, it does not learn dependencies between its own predictions. Since BERT is not actually used to unmask tokens, this is not directly a problem. The reason this can be a problem is that this reduces the number of dependencies BERT learns at once, making the learning signal weaker than it could be.\n",
    "\n",
    "    Note that neither of these problems is present in traditional language models. Language models have no [MASK] token and generate all words in a specified order so it learns dependencies between all the words in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "Us1SGuN7Ai-N"
   },
   "source": [
    "## Permutation Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "iRWhkajLAi-O"
   },
   "source": [
    "Of course, despite its flaws, BERT has one major advantage over traditional language models: it captures bidirectional context. This bidirectionality was a crucial factor in BERT's success, so going back to traditional language modeling is simply not an option. The question then becomes: can we train a model to incorporate bidirectional context while avoiding the [MASK] token and parallel independent predictions?\n",
    "\n",
    "The answer is yes: XLNet does this by introducing a variant of language modeling called \"permutation language modeling\". Permutation language models are trained to predict one token given preceding context like traditional language model, but instead of predicting the tokens in sequential order, it predicts tokens in some random order. To illustrate, let's take the following sentence as an example:\n",
    "\n",
    "    I like cats more than dogs.\n",
    "\n",
    "A traditional language model would predict the tokens in the order\n",
    "\n",
    "    \"I\", \"like\", \"cats\", \"more\", \"than\", \"dogs\"\n",
    "\n",
    "where each token uses all previous tokens as context.\n",
    "\n",
    "![alt text](https://i1.wp.com/mlexplained.com/wp-content/uploads/2019/06/ezgif.com-gif-maker-1.gif?resize=447%2C170)\n",
    "\n",
    "In permutation language modeling, the order of prediction is not necessarily left to right and is sampled randomly instead. For instance, it could be\n",
    "\n",
    "    \"cats\", \"than\", \"I\", \"more\", \"dogs\", \"like\"\n",
    "\n",
    "where \"than\" would be conditioned on seeing \"cats\", \"I\" would be conditioned on seeing \"cats, than\" and so on. The following animation demonstrates this.\n",
    "\n",
    "![alt text](https://i0.wp.com/mlexplained.com/wp-content/uploads/2019/06/ezgif.com-gif-maker-2.gif?resize=421%2C158)\n",
    "\n",
    "Notice how the model is forced to model bidirectional dependencies with permutation language modeling. In expectation, the model should learn to model the dependencies between all combinations of inputs in contrast to traditional language models that only learn dependencies in one direction.\n",
    "\n",
    "The difference between permutation language modeling and BERT is best illustrated below.\n",
    "\n",
    "![alt text](https://i0.wp.com/mlexplained.com/wp-content/uploads/2019/06/Screen-Shot-2019-06-22-at-5.38.12-PM.png?resize=1024%2C567&ssl=1)\n",
    "The conceptual difference between BERT and XLNet. Transparent words are masked out so the model cannot rely on them. XLNet learns to predict the words in an arbitrary order but in an autoregressive, sequential manner (not necessarily left-to-right). BERT predicts all masked words simultaneously.\n",
    "\n",
    "As a word of caution, in permutation language modeling, we are not changing the actual order of words in the input sentence. We are just changing the order in which we predict them. If you're used to thinking of language modeling in a sequential manner, this may be hard to grasp: how can we change the order in which we predict tokens while not changing the order in which we feed them to the model? Just remember that Transformers use masking to choose which inputs to feed into the model and use positional embeddings to provide positional information. This means that we can feed input tokens in an arbitrary order simply by adjusting the mask to cover the tokens we want to hide from the model. As long as we keep the positional embeddings consistent, the model will see the tokens \"in the right order\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "6rtY5Yn0Ai-O"
   },
   "source": [
    "## The Transformer XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "D4QfltC7Ai-O"
   },
   "source": [
    "Transformers were a game-changer in NLP due to their incredible performance and ease of training. However, they had a major drawback compared to RNNs: they had limited context.\n",
    "\n",
    "Suppose you had a 50000-word long piece of text that you wanted to feed to a model. Feeding this into any model all at once would be infeasible given memory constraints. For an RNN you could work around this by simply chunking the text, then feeding the RNN one chunk at a time without resetting the hidden state between chunks. This works because the RNN is recurrent and as long as you keep the hidden state, the RNN can \"remember\" previous chunks, giving it a theoretically infinite memory.\n",
    "\n",
    "For a Transformer, this is impossible because Transformers take fixed-length sequences as input have no notion of \"memory\". All its computations are stateless (this was actually one of the major selling points of the Transformer: no state means computation can be parallelized)  so there is an upper limit on the distance of relationships a vanilla Transformer can model.\n",
    "\n",
    "50000 words might be a bit of a stretch, but there are plenty of scenarios where you would want to feed very long sequences to a model. Language modeling is a prime example of this. \n",
    "\n",
    "The Transformer XL is a simple extension of the Transformer that seeks to resolve this problem. The idea is simple: what if we added recurrence to the Transformer? Adding recurrence at the word level would just make it an RNN. But what if we added recurrence at a \"segment\" level. In other words, what if we added state between consecutive sequences of computations? The Transformer XL accomplishes this by caching the hidden states of the previous sequence and passing them as keys/values when processing the current sequence. For example, if we had the consecutive sentences\n",
    "\n",
    "    \"I went to the store. I bought some cookies.\"\n",
    "\n",
    "we can feed \"I went to the store.\" first, cache the outputs of the intermediate layers, then feed the sentence \"I bought some cookies.\" and the cached outputs into the model.\n",
    "\n",
    "This idea is great, but there is one flaw: position. In the Transformer, we handled position using positional embeddings. The first word in a sentence would have the \"first position\" embedding added to it, the second word would have the \"second position\" embedding added, and so on. But with recurrence, what happens to the positional embedding of the first word in the previous segment? If we're caching the Transformer outputs, what happens to the positional embedding of the first word in the current segment?\n",
    "\n",
    "To address these issues, the Transformer XL introduces the notion of relative positional embeddings. Instead of having an embedding represent the absolute position of a word, the Transformer XL uses an embedding to encode the relative distance between words. This embedding is used while computing the attention score between any two words: in other words, the relative positional embedding enables the model to learn how to compute the attention score for words that are n  words before and after the current word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "TN7OYf2pAi-O"
   },
   "source": [
    "## Using the Transformer XL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "OL7IIqO-Ai-O"
   },
   "source": [
    "Aside from using permutation language modeling, XLNet improves upon BERT by using the Transformer XL as its base architecture. The Transformer XL showed state-of-the-art performance in language modeling, so was a natural choice for XLNet.\n",
    "\n",
    "XLNet uses the two key ideas from Transformer XL: relative positional embeddings and the recurrence mechanism. The hidden states from the previous segment are cached and frozen while conducting the permutation language modeling for the current segment. Since all the words from the previous segment are used as input, there is no need to know the permutation order of the previous segment.\n",
    "\n",
    "The authors found that using the Transformer XL improved performance over BERT, even in the absence of permutation language modeling. This shows that better language models can lead to better representations, and thus better performance across a multitude of tasks, motivating the necessity of research into language modeling.\n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "1. https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/\n",
    "\n",
    "\n",
    "2. https://huggingface.co/transformers/model_doc/xlnet.html\n",
    "\n",
    "\n",
    "3. https://mccormickml.com/2019/09/19/XLNet-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "007rdwNgAi-P"
   },
   "source": [
    "## Basic XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "D8H6oZb09r5w"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "X76iJiMb-4NN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "aZVqZ8br-pZG",
    "outputId": "3b89d8a3-ce05-4807-ce5f-a8bfb52ca36e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>too bland and fustily tasteful to be truly pru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>it does n't work as either</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>this one aims for the toilet and scores a dire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>in the name of an allegedly inspiring and easi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>the movie is undone by a filmmaking methodolog...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "1995  too bland and fustily tasteful to be truly pru...      0\n",
       "1996                         it does n't work as either      0\n",
       "1997  this one aims for the toilet and scores a dire...      0\n",
       "1998  in the name of an allegedly inspiring and easi...      0\n",
       "1999  the movie is undone by a filmmaking methodolog...      0\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "imdb_df.columns=['review','label']\n",
    "imdb_df = imdb_df.head(2000)\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "-uzkCoYd_ZSu"
   },
   "outputs": [],
   "source": [
    "#XLNet model:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.XLNetModel, ppb.XLNetTokenizer, 'xlnet-base-cased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "hidden": true,
    "id": "GwuIiFokBEmU",
    "outputId": "dd2a1e48-9675-4f84-fe58-63f44b5a9e08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [24, 16003, 17, 19, 5787, 21, 1381, 21469, 17,...\n",
       "1       [3070, 17, 88, 10305, 8145, 66, 40, 18, 3821, ...\n",
       "2       [63, 28863, 58, 2477, 17, 6362, 17, 180, 26, 4...\n",
       "3       [52, 27, 24, 19849, 8665, 17, 2411, 10200, 31,...\n",
       "4       [17, 2595, 597, 4759, 2133, 118, 17, 26, 23, 1...\n",
       "                              ...                        \n",
       "1995    [269, 25928, 21, 17, 4257, 11332, 111, 3736, 1...\n",
       "1996       [36, 358, 17, 180, 26, 46, 154, 34, 725, 4, 3]\n",
       "1997    [52, 65, 6471, 28, 18, 8976, 21, 5100, 24, 156...\n",
       "1998    [25, 18, 304, 20, 48, 5168, 25, 7508, 56, 21, ...\n",
       "1999    [18, 1432, 27, 422, 13535, 37, 24, 468, 3746, ...\n",
       "Name: review, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = imdb_df['review'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "hidden": true,
    "id": "jh2MathyNbqD",
    "outputId": "41d82cd9-4cda-43e6-e1fb-408c7cc20e70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   24, 16003,    17, ...,     0,     0,     0],\n",
       "       [ 3070,    17,    88, ...,     0,     0,     0],\n",
       "       [   63, 28863,    58, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [   52,    65,  6471, ...,     0,     0,     0],\n",
       "       [   25,    18,   304, ...,     0,     0,     0],\n",
       "       [   18,  1432,    27, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "MWjpuuKQFJlx"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "hidden": true,
    "id": "PtJkengsCGBN",
    "outputId": "f942971d-db1e-445a-f369-66680e079045"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   24, 16003,    17, ...,     0,     0,     0],\n",
       "       [ 3070,    17,    88, ...,     0,     0,     0],\n",
       "       [   63, 28863,    58, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  545,   197,    65, ...,     0,     0,     0],\n",
       "       [   18,   468,    27, ...,     0,     0,     0],\n",
       "       [   28,   127,    20, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "T1oXLpnEC9nb"
   },
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "yvNUHkkIDFQt"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "hidden": true,
    "id": "9TTyVnAZDJiS",
    "outputId": "121f790b-94be-4e5e-bd63-3207c6929b39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.34125707,  2.3395584 , -1.4445125 , ..., -1.4342374 ,\n",
       "        -1.7864078 ,  0.650553  ],\n",
       "       [-2.0320308 , -0.9894735 ,  0.84645516, ..., -0.00756087,\n",
       "        -0.7017949 , -0.27526903],\n",
       "       [ 0.19869834,  1.1511657 , -1.9059381 , ..., -2.1064408 ,\n",
       "         3.143326  ,  0.2760778 ],\n",
       "       ...,\n",
       "       [-0.8050524 , -1.3606195 ,  0.12521945, ..., -1.3168265 ,\n",
       "         0.6666188 ,  1.0935333 ],\n",
       "       [-2.7931774 ,  1.2391853 , -1.8684678 , ..., -0.39258152,\n",
       "         1.5283482 ,  4.0816474 ],\n",
       "       [-3.2236118 , -1.4749207 , -1.1230023 , ..., -3.2364156 ,\n",
       "         2.1875968 ,  1.2608442 ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "aoqOTNevDQ76"
   },
   "outputs": [],
   "source": [
    "labels = imdb_df['label']\n",
    "train_features, test_features, train_labels,  test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "hidden": true,
    "id": "WQngmoeTDXum",
    "outputId": "5b34f807-64a6-4583-bc7e-b6ef8f53c865"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Nqhdq7YfDek4",
    "outputId": "7399d70e-eb7f-4df9-8e1a-e8432716b50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.722"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "w6xneN-uAi_E"
   },
   "source": [
    "## Fine Tune XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ZuNvd83dxGgZ",
    "outputId": "df6a8f44-e95b-4ec2-f413-f7de3295052f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "\r",
      "\u001b[K     |█▉                              | 10kB 27.1MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 20kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 30kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 40kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 51kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 61kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 71kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 81kB 4.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 92kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 102kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 112kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 122kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 133kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 143kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 153kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 163kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 174kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 184kB 5.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.38.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.38)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.12.38)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.85)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.4.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.38 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.15.38)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->pytorch-transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.38->boto3->pytorch-transformers) (2.8.1)\n",
      "Installing collected packages: pytorch-transformers\n",
      "Successfully installed pytorch-transformers-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "PwlOsBoiwsqj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from pytorch_transformers import AdamW\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "egIf5HV9w7nD",
    "outputId": "439bd131-3c21-473b-a436-a05c5048487c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla P100-PCIE-16GB'"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ShmjUe_AMeTR",
    "outputId": "d4c2efda-5cf7-4ce9-e952-0907f5487086"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "6915  painful , horrifying and oppressively tragic ,...      1\n",
       "6916  take care is nicely performed by a quintet of ...      0\n",
       "6917  the script covers huge , heavy topics in a bla...      0\n",
       "6918  a seriously bad film with seriously warped log...      0\n",
       "6919  a deliciously nonsensical comedy about a city ...      1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "df.columns=['review','label']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "bDGRkIuoqj2H"
   },
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "sentences = df.review.values\n",
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "hidden": true,
    "id": "cmXVjYo9xYgn",
    "outputId": "b5792998-5abe-4d70-b19a-fab6de3ecf2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 798011/798011 [00:00<00:00, 2533595.99B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁a', '▁stirring', '▁', ',', '▁funny', '▁and', '▁finally', '▁transporting', '▁', 're', '▁im', 'agin', 'ing', '▁of', '▁beauty', '▁and', '▁the', '▁beast', '▁and', '▁1930', 's', '▁horror', '▁films', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "fvriyiV2xYeR"
   },
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "MAX_LEN = 128\n",
    "\n",
    "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "t6NxeA8NxYaY"
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "qe-n6tzvxYYM"
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ihV4WzgwxYUY"
   },
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "e6QesL9txYQf"
   },
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "hidden": true,
    "id": "gFeGZMOaxYII",
    "outputId": "132e0fe2-36f9-4e02-8c01-e5797881287d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690/690 [00:00<00:00, 294202.48B/s]\n",
      "100%|██████████| 467042463/467042463 [00:14<00:00, 32233202.26B/s]\n"
     ]
    }
   ],
   "source": [
    "# Load XLNEtForSequenceClassification, the pretrained XLNet model with a single linear classification layer on top. \n",
    "\n",
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
    "model.cuda()\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "lOjgSjEwxX7J"
   },
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "goWMWG9MyGFE"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "hidden": true,
    "id": "n4jon1iXyF7e",
    "outputId": "b9bbde3a-9d14-4a50-ec73-4e29809aea00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.4924195027504212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|██▌       | 1/4 [01:52<05:37, 112.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9295454545454546\n",
      "Train loss: 0.22406736870224658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 2/4 [03:45<03:45, 112.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9389204545454546\n",
      "Train loss: 0.1296544223259657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|███████▌  | 3/4 [05:37<01:52, 112.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9375\n",
      "Train loss: 0.08133496565696521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [07:30<00:00, 112.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9417613636363636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      logits = output[0]\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "uWlF5O9CyQel"
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/test.tsv', \n",
    "                      delimiter='\\t', header=None)\n",
    "df2.columns=['review','label']\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.review.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for XLNet to work properly\n",
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "labels = df.label.values\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "\n",
    "MAX_LEN = 128\n",
    "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask) \n",
    "\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "  \n",
    "batch_size = 32  \n",
    "\n",
    "\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "# Prediction on test set\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "  with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "6mQa06Qey9XV",
    "outputId": "ffc1abc0-882f-4fdd-8f38-dca2a1e601fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_set = []\n",
    "\n",
    "for i in range(len(true_labels)):\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  accuracy = accuracy_score(true_labels[i], pred_labels_i)                \n",
    "  accuracy_set.append(accuracy)\n",
    "\n",
    "acc = accuracy_score(true_labels[i], pred_labels_i)\n",
    "print('Accuracy score: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Comparison between ULMFit, BERT and XLNet (Own Assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "OwLft5_soOdX"
   },
   "source": [
    "\n",
    "1. Base neural network\n",
    "  \n",
    "  a. ULMFit - LSTM (variant of RNN, using recurrence mechanism)\n",
    "  \n",
    "  b. BERT - Transformer (using attention mechanism)\n",
    "  \n",
    "  c. XLNet - TransformerXL (using segment level recurrence mechanism)\n",
    "\n",
    "  ![alt text](https://2.bp.blogspot.com/--MRVzjIXx5I/XFCm-nmEDcI/AAAAAAAADuM/HoS7BQOmvrQyk833pMVHlEbdq_s_mXT2QCLcBGAs/s640/GIF2.gif)\n",
    "\n",
    "\n",
    "\n",
    "2. Maximum length of input\n",
    "\n",
    "  a. ULMFit - no limit, but as the length of input getting longer, the training time increases significantly as LSTM process input one by one\n",
    "\n",
    "  b. BERT - 512 tokens, if exceed, it will truncate the sequence to maximum length set, if fall short, it will pad the sequence to same length\n",
    "\n",
    "  c. XLNet - does not have the limitation of sentence length like BERT - XLNet processes sentences of any length by treating them as segment and carries state across segment\n",
    "\n",
    "\n",
    "\n",
    "3. Time consumed to train\n",
    "\n",
    "  a. ULMFit - Longer than BERT and XLNet as LSTM using recurrence mechanism (one epoch around 6 minutes to train, with GPU in colab)\n",
    "\n",
    "  b. BERT - faster then ULMFit due to parallelization (all inputs process at once), equally fast as XLNet (one epoch aound 1 minute to train, with GPU in colab)\n",
    "\n",
    "  c. XLNet - equally fast as BERT, however might face with insufficent memory (RAM) issues when handling a large dataset (in normal Google Colab with GPU, can't process 25k movie reviews dataset)\n",
    "\n",
    "\n",
    "\n",
    "4. Accuracy and community support\n",
    "\n",
    "  a. ULMFit - same as BERT (accuracy score of 0.93) but have strong commmunity support from fastai which is well developed and documented\n",
    "\n",
    "  b. BERT - same as ULMFit (accuracy score of 0.93) but less community support from huggingface which still in developing phase\n",
    "\n",
    "  c. XLNet - better than BERT and XLNet (accuracy score of 1.00) but since it's rather new in the field, resources available is even less than BERT (tutorial and blogpost)\n",
    "  \n",
    "Reference for NLP progress:\n",
    "http://nlpprogress.com/english/sentiment_analysis.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "6wQD_KG5lV2E",
    "zfTybDq7mthZ",
    "mXrs4HPbAi2G",
    "p9VQ_NcFyXHj",
    "arnleFafAi2g",
    "D7qlOvjKAi2r",
    "9ApAb4F6Ai3N",
    "bC2ZbH5vAi31",
    "GTKikVnhAi33",
    "ZsIsEjYLeJsd",
    "8BNmhN-tfDjd",
    "GYI627ymAi4v",
    "p3G2UNKdAi5r",
    "7rh59qY_Ai54",
    "D6L5gPBqAi58",
    "QHeEapwvG_Yh",
    "gG6DOn-rHSUJ",
    "kwS2nA7WAi7W",
    "i0qyZnnjAi7h",
    "WZqRqrPrAi7k",
    "FmgGOkOKAi-E",
    "zed_-kDVAi-N",
    "007rdwNgAi-P",
    "w6xneN-uAi_E"
   ],
   "machine_shape": "hm",
   "name": "Review_of_Sentiment_Analysis_Models1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08d7e07dd04249749e06066f6793a86e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0bf515550902405b9422b04826fed59b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13350ea1e587493ebee49c66a376de50": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4dcab2a594194141b0971580b80aae50",
      "max": 230,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08d7e07dd04249749e06066f6793a86e",
      "value": 230
     }
    },
    "13c7bd6e16d643f5aa88d92bd9061767": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba9bc9690b2d4bad9d1f7ea13372d5d7",
      "placeholder": "​",
      "style": "IPY_MODEL_8e3afd1eb1c04b8da54a44266b7b3a45",
      "value": "100% 230/230 [00:00&lt;00:00, 7.65kB/s]"
     }
    },
    "29d4fc8c9c7f4e8ebbbd2eedb9026952": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fb228ab02514680bff87c2ccc995b2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5165d03ab3e74642b58dfb326940df93",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f90392b10f944f1bb170ab06eb7d6fa",
      "value": 231508
     }
    },
    "42fc29f530ab46348101d8577f661a85": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4a22cac957fc44e182a830cc2089ebf2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dcab2a594194141b0971580b80aae50": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5165d03ab3e74642b58dfb326940df93": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57b1a671eb69405092c81233e7a26d46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc5a8d6ab2af47b5893bfe2854bba8cf",
      "max": 361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b73957c1329742218804441595061dd7",
      "value": 361
     }
    },
    "5a7df33964e04a509ea103e497105cb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a22cac957fc44e182a830cc2089ebf2",
      "placeholder": "​",
      "style": "IPY_MODEL_aa70859d034942e9836f4c180e522141",
      "value": " 232k/232k [00:00&lt;00:00, 2.82MB/s]"
     }
    },
    "6c7d9d6692df49878b9a4d73dfba383b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7185127225df4228a8c1cc5ee28e37ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57b1a671eb69405092c81233e7a26d46",
       "IPY_MODEL_72e8958d9f5c4ac7b7c6df0f972fe24d"
      ],
      "layout": "IPY_MODEL_6c7d9d6692df49878b9a4d73dfba383b"
     }
    },
    "72e8958d9f5c4ac7b7c6df0f972fe24d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_933e7f31e2d04ff7927f2b956f5ce0f9",
      "placeholder": "​",
      "style": "IPY_MODEL_8aff4266553743aab74edf7dfb5e786d",
      "value": " 361/361 [00:08&lt;00:00, 43.8B/s]"
     }
    },
    "7f90392b10f944f1bb170ab06eb7d6fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "887586c393bb44daae9a8f8727f96eb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13350ea1e587493ebee49c66a376de50",
       "IPY_MODEL_13c7bd6e16d643f5aa88d92bd9061767"
      ],
      "layout": "IPY_MODEL_f935f7a21de84e48b2458a5b6cc8d0b0"
     }
    },
    "8aff4266553743aab74edf7dfb5e786d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bd677b1f6224331a45e2905cf56d74a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad2e79bec9d040378d3a20a7ad6fc576",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_42fc29f530ab46348101d8577f661a85",
      "value": 440473133
     }
    },
    "8e3afd1eb1c04b8da54a44266b7b3a45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "933e7f31e2d04ff7927f2b956f5ce0f9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa70859d034942e9836f4c180e522141": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ad2e79bec9d040378d3a20a7ad6fc576": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b73957c1329742218804441595061dd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ba9bc9690b2d4bad9d1f7ea13372d5d7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "baf80a6b07414f5297cfb01d24a4ae80": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6ccc7c9c979400ab53c1a18b03b818d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3fb228ab02514680bff87c2ccc995b2d",
       "IPY_MODEL_5a7df33964e04a509ea103e497105cb8"
      ],
      "layout": "IPY_MODEL_29d4fc8c9c7f4e8ebbbd2eedb9026952"
     }
    },
    "dc5a8d6ab2af47b5893bfe2854bba8cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e551905f01314b32968deb834ed472e8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5c43ad5e0da463ea18829d0b34cffac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baf80a6b07414f5297cfb01d24a4ae80",
      "placeholder": "​",
      "style": "IPY_MODEL_0bf515550902405b9422b04826fed59b",
      "value": " 440M/440M [00:08&lt;00:00, 54.9MB/s]"
     }
    },
    "ec3b05fe0b054b1abb0a1dcbf3c5494f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bd677b1f6224331a45e2905cf56d74a",
       "IPY_MODEL_e5c43ad5e0da463ea18829d0b34cffac"
      ],
      "layout": "IPY_MODEL_e551905f01314b32968deb834ed472e8"
     }
    },
    "f935f7a21de84e48b2458a5b6cc8d0b0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
